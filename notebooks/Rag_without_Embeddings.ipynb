{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJSlGjQx0XvA"
      },
      "source": [
        "# RAG without embeddings: A keyword-first retrieval stack\n",
        "\n",
        "Not every search problem needs a vector store.\n",
        "\n",
        "There are plenty of use cases especially in incident response, enterprise ops, or tightly-scoped document corpora where plain old keyword retrieval can get you surprisingly far.\n",
        "\n",
        "This notebook explores what that looks like in practice:  \n",
        "A **BM25-powered RAG pipeline** built entirely without embeddings.\n",
        "\n",
        "We’ll use:\n",
        "- **Unstructured** to extract and chunk source docs from S3\n",
        "- **Elasticsearch Serverless** to handle retrieval via BM25\n",
        "- **LangChain + OpenAI** to run natural language queries over the results\n",
        "\n",
        "Along the way, we’ll see where this setup shines and where it quietly falls apart.  \n",
        "Some queries will resolve beautifully. Others will fail in subtle ways, with answers that *sound* right but aren't grounded.\n",
        "\n",
        "This isn’t about proving BM25 is enough. It’s about understanding what you get when you start simple.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBCWCbIH-y2B",
        "outputId": "6b41b07d-0b80-4336-de7e-8e5e92fedfa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting unstructured-client\n",
            "  Downloading unstructured_client-0.42.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-9.1.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-elasticsearch\n",
            "  Downloading langchain_elasticsearch-0.3.2-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (24.1.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (43.0.3)\n",
            "Requirement already satisfied: httpcore>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (1.0.9)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (2.11.7)\n",
            "Collecting pypdf>=4.0 (from unstructured-client)\n",
            "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (1.0.0)\n",
            "Collecting elastic-transport<10,>=9.1.0 (from elasticsearch)\n",
            "  Downloading elastic_transport-9.1.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from elasticsearch) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from elasticsearch) (4.14.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.72)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.97.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-8.19.0-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting elastic-transport<9,>=8.15.1 (from elasticsearch)\n",
            "  Downloading elastic_transport-8.17.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client) (1.17.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2025.7.14)\n",
            "Requirement already satisfied: simsimd>=3 in /usr/local/lib/python3.11/dist-packages (from elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (6.5.0)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore>=1.0.9->unstructured-client) (0.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client) (4.9.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client) (3.10)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.1)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->elasticsearch) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client) (2.22)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading unstructured_client-0.42.1-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_elasticsearch-0.3.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading elasticsearch-8.19.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.9/926.9 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading elastic_transport-8.17.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, pypdf, mypy-extensions, marshmallow, httpx-sse, elastic-transport, typing-inspect, elasticsearch, unstructured-client, pydantic-settings, dataclasses-json, langchain-openai, langchain-elasticsearch, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 elastic-transport-8.17.1 elasticsearch-8.19.0 httpx-sse-0.4.1 langchain-community-0.3.27 langchain-elasticsearch-0.3.2 langchain-openai-0.3.28 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 pypdf-5.9.0 python-dotenv-1.1.1 typing-inspect-0.9.0 unstructured-client-0.42.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U unstructured-client elasticsearch langchain-community langchain-openai langchain-elasticsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKwIDPhOt43L"
      },
      "source": [
        "## Setting up credentials and environment variables\n",
        "\n",
        "Before we define our workflow, we’ll load the necessary credentials for all the external services we’ll be using — Unstructured, AWS S3 (as our source), and Elasticsearch (as our destination).\n",
        "\n",
        "These are securely pulled from Colab secrets using `userdata.get(...)`, so make sure you’ve already added them via the “🔐 Secrets” tab in Colab.\n",
        "\n",
        "Here’s what each one is used for:\n",
        "\n",
        "- **Unstructured API key**: Required to access the Unstructured Workflows API.\n",
        "- **S3 credentials**: Used to fetch documents from an S3 bucket or folder.\n",
        "- **Elasticsearch credentials**: Used to push the processed, structured data into an Elasticsearch Serverless index.\n",
        "\n",
        "---\n",
        "\n",
        "### Where to get these values\n",
        "\n",
        "Here’s a quick guide on how to fetch the required credentials:\n",
        "\n",
        "#### 🔑 Unstructured API Key\n",
        "[Contact us](https://unstructured.io/enterprise) to get access or log in if you're already a user.\n",
        "\n",
        "\n",
        "#### 🪣 S3 Credentials\n",
        "We’re using the [S3 Source Connector](https://docs.unstructured.io/api-reference/workflow/sources/s3). You’ll need:\n",
        "\n",
        "- **AWS Access Key ID** and **Secret Access Key**: You can create these from your AWS IAM dashboard by creating a user with “AmazonS3ReadOnlyAccess” or similar permissions.\n",
        "- **S3 Remote URL**: This should point to the folder or bucket you want to ingest from — e.g. `s3://your-bucket-name/path-to-folder/`. Make sure it’s in URI format.\n",
        "\n",
        "\n",
        "\n",
        "#### 🔍 Elasticsearch (Serverless)\n",
        "We’re using the [Elasticsearch destination connector](https://docs.unstructured.io/api-reference/workflow/destinations/elasticsearch). To set this up:\n",
        "\n",
        "1. Go to [https://cloud.elastic.co](https://cloud.elastic.co) and create a **Serverless Project**.\n",
        "2. Under **Project Settings → API Keys**, create a new key.\n",
        "3. Grab the following values:\n",
        "   - **API key** (you’ll use this as `ES_API_KEY`)\n",
        "   - **Deployment URL** (this becomes `ES_HOST_NAME`)\n",
        "   - Your target **index name** (set this as `ES_INDEX_NAME`)\n",
        "\n",
        "That’s it — once these are in place as secrets, we’re ready to configure the connectors programmatically in the next step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzK2Wkd4_B78"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from google.colab import userdata\n",
        "\n",
        "# Unstructured\n",
        "os.environ['UNSTRUCTURED_API_KEY'] = userdata.get('UNSTRUCTURED_API_KEY')\n",
        "\n",
        "# AWS S3\n",
        "os.environ['AWS_ACCESS'] = userdata.get('AWS_ACCESS')\n",
        "os.environ['AWS_SECRET'] = userdata.get('AWS_SECRET')\n",
        "os.environ['S3_REMOTE_URL'] = userdata.get(\"S3_REMOTE_URL\")\n",
        "\n",
        "\n",
        "# Elasticsearch Serverless\n",
        "os.environ['ES_INDEX_NAME'] = userdata.get('ES_INDEX_NAME')\n",
        "os.environ['ES_HOST_NAME'] = userdata.get('ES_HOST_NAME')\n",
        "os.environ['ES_API_KEY'] = userdata.get('ES_API_KEY')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5ODuJ17HdzP"
      },
      "outputs": [],
      "source": [
        "# instantiate Unstructured Client\n",
        "from unstructured_client import UnstructuredClient\n",
        "\n",
        "unstructured_client = UnstructuredClient(api_key_auth=os.environ['UNSTRUCTURED_API_KEY'])\n",
        "\n",
        "# helper function\n",
        "def pretty_print_model(response_model):\n",
        "    print(response_model.model_dump_json(indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpPiTZ63ulgQ"
      },
      "source": [
        "### Registering the S3 source connector\n",
        "\n",
        "Now that our credentials are set, let’s connect to the raw data stored in S3.\n",
        "\n",
        "This step registers an **S3 source connector** with the Unstructured API. Once created, this connector tells the system where to pull documents from during workflow execution.\n",
        "\n",
        "Here’s what’s happening:\n",
        "- We use the S3 credentials and remote URL from earlier.\n",
        "- `recursive=True` ensures that files inside nested folders will also be processed.\n",
        "\n",
        "Once the source is registered, Unstructured will return a unique `source_id` — you’ll use this to define the pipeline input in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEGFPod_zBhE",
        "outputId": "6ae57580-3068-4be6-c3a4-4342194ff751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"config\": {\n",
            "        \"anonymous\": false,\n",
            "        \"recursive\": true,\n",
            "        \"remote_url\": \"s3://ajay-uns-devrel-content/agentic-analysis/\",\n",
            "        \"key\": \"**********\",\n",
            "        \"secret\": \"**********\"\n",
            "    },\n",
            "    \"created_at\": \"2025-08-06T14:34:21.898458Z\",\n",
            "    \"id\": \"fbb2a2da-156e-4317-a394-40596bc7b102\",\n",
            "    \"name\": \"Rag w/o Embeddings Source_ 14:34:21\",\n",
            "    \"type\": \"s3\",\n",
            "    \"updated_at\": \"2025-08-06T14:34:22.081140Z\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from unstructured_client.models.operations import CreateSourceRequest\n",
        "from unstructured_client.models.shared import CreateSourceConnector\n",
        "\n",
        "formatted_time = datetime.now().strftime(\"%H:%M:%S\")\n",
        "source_response = unstructured_client.sources.create_source(\n",
        "    request=CreateSourceRequest(\n",
        "        create_source_connector=CreateSourceConnector(\n",
        "            name=f\"Rag w/o Embeddings Source_ {formatted_time}\",\n",
        "            type=\"s3\",\n",
        "            config={\n",
        "              \"key\": os.environ.get('AWS_ACCESS'),\n",
        "              \"secret\": os.environ.get('AWS_SECRET'),\n",
        "              \"remote_url\": os.environ.get('S3_REMOTE_URL'),\n",
        "              \"recursive\": True\n",
        "            }\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "pretty_print_model(source_response.source_connector_information)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCHd82pruznb"
      },
      "source": [
        "### Registering the Elasticsearch destination connector\n",
        "\n",
        "With our source in place, we now define where the processed data should go.\n",
        "\n",
        "In this case, we’re using **Elasticsearch Serverless** as our destination. This connector pushes cleaned, structured chunks directly into your configured index — making them queryable for downstream RAG tasks.\n",
        "\n",
        "Here’s a breakdown of what’s passed into the connector:\n",
        "- `hosts`: The Elasticsearch deployment URL (from your Serverless project).\n",
        "- `es_api_key`: The API key you created earlier for secure access.\n",
        "- `index_name`: The target index where documents will be stored.\n",
        "\n",
        "> 📌 Note: The index will be created automatically if it doesn’t already exist.\n",
        "\n",
        "After this step, Unstructured will return a `destination_id`, which we’ll use to tie the source and destination together in the next step: building the workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bBlUKx1zoEi",
        "outputId": "106525eb-fb68-44f7-ab88-c724e602908b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"config\": {\n",
            "        \"es_api_key\": \"**********\",\n",
            "        \"hosts\": [\n",
            "            \"https://my-elasticsearch-project-cf9288.es.us-east-1.aws.elastic.cloud:443\"\n",
            "        ],\n",
            "        \"index_name\": \"es-demo\"\n",
            "    },\n",
            "    \"created_at\": \"2025-08-06T14:36:34.442290Z\",\n",
            "    \"id\": \"19bd8287-d7b5-4d7d-84ab-63ad14e07b70\",\n",
            "    \"name\": \"ES_Destination_connector_14:34:21\",\n",
            "    \"type\": \"elasticsearch\",\n",
            "    \"updated_at\": \"2025-08-06T14:36:34.562580Z\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from unstructured_client.models.operations import CreateDestinationRequest\n",
        "from unstructured_client.models.shared import CreateDestinationConnector\n",
        "\n",
        "destination_response = unstructured_client.destinations.create_destination(\n",
        "    request=CreateDestinationRequest(\n",
        "        create_destination_connector=CreateDestinationConnector(\n",
        "            name=f\"ES_Destination_connector_{formatted_time}\",\n",
        "            type=\"elasticsearch\",\n",
        "            config={\n",
        "                \"hosts\": [os.environ['ES_HOST_NAME']],\n",
        "                \"es_api_key\": os.environ['ES_API_KEY'],\n",
        "                \"index_name\": os.environ['ES_INDEX_NAME']\n",
        "            }\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "pretty_print_model(destination_response.destination_connector_information)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cc-hhaOvRs_"
      },
      "source": [
        "### Building an Unstructured workflow\n",
        "\n",
        "Now that we’ve registered both our source and destination connectors, it’s time to define how documents should be processed.\n",
        "\n",
        "This step creates a **custom workflow** in Unstructured that connects:\n",
        "1. The S3 source (documents in)\n",
        "2. A two-step transformation pipeline\n",
        "3. The Elasticsearch destination (clean chunks out)\n",
        "\n",
        "Here’s what the processing nodes do:\n",
        "\n",
        "- **Partitioner**: Uses a Vision-Language Model (Anthropic Claude Sonnet) to extract clean structured content — preserving layout, tables, and section headers.\n",
        "- **Chunker**: Breaks up the content into smaller pieces. We’re using a title-aware strategy with controlled overlap (`4096` max characters, `150` character overlap) to preserve context for retrieval.\n",
        "\n",
        "> 🔍 No embedder here, and that’s intentional.  \n",
        "> For this tutorial, we’ll be using **BM25** for retrieval instead of dense vector embeddings, so there’s no need to generate embeddings in this pipeline.\n",
        "\n",
        "Once the workflow is created, we save the `workflow_id` so we can run it in the next step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2usCToqHy3O",
        "outputId": "377e58a4-0e50-4d1a-b3dc-16a5b8b3fccd"
      },
      "outputs": [],
      "source": [
        "from unstructured_client.models.shared import (\n",
        "    WorkflowNode,\n",
        "    WorkflowType,\n",
        "    Schedule\n",
        ")\n",
        "\n",
        "parition_node = WorkflowNode(\n",
        "    name=\"Partitioner\",\n",
        "    subtype=\"vlm\",\n",
        "    type=\"partition\",\n",
        "    settings={\n",
        "        \"provider\": \"anthropic\",\n",
        "        \"model\": \"claude-sonnet-4-5-20250929\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "chunk_node = WorkflowNode(\n",
        "    name=\"Chunker\",\n",
        "    subtype=\"chunk_by_title\",\n",
        "    type=\"chunk\",\n",
        "    settings={\n",
        "        \"new_after_n_chars\": 1000,\n",
        "        \"max_characters\": 4096,\n",
        "        \"overlap\": 150\n",
        "    }\n",
        ")\n",
        "\n",
        "response = unstructured_client.workflows.create_workflow(\n",
        "    request={\n",
        "        \"create_workflow\": {\n",
        "            \"name\": f\"Rag w/o Embeddings Tutorial Workflow_ {time.time()}\",\n",
        "            \"source_id\": source_response.source_connector_information.id,\n",
        "            \"destination_id\": destination_response.destination_connector_information.id,\n",
        "            \"workflow_type\": WorkflowType.CUSTOM,\n",
        "            \"workflow_nodes\": [\n",
        "                parition_node,\n",
        "                chunk_node\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "pretty_print_model(response.workflow_information)\n",
        "workflow_id = response.workflow_information.id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOIkt9GOwf6i"
      },
      "source": [
        "### Run the workflow\n",
        "\n",
        "Run the following cell to start running the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QLgPRt-JNYD",
        "outputId": "8660feae-8dec-4836-a947-709b47bcb792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"created_at\": \"2025-07-19T19:40:36.320615Z\",\n",
            "    \"id\": \"5270557c-2e97-4bc2-998c-0eb8af189c18\",\n",
            "    \"status\": \"SCHEDULED\",\n",
            "    \"workflow_id\": \"46a8b815-0528-4afe-bba4-03f05f4310b5\",\n",
            "    \"workflow_name\": \"Rag w/o Embeddings Tutorial Workflow_ 1752954034.9518843\",\n",
            "    \"job_type\": \"ephemeral\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "res = unstructured_client.workflows.run_workflow(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id,\n",
        "    }\n",
        ")\n",
        "\n",
        "pretty_print_model(res.job_information)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObIv1fHfwigb"
      },
      "source": [
        "### Get the workflow run's job ID\n",
        "\n",
        "Run the following cell to get the workflow run's job ID, which is needed to poll for job completion later. If successful, Unstructured prints the job's ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP5ZPuQJJgQp",
        "outputId": "66223620-9b5f-4bab-84db-398dcfd7a2c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "job_id: 5270557c-2e97-4bc2-998c-0eb8af189c18\n"
          ]
        }
      ],
      "source": [
        "response = unstructured_client.jobs.list_jobs(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id\n",
        "    }\n",
        ")\n",
        "\n",
        "last_job = response.response_list_jobs[0]\n",
        "job_id = last_job.id\n",
        "print(f\"job_id: {job_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJoLbPwLJupD"
      },
      "source": [
        "### Poll for job completion\n",
        "\n",
        "Run the following cell to confirm the job has finished running. If successful, Unstructured prints `\"status\": \"COMPLETED\"` within the information about the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEfi8Q_SJzuh",
        "outputId": "8010469c-764b-423e-c5a2-21debc442537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job is scheduled, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is completed\n",
            "{\n",
            "    \"created_at\": \"2025-07-19T19:40:36.320615\",\n",
            "    \"id\": \"5270557c-2e97-4bc2-998c-0eb8af189c18\",\n",
            "    \"status\": \"COMPLETED\",\n",
            "    \"workflow_id\": \"46a8b815-0528-4afe-bba4-03f05f4310b5\",\n",
            "    \"workflow_name\": \"Rag w/o Embeddings Tutorial Workflow_ 1752954034.9518843\",\n",
            "    \"job_type\": \"ephemeral\",\n",
            "    \"runtime\": \"PT0S\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def poll_job_status(job_id, wait_time=30):\n",
        "    while True:\n",
        "        response = unstructured_client.jobs.get_job(\n",
        "            request={\n",
        "                \"job_id\": job_id\n",
        "            }\n",
        "        )\n",
        "\n",
        "        job = response.job_information\n",
        "\n",
        "        if job.status == \"SCHEDULED\":\n",
        "            print(f\"Job is scheduled, polling again in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        elif job.status == \"IN_PROGRESS\":\n",
        "            print(f\"Job is in progress, polling again in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        else:\n",
        "            print(\"Job is completed\")\n",
        "            break\n",
        "\n",
        "    return job\n",
        "\n",
        "job = poll_job_status(job_id)\n",
        "pretty_print_model(job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqlCmPPDwGQb"
      },
      "source": [
        "At this point, we’ve successfully run the full Unstructured pipeline:\n",
        "\n",
        "- Documents were pulled from S3\n",
        "- Cleaned and chunked using the Partitioner and Chunker nodes\n",
        "- And indexed into our Elasticsearch Serverless instance\n",
        "\n",
        "All of this happened without generating embeddings — and that’s by design.\n",
        "\n",
        "In the next section, we’ll build a lightweight **RAG pipeline** that uses traditional keyword-based search (**BM25**) to retrieve context from Elasticsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ZYinLyKkc0"
      },
      "source": [
        "## RAG\n",
        "\n",
        "In this section, we’ll build a Retrieval-Augmented Generation (RAG) pipeline but without using any embeddings.  \n",
        "Instead, we’ll rely on a classic scoring algorithm called **BM25**, which powers the keyword-based search inside Elasticsearch.\n",
        "\n",
        "### What is BM25?\n",
        "\n",
        "BM25 is a **ranking function** that scores documents based on how well they match a query using exact terms, partial matches, and some clever normalization behind the scenes.\n",
        "\n",
        "It’s been a staple in information retrieval for decades, and it still holds up remarkably well when:\n",
        "- Your documents are chunked cleanly\n",
        "- Your queries are fairly literal (i.e., not abstract or fuzzy)\n",
        "\n",
        "Here’s how it works, at a high level:\n",
        "\n",
        "- **Matching terms boost relevance**: If a chunk contains your search terms, it scores higher.\n",
        "- **Rare words carry more weight**: Matches on uncommon terms matter more than matches on generic words.\n",
        "- **Document length is normalized**: Longer chunks don’t get an unfair advantage just because they mention everything.\n",
        "\n",
        "Unlike dense embeddings, BM25 doesn’t “understand” semantic meaning. It’s not going to connect synonyms or paraphrases. But when your queries are sharp and your chunking is good — it can work surprisingly well.\n",
        "\n",
        "> 🧠 Why use this?\n",
        "> - It’s **fast**, **transparent**, and doesn’t need a GPU or embedding model.\n",
        "> - It’s perfect for bootstrapping or low-latency use cases.\n",
        "\n",
        "We’ll now query the indexed data in Elasticsearch using BM25 and pass the results into our LLM to generate grounded answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbsTPR6rxo2c"
      },
      "outputs": [],
      "source": [
        "from langchain_elasticsearch import ElasticsearchStore, BM25Strategy\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from elasticsearch import Elasticsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewi4CNb7xjJV"
      },
      "source": [
        "### Setting up the BM25-backed RAG pipeline\n",
        "\n",
        "With our data indexed and ready, we can now run queries over it using BM25 retrieval.\n",
        "\n",
        "Here’s how this section works:\n",
        "1. We connect to the Elasticsearch Serverless instance using the `Elasticsearch` Python client.\n",
        "2. We initialize a `BM25Strategy` — this wraps keyword-based scoring around our document chunks.\n",
        "3. We query Elasticsearch for the top-k most relevant chunks (`similarity_search`), and pass them to GPT-4o to generate an answer.\n",
        "\n",
        "#### BM25 parameters: `k1` and `b`\n",
        "\n",
        "- **`k1` (default: `1.2`)**  \n",
        "  Controls **term frequency scaling** — how much repeated terms matter.  \n",
        "  - Higher `k1` = more boost for repeated keywords  \n",
        "  - Lower `k1` = frequency saturates quickly\n",
        "\n",
        "- **`b` (default: `0.75`)**  \n",
        "  Controls **document length normalization** — i.e., should longer chunks be penalized?  \n",
        "  - `b = 0` → No length penalty (longer chunks may dominate)  \n",
        "  - `b = 1` → Full normalization (neutralizes doc length bias)\n",
        "\n",
        "These values work well in practice, but you can tune them if:\n",
        "- Your chunks are very short/long\n",
        "- You see irrelevant long documents dominating results\n",
        "\n",
        "\n",
        "The `run_query_direct(...)` function wraps the whole RAG flow:\n",
        "\n",
        "- It retrieves the top-k hits via BM25\n",
        "- Assembles a context string\n",
        "- Injects it into a prompt\n",
        "- And uses GPT-4o to answer based only on that context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY3cdKCeNw5u"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "def connect_elasticsearch():\n",
        "    return Elasticsearch(\n",
        "        os.environ['ES_HOST_NAME'],\n",
        "        api_key=os.environ['ES_API_KEY']\n",
        "    )\n",
        "\n",
        "def init_bm25_store(es_client, index_name):\n",
        "    bm25_strategy = BM25Strategy(k1=1.2, b=0.75)\n",
        "\n",
        "    store = ElasticsearchStore(\n",
        "        es_connection=es_client,\n",
        "        index_name=index_name,\n",
        "        strategy=bm25_strategy\n",
        "    )\n",
        "    return store\n",
        "\n",
        "def run_query_direct(store, query, k=5):\n",
        "    print(f\"\\n--- QUERY: {query} ---\")\n",
        "\n",
        "    docs = store.similarity_search(query, k=k)\n",
        "\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "    Answer the following question based only on the provided context:\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\")\n",
        "\n",
        "    formatted_prompt = prompt.format(context=context, question=query)\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "\n",
        "    print(\"RETRIEVED DOCUMENTS:\")\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        print(f\"{i}. {doc.page_content[:200]}...\")\n",
        "\n",
        "\n",
        "    print(f\"\\nANSWER:\")\n",
        "    print(response.content)\n",
        "\n",
        "    return response.content, docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fe92JbhvOBu5"
      },
      "outputs": [],
      "source": [
        "es_client = connect_elasticsearch()\n",
        "store = init_bm25_store(es_client, os.environ['ES_INDEX_NAME'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxwxZFNkyjj5"
      },
      "source": [
        "Now let's run a sample question on our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2DfXAfOO1lx",
        "outputId": "f7f5db43-b103-4c1f-db28-8dd8f4c7b762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- QUERY: What are the containment procedures? ---\n",
            "RETRIEVED DOCUMENTS:\n",
            "1. Analyze for Common Adversary TTPs\n",
            "\n",
            "Compare TTPs to adversary TTPs documented in ATT&CK and analyze how the TTPs fit into the attack lifecycle. TTPs describe \"why,\" \"what,\" and \"how.\" Tactics describe ...\n",
            "2. TLP:CLEAR\n",
            "\n",
            "Incident Response Process flowchart showing the workflow from START through various phases including Declare Incident, Determine Investigation Scope, Share CTI, Collect and Preserve Data, P...\n",
            "3. TLP:CLEAR\n",
            "\n",
            "Step Incident Response Procedure Action Taken Date Completed 9c. Reset passwords on compromised accounts. 9d. Implement multi-factor authentication for all access methods. 9e. Install updat...\n",
            "4. 7. Contain Activity (Short-term Mitigations)\n",
            "\n",
            "7a. Determine appropriate containment strategy, including: • Requirement to preserve evidence • Availability of services (e.g., network connectivity, serv...\n",
            "5. TLP:CLEAR\n",
            "\n",
            "Term Definition Source National Security Systems (NSS) National Security Systems (NSS) are information systems as defined in 44 U.S.C.3552(b)(6). {A}The term \"national security system\" mean...\n",
            "\n",
            "ANSWER:\n",
            "The containment procedures outlined in the provided context involve several strategic actions aimed at mitigating immediate threats while preserving evidence and maintaining system operations where possible. Here is a summary of the containment procedures:\n",
            "\n",
            "1. **Determine Appropriate Containment Strategy**:\n",
            "   - Assess the requirement to preserve evidence.\n",
            "   - Consider the availability of services like network connectivity and services continuity.\n",
            "   - Take into account resource constraints and the duration of containment steps.\n",
            "\n",
            "2. **System Backup**:\n",
            "   - Create backups to preserve evidence and facilitate continued investigation.\n",
            "\n",
            "3. **Coordinate with Law Enforcement**:\n",
            "   - If necessary, engage with law enforcement to collect and preserve evidence before eradication.\n",
            "\n",
            "4. **Isolation of Affected Systems and Networks**:\n",
            "   - Implement perimeter containment and internal network containment.\n",
            "   - Conduct host-based or endpoint isolation.\n",
            "   - Temporarily disconnect public-facing systems from the internet.\n",
            "\n",
            "5. **Update Security Configurations**:\n",
            "   - Close specific network ports and mail servers.\n",
            "   - Update firewall filtering rules.\n",
            "\n",
            "6. **Credential and Access Management**:\n",
            "   - Change system admin passwords.\n",
            "   - Rotate private keys and service/application account secrets where compromise is suspected.\n",
            "   - Revoke privileged access.\n",
            "\n",
            "7. **Blocking and Monitoring**:\n",
            "   - Block and log unauthorized access and traffic to and from known attacker IP addresses and other identified sources of threat.\n",
            "   - Prevent DNS resolution of known attacker domain names.\n",
            "\n",
            "8. **Restrict Network Communications**:\n",
            "   - Prevent compromised systems from connecting to other systems on the network.\n",
            "\n",
            "9. **Adversary Activity Monitoring**:\n",
            "   - Advanced Security Operations Centers (SOCs) may redirect adversaries to a sandbox environment to monitor activities, gather evidence, and identify TTPs.\n",
            "   - Continuously monitor for signs of threat actor response to containment activities.\n",
            "\n",
            "10. **Reporting and Adjustments**:\n",
            "    - Update the timeline and findings with new indicators.\n",
            "    - If new signs of compromise are found, return to technical analysis to reassess and potentially expand the investigation scope.\n",
            "\n",
            "11. **Evidence Preservation**:\n",
            "    - Upon successful containment with no new signs of compromise, preserve evidence for future reference or law enforcement investigation.\n",
            "\n",
            "These steps collectively help to limit the spread of the attack while facilitating further investigation and evidence collection, ultimately supporting subsequent eradication efforts.\n"
          ]
        }
      ],
      "source": [
        "response, docs = run_query_direct(store, \"What are the containment procedures?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcLnWVnqyuRN"
      },
      "source": [
        "\n",
        "Because the query used clear, operational language (“containment procedures”), BM25 was able to surface high-signal chunks that directly addressed the topic — including full containment checklists and tactical steps.\n",
        "\n",
        "The LLM then stitched together the overlapping context into a clean, actionable list — covering evidence preservation, system isolation, access revocation, and more.\n",
        "\n",
        "> ✅ This is where keyword search shines: when your documents are structured, and your query terms match section headers or list items directly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS5SQkpJzKVg"
      },
      "source": [
        "Now let's try a more abstract query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw7xpb-NO13b",
        "outputId": "7c387616-6a12-440a-8de2-a8bd256c0898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- QUERY: Where in the document does it describe coordination between the SOC and executive leadership? ---\n",
            "RETRIEVED DOCUMENTS:\n",
            "1. TLP:CLEAR TLP:CLEAR label in black background with white text CISA | Cybersecurity and Infrastructure Security Agency 2\n",
            "\n",
            "TLP:CLEAR\n",
            "\n",
            "INTRODUCTION\n",
            "\n",
            "The Cybersecurity and Infrastructure Security Agency (...\n",
            "2. TLP:CLEAR\n",
            "\n",
            "Term Definition Source National Security Systems (NSS) National Security Systems (NSS) are information systems as defined in 44 U.S.C.3552(b)(6). {A}The term \"national security system\" mean...\n",
            "3. APPENDIX G: SOURCE TEXT\n",
            "\n",
            "Agency Responsibilities References Cyber Response Group (CRG) Coordinates the development and implementation of the federal government's policies, strategies, and procedures f...\n",
            "4. TLP:CLEAR\n",
            "\n",
            "Step Incident Response Procedure Action Taken Date Completed 9c. Reset passwords on compromised accounts. 9d. Implement multi-factor authentication for all access methods. 9e. Install updat...\n",
            "5. Coordination with CISA\n",
            "\n",
            "Cyber defense capabilities vary widely. For this reason, coordination involves different degrees of engagement between the affected agency and CISA. As a baseline, every cybers...\n",
            "\n",
            "ANSWER:\n",
            "The document discusses coordination between CISA and affected FCEB agencies in several places, but it does not specifically mention coordination between the Security Operations Center (SOC) and executive leadership. The document primarily focuses on the coordination of incident response activities, communication, and information sharing between CISA and FCEB agencies, along with the responsibilities of various groups such as the Cyber Unified Coordination Group (C-UCG). However, it emphasizes the importance of communication and situational awareness between agency leadership and CISA, which may imply coordination at various organizational levels, including the SOC and executive leadership.\n"
          ]
        }
      ],
      "source": [
        "response, docs = run_query_direct(store, \"Where in the document does it describe coordination between the SOC and executive leadership?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBfXX1DQzoIz"
      },
      "source": [
        "We asked about coordination between the **SOC** and **executive leadership**, but the documents retrieved didn’t contain an exact match. Instead, they surfaced adjacent topics like:\n",
        "\n",
        "- Reporting incidents to **CISA** and **IT leadership**\n",
        "- Establishing cross-agency communications protocols\n",
        "\n",
        "The LLM still produced a fluent answer — but it was largely inferred from context, not grounded in an exact passage. This is a classic case where **BM25 lacks the fuzziness or semantic awareness** needed to bridge slightly different wording.\n",
        "\n",
        "> ⚠️ This is where embedding-based retrieval would outperform:  \n",
        "> A vector store could connect “SOC coordination” with descriptions of escalation protocols, even if those words aren’t used verbatim.\n",
        "\n",
        "So while BM25 gave us *close-ish* chunks, the final answer wasn’t fully supported by the source, and that’s important to catch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um4A3Se4z_cN"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This walkthrough demonstrated how to build a RAG pipeline without using embeddings — relying instead on **BM25 keyword search** for document retrieval.\n",
        "\n",
        "We saw that:\n",
        "\n",
        "- 🔍 BM25 performs well when queries use **precise terms** that align closely with the document’s language or structure.\n",
        "- ⚠️ It falls short when the language **diverges** — like asking abstract or cross-functional questions not spelled out in exact keywords.\n",
        "- 🤖 The LLM can sometimes *paper over* poor retrieval by guessing — but that breaks the grounding contract of RAG.\n",
        "\n",
        "### When does this approach make sense?\n",
        "\n",
        "Use BM25-based RAG when:\n",
        "- Your document set is small to medium-sized\n",
        "- You don’t want to manage embeddings or vector stores\n",
        "- Your queries are likely to match real wording in the docs (e.g., checklists, procedures, FAQs)\n",
        "\n",
        "But if you’re working with more ambiguous queries — or documents with varied phrasing — **embedding-based search or a hybrid strategy** will perform better.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Next steps\n",
        "\n",
        "Try extending this notebook by:\n",
        "- Swapping in a **hybrid retrieval strategy** (BM25 + vectors)\n",
        "- Adding an **embedding step** to the Unstructured workflow\n",
        "- Testing queries that deliberately push the limits of lexical matching\n",
        "\n",
        "You now have a full BM25-based RAG system running, feel free to plug in your own docs and explore how it holds up.\n",
        "\n",
        "> ⚡️ Want to go deeper?  \n",
        "> Check out [Unstructured’s API docs](https://docs.unstructured.io) for advanced connectors, chunking strategies, and embedding options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVX0TlZOSFBT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
