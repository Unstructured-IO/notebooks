{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "03935b29",
      "metadata": {
        "id": "03935b29"
      },
      "source": [
        "# S3 to MongoDB Pipeline using Unstructured Workflow Endpoint\n",
        "\n",
        "This notebook demonstrates a complete end-to-end document processing pipeline using the **Unstructured Workflow Endpoint**. The pipeline processes PDF documents from URLs and stores the structured results in MongoDB, showcasing the power and flexibility of the Unstructured platform for enterprise document processing.\n",
        "\n",
        "## What is the Unstructured Workflow Endpoint?\n",
        "\n",
        "The **Unstructured Workflow Endpoint** is a powerful cloud-based platform that enables organizations to build sophisticated document processing pipelines without managing infrastructure. Key features include:\n",
        "\n",
        "- **🔗 Source Connectors**: Connect to various data sources (S3, Azure Blob, Google Drive, etc.)\n",
        "- **🎯 Destination Connectors**: Output processed data to databases, data warehouses, or vector stores\n",
        "- **⚙️ Processing Nodes**: Modular components for partitioning, chunking, embedding, and more\n",
        "- **🤖 AI-Powered Processing**: Leverage advanced models like Vision Language Models (VLMs) for intelligent document understanding\n",
        "- **📊 Workflow Orchestration**: Chain multiple processing steps into sophisticated pipelines\n",
        "- **🔄 Job Management**: Monitor, track, and manage processing jobs at scale\n",
        "\n",
        "## Pipeline Architecture Overview\n",
        "\n",
        "Our pipeline demonstrates a real-world RAG (Retrieval-Augmented Generation) preparation workflow:\n",
        "\n",
        "```\n",
        "PDF URLs → Download → S3 Upload → Unstructured Workflow Endpoint → MongoDB\n",
        "                                        ↓\n",
        "                                  [Partition] → [Chunk] → [Embed]\n",
        "```\n",
        "\n",
        "### Processing Steps Explained:\n",
        "\n",
        "1. **📥 Document Acquisition**: Downloads PDF files from specified URLs to demonstrate batch processing\n",
        "2. **☁️ Cloud Storage**: Uploads documents to Amazon S3, the source for our workflow\n",
        "3. **🔌 Source Configuration**: Creates an S3 source connector that tells the API where to find documents\n",
        "4. **🎯 Destination Setup**: Configures a MongoDB destination connector for storing processed results\n",
        "5. **⚙️ Workflow Creation**: Builds a custom processing pipeline with three powerful nodes:\n",
        "   - **🧠 VLM Partitioner**: Uses GPT-4o Vision to intelligently segment documents, understanding layout, tables, images, and text hierarchy\n",
        "   - **✂️ Smart Chunker**: Breaks documents into semantically meaningful chunks based on document structure (titles, sections)\n",
        "   - **🔢 Vector Embedder**: Generates high-quality embeddings using OpenAI's text-embedding-3-small for semantic search\n",
        "6. **🚀 Execution & Monitoring**: Runs the workflow and provides real-time job monitoring with detailed status updates\n",
        "7. **✅ Verification**: Validates that processed documents are correctly stored in MongoDB with proper structure\n",
        "\n",
        "## Why Use Unstructured Workflow Endpoint?\n",
        "\n",
        "- **🏗️ No Infrastructure Management**: Focus on your use case, not on managing servers or scaling\n",
        "- **🎯 Production-Ready**: Built for enterprise scale with reliability and monitoring\n",
        "- **🔧 Flexible Architecture**: Mix and match processing nodes to create custom workflows\n",
        "- **📈 Scalable**: Process thousands of documents efficiently\n",
        "- **🤖 AI-Native**: Leverage cutting-edge AI models for document understanding\n",
        "- **🔗 Integration-Friendly**: Easy integration with existing data infrastructure\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, ensure you have:\n",
        "- **Unstructured API key** (sign up at [unstructured.io](https://unstructured.io))\n",
        "- **AWS credentials** with S3 read/write permissions\n",
        "- **MongoDB instance** (MongoDB Atlas)\n",
        "- **Python environment** with required packages installed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ca4d57",
      "metadata": {
        "id": "c1ca4d57"
      },
      "source": [
        "## Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7yDyQmJB4e5r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yDyQmJB4e5r",
        "outputId": "f465a9e8-f95b-4afd-97fd-42221e3d4967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (1.38.31)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.11/dist-packages (4.13.0)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.11/dist-packages (0.36.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (2025.4.26)\n",
            "Requirement already satisfied: botocore<1.39.0,>=1.38.31 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.38.31)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3) (0.13.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (24.1.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (43.0.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (2.11.5)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (5.6.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.31->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client) (0.4.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.31->boto3) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3 python-dotenv pymongo unstructured-client requests certifi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9899746e",
      "metadata": {
        "id": "9899746e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import uuid\n",
        "import requests\n",
        "import boto3\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "from pymongo import MongoClient\n",
        "from unstructured_client import UnstructuredClient\n",
        "from unstructured_client.models.operations import (\n",
        "    CreateSourceRequest,\n",
        "    CreateDestinationRequest\n",
        ")\n",
        "from unstructured_client.models.shared import (\n",
        "    CreateSourceConnector,\n",
        "    SourceConnectorType,\n",
        "    S3SourceConnectorConfigInput,\n",
        "    CreateDestinationConnector,\n",
        "    DestinationConnectorType,\n",
        "    MongoDBConnectorConfigInput,\n",
        "    WorkflowNode,\n",
        "    WorkflowType,\n",
        "    Schedule\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6238a53",
      "metadata": {
        "id": "c6238a53"
      },
      "source": [
        "## Configuration and Environment Setup\n",
        "\n",
        "This section loads environment variables and initializes the necessary clients for our pipeline.\n",
        "We'll validate all required credentials before proceeding with the workflow.\n",
        "\n",
        "The code below gives an example of how to hard-code your environment variables, which is totally sufficient for the purposes of this notebook. As an alternative to hard-coding, we suggest storing credentials as google colab user data and loading these credentials into the notebook (i.e. `os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')`). This is helpful if you plan on making commits to github and want to avoid accidentally commiting sensitive credentials to your git history. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "aea7a1e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aea7a1e6",
        "outputId": "dafe7d0d-0909-40da-8f7e-7ba787c6150b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All required configuration values set successfully\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# Configuration - Set these explicitly\n",
        "AWS_ACCESS_KEY_ID = \"your-access-key-id\"\n",
        "AWS_SECRET_ACCESS_KEY = \"your-secret-access-key\"\n",
        "AWS_REGION = \"your-region\"\n",
        "S3_BUCKET = \"your-s3-bucket-name\"\n",
        "UNSTRUCTURED_API_KEY = \"your-unstructured-api-key\"\n",
        "MONGO_URI = \"your-mongodb-connection-string\"\n",
        "AWS_SESSION_TOKEN = \"\"  # Optional: provide if using temporary credentials\n",
        "\n",
        "# Validation\n",
        "REQUIRED_VARS = {\n",
        "    \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n",
        "    \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n",
        "    \"AWS_REGION\": AWS_REGION,\n",
        "    \"S3_BUCKET\": S3_BUCKET,\n",
        "    \"UNSTRUCTURED_API_KEY\": UNSTRUCTURED_API_KEY,\n",
        "    \"MONGO_URI\": MONGO_URI\n",
        "}\n",
        "\n",
        "missing_vars = [key for key, value in REQUIRED_VARS.items() if not value]\n",
        "if missing_vars:\n",
        "    print(f\"❌ Missing required configuration values: {', '.join(missing_vars)}\")\n",
        "    print(\"Please update the script with the required information.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"✅ All required configuration values set successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "936b5ed1",
      "metadata": {
        "id": "936b5ed1"
      },
      "source": [
        "### Initialize Clients and Test Connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bcefb211",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcefb211",
        "lines_to_next_cell": 1,
        "outputId": "882f5987-77e6-441e-8a62-f2a53f368447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Initializing Unstructured client...\n",
            "🔧 Initializing AWS S3 client...\n",
            "🔧 Testing MongoDB connection...\n",
            "✅ MongoDB connection successful\n",
            "🎉 All clients initialized and connections tested successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create downloads directory for temporary file storage\n",
        "DOWNLOADS_DIR = \"downloads\"\n",
        "Path(DOWNLOADS_DIR).mkdir(exist_ok=True)\n",
        "\n",
        "# Initialize Unstructured client\n",
        "print(\"🔧 Initializing Unstructured client...\")\n",
        "unstructured_client = UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY)\n",
        "\n",
        "# Helper function to pretty print API responses\n",
        "def pretty_print_model(response_model):\n",
        "    \"\"\"Pretty print model responses for better readability\"\"\"\n",
        "    print(response_model.model_dump_json(indent=4))\n",
        "\n",
        "# Initialize AWS S3 client\n",
        "print(\"🔧 Initializing AWS S3 client...\")\n",
        "s3 = boto3.client(\n",
        "    \"s3\",\n",
        "    region_name=AWS_REGION,\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        ")\n",
        "\n",
        "# Test MongoDB connection to ensure it's accessible\n",
        "print(\"🔧 Testing MongoDB connection...\")\n",
        "try:\n",
        "    mongo_client = MongoClient(MONGO_URI, tls=True)\n",
        "    mongo_client.admin.command('ping')\n",
        "    print(\"✅ MongoDB connection successful\")\n",
        "    mongo_client.close()\n",
        "except Exception as e:\n",
        "    print(f\"❌ MongoDB connection failed: {e}\")\n",
        "    print(\"Please check your MONGO_URI and ensure MongoDB is accessible\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"🎉 All clients initialized and connections tested successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "754a6d79",
      "metadata": {
        "id": "754a6d79"
      },
      "source": [
        "## Utility Functions\n",
        "\n",
        "This section defines the core functions that power our document processing pipeline. Each function serves a specific purpose in the workflow and demonstrates different aspects of the Unstructured Workflow Endpoint.\n",
        "\n",
        "### Function Categories:\n",
        "\n",
        "1. **📥 Document Management Functions**\n",
        "   - `download_pdf()`: Downloads PDF files from URLs with robust error handling\n",
        "   - `upload_to_s3()`: Uploads files to S3 with proper organization and validation\n",
        "\n",
        "2. **🔌 Connector Management Functions**\n",
        "   - `create_s3_source_connector()`: Creates and configures S3 source connectors\n",
        "   - `create_mongodb_destination_connector()`: Sets up MongoDB destination connectors\n",
        "\n",
        "3. **⚙️ Workflow Orchestration Functions**\n",
        "   - `create_workflow()`: Builds custom processing workflows with multiple nodes\n",
        "   - `run_workflow()`: Executes workflows and returns job information\n",
        "\n",
        "4. **📊 Monitoring and Verification Functions**\n",
        "   - `poll_job_status()`: Monitors job progress with real-time status updates\n",
        "   - `verify_mongodb_results()`: Validates processed data in the destination database\n",
        "\n",
        "These functions showcase the flexibility and power of the Unstructured Workflow Endpoint, demonstrating how to build production-ready document processing pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2936469",
      "metadata": {
        "id": "a2936469"
      },
      "source": [
        "### Document Management Functions\n",
        "\n",
        "These functions handle the initial stages of our pipeline: acquiring documents and preparing them for processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9dacb818",
      "metadata": {
        "id": "9dacb818",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def download_pdf(url):\n",
        "    \"\"\"Download a PDF from a given URL with robust error handling.\"\"\"\n",
        "    try:\n",
        "        filename = url.split(\"/\")[-1]\n",
        "        if not filename.endswith('.pdf'):\n",
        "            filename += '.pdf'\n",
        "\n",
        "        local_path = os.path.join(DOWNLOADS_DIR, filename)\n",
        "\n",
        "        print(f\"  📥 Downloading {filename}...\")\n",
        "        response = requests.get(url, timeout=30, stream=True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Validate content type\n",
        "        content_type = response.headers.get('content-type', '').lower()\n",
        "        if 'pdf' not in content_type and 'application/octet-stream' not in content_type:\n",
        "            print(f\"  ⚠️ Warning: Expected PDF but got content-type: {content_type}\")\n",
        "\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        # Verify download success\n",
        "        if not os.path.exists(local_path) or os.path.getsize(local_path) == 0:\n",
        "            print(f\"  ❌ Downloaded file is empty or doesn't exist: {filename}\")\n",
        "            return None, None\n",
        "\n",
        "        file_size = os.path.getsize(local_path)\n",
        "        print(f\"  ✅ Downloaded {filename} ({file_size:,} bytes)\")\n",
        "        return filename, local_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Error downloading {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def upload_to_s3(local_path, filename):\n",
        "    \"\"\"Upload a file to S3 with error handling.\"\"\"\n",
        "    try:\n",
        "        s3_key = f\"workflow-docs/pdf/{filename}\"\n",
        "\n",
        "        if not os.path.exists(local_path):\n",
        "            print(f\"  ❌ Local file not found: {local_path}\")\n",
        "            return None\n",
        "\n",
        "        file_size = os.path.getsize(local_path)\n",
        "        s3.upload_file(local_path, S3_BUCKET, s3_key)\n",
        "        print(f\"  ✅ Uploaded to S3: s3://{S3_BUCKET}/{s3_key} ({file_size:,} bytes)\")\n",
        "        return s3_key\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Error uploading {filename} to S3: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2542bb56",
      "metadata": {
        "id": "2542bb56"
      },
      "source": [
        "### Connector Management Functions\n",
        "\n",
        "**Source and Destination Connectors** are fundamental components of the Unstructured Workflow Endpoint. They define where your data comes from and where processed results go.\n",
        "\n",
        "#### Understanding Connectors:\n",
        "- **Source Connectors**: Tell the API where to find your documents (S3, Azure Blob, Google Drive, etc.)\n",
        "- **Destination Connectors**: Define where processed data should be stored (MongoDB, Snowflake, Pinecone, etc.)\n",
        "- **Reusability**: Once created, connectors can be reused across multiple workflows\n",
        "- **Security**: Credentials are securely stored and managed by the Unstructured platform, which is SOC 2 Type II, ISO 27001, and HIPAA compliant, ensuring industry-standard protections for confidentiality, integrity, and availability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "33ef9f0b",
      "metadata": {
        "id": "33ef9f0b",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def create_s3_source_connector():\n",
        "    \"\"\"\n",
        "    Create an S3 source connector for the Unstructured Workflow Endpoint.\n",
        "\n",
        "    This function demonstrates how to configure the API to read documents from Amazon S3.\n",
        "    The connector will scan the specified S3 path for documents to process.\n",
        "\n",
        "    Key Configuration Options:\n",
        "    - remote_url: S3 path where documents are stored\n",
        "    - recursive: Whether to scan subdirectories\n",
        "    - credentials: AWS access keys for authentication\n",
        "    - region: AWS region for the S3 bucket\n",
        "\n",
        "    Returns:\n",
        "        str: Source connector ID if successful, None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"🔗 Creating S3 source connector...\")\n",
        "\n",
        "        response = unstructured_client.sources.create_source(\n",
        "            request=CreateSourceRequest(\n",
        "                create_source_connector=CreateSourceConnector(\n",
        "                    name=f\"s3_source_connector_{int(time.time())}\",  # Unique name with timestamp\n",
        "                    type=SourceConnectorType.S3,  # Specify S3 as the source type\n",
        "                    config=S3SourceConnectorConfigInput(\n",
        "                        remote_url=f\"s3://{S3_BUCKET}/workflow-docs/\",  # S3 path to scan\n",
        "                        recursive=True,  # Scan subdirectories\n",
        "                        key=AWS_ACCESS_KEY_ID,  # AWS credentials\n",
        "                        secret=AWS_SECRET_ACCESS_KEY,\n",
        "                        region=AWS_REGION  # AWS region\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "        source_id = response.source_connector_information.id\n",
        "        print(f\"  ✅ Created S3 source connector: {source_id}\")\n",
        "        return source_id\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Error creating S3 source connector: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_mongodb_destination_connector():\n",
        "    \"\"\"\n",
        "    Create a MongoDB destination connector for the Unstructured Workflow Endpoint.\n",
        "\n",
        "    This function demonstrates how to configure the API to store processed documents\n",
        "    in MongoDB. The connector handles the structured output from the workflow nodes.\n",
        "\n",
        "    Key Configuration Options:\n",
        "    - uri: MongoDB connection string (supports MongoDB Atlas)\n",
        "    - database: Target database name\n",
        "    - collection: Target collection name\n",
        "    - batch_size: Number of documents to write in each batch (optimizes performance)\n",
        "\n",
        "    Returns:\n",
        "        str: Destination connector ID if successful, None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"🔗 Creating MongoDB destination connector...\")\n",
        "\n",
        "        response = unstructured_client.destinations.create_destination(\n",
        "            request=CreateDestinationRequest(\n",
        "                create_destination_connector=CreateDestinationConnector(\n",
        "                    name=f\"mongodb_dest_connector_{int(time.time())}\",  # Unique name with timestamp\n",
        "                    type=DestinationConnectorType.MONGODB,  # Specify MongoDB as destination\n",
        "                    config=MongoDBConnectorConfigInput(\n",
        "                        uri=MONGO_URI,  # MongoDB connection string\n",
        "                        database=\"unstructured\",  # Target database\n",
        "                        collection=\"workflow_documents\",  # Target collection\n",
        "                        batch_size=20  # Batch size for optimal performance\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "        destination_id = response.destination_connector_information.id\n",
        "        print(f\"  ✅ Created MongoDB destination connector: {destination_id}\")\n",
        "        return destination_id\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Error creating MongoDB destination connector: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e0def4",
      "metadata": {
        "id": "31e0def4"
      },
      "source": [
        "### Workflow Orchestration Functions\n",
        "\n",
        "**Workflows** are the heart of the Unstructured platform. They define how documents are processed through a series of connected nodes, each performing specific operations on the data.\n",
        "\n",
        "#### Understanding Workflow Nodes:\n",
        "- **Partition Nodes**: Break documents into logical elements (paragraphs, tables, images)\n",
        "- **Chunk Nodes**: Split large elements into smaller, manageable pieces\n",
        "- **Embed Nodes**: Generate vector embeddings for semantic search\n",
        "- **Enrichment Nodes**: Apply custom transformations to the data such as summarization of text, images, and tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a8a25386",
      "metadata": {
        "id": "a8a25386",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def create_workflow(source_id, destination_id):\n",
        "    \"\"\"\n",
        "    Create a sophisticated document processing workflow using the Unstructured Workflow Endpoint.\n",
        "\n",
        "    This function demonstrates the power of the platform by creating a three-stage pipeline\n",
        "    that transforms raw PDFs into searchable, embedded document chunks ready for RAG applications.\n",
        "\n",
        "    Workflow Architecture:\n",
        "    PDF → [VLM Partition] → [Smart Chunk] → [Vector Embed] → MongoDB\n",
        "\n",
        "    Returns:\n",
        "        str: Workflow ID if successful, None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"⚙️ Creating advanced document processing workflow...\")\n",
        "\n",
        "        # 🧠 STAGE 1: VLM Partitioner Node\n",
        "        # Uses GPT-4o Vision to intelligently understand document structure\n",
        "        partition_node = WorkflowNode(\n",
        "            name=\"VLM_Partitioner\",\n",
        "            subtype=\"vlm\",  # Vision Language Model partitioning\n",
        "            type=\"partition\",\n",
        "            settings={\n",
        "                \"provider\": \"openai\",  # Use OpenAI's models\n",
        "                \"model\": \"gpt-4o\",     # GPT-4o with vision capabilities\n",
        "            }\n",
        "        )\n",
        "        print(\"  🧠 Configured VLM Partitioner: GPT-4o will analyze document layout, tables, and structure\")\n",
        "\n",
        "        # ✂️ STAGE 2: Smart Chunker Node\n",
        "        # Breaks documents into semantically meaningful pieces based on titles/sections\n",
        "        chunk_node = WorkflowNode(\n",
        "            name='Smart_Chunker',\n",
        "            subtype='chunk_by_title',  # Chunk based on document structure\n",
        "            type=\"chunk\",\n",
        "            settings={\n",
        "                'new_after_n_chars': 1500,    # Start new chunk after 1500 characters\n",
        "                'max_characters': 2048,       # Maximum chunk size (optimal for embeddings)\n",
        "                'overlap': 0                  # No overlap between chunks\n",
        "            }\n",
        "        )\n",
        "        print(\"  ✂️ Configured Smart Chunker: Will create 1500-2048 character chunks based on document structure\")\n",
        "\n",
        "        # 🔢 STAGE 3: Vector Embedder Node\n",
        "        # Generates high-quality embeddings for semantic search and RAG\n",
        "        embedder_node = WorkflowNode(\n",
        "            name='Vector_Embedder',\n",
        "            subtype='openai',  # Use OpenAI's embedding models\n",
        "            type=\"embed\",\n",
        "            settings={\n",
        "                'model_name': 'text-embedding-3-small'  # Latest OpenAI embedding model\n",
        "            }\n",
        "        )\n",
        "        print(\"  🔢 Configured Vector Embedder: Will generate embeddings using text-embedding-3-small\")\n",
        "\n",
        "        # 🔗 Create the complete workflow\n",
        "        response = unstructured_client.workflows.create_workflow(\n",
        "            request={\n",
        "                \"create_workflow\": {\n",
        "                    \"name\": f\"S3-to-MongoDB-RAG-Pipeline_{int(time.time())}\",\n",
        "                    \"source_id\": source_id,\n",
        "                    \"destination_id\": destination_id,\n",
        "                    \"workflow_type\": WorkflowType.CUSTOM,  # Custom workflow with our specific nodes\n",
        "                    \"workflow_nodes\": [\n",
        "                        partition_node,  # Process documents with VLM\n",
        "                        chunk_node,     # Break into manageable pieces\n",
        "                        embedder_node   # Generate vector embeddings\n",
        "                    ],\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(\"  📊 Workflow created successfully! Here's the configuration:\")\n",
        "        pretty_print_model(response.workflow_information)\n",
        "\n",
        "        workflow_id = response.workflow_information.id\n",
        "        print(f\"  ✅ Workflow ID: {workflow_id}\")\n",
        "        print(f\"  🎯 This workflow will process documents through: VLM Partition → Smart Chunk → Vector Embed\")\n",
        "        return workflow_id\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Error creating workflow: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db1acced",
      "metadata": {
        "id": "db1acced"
      },
      "source": [
        "### Job Management and Monitoring Functions\n",
        "\n",
        "The Unstructured Workflow Endpoint provides comprehensive job management capabilities, allowing you to monitor progress, handle failures, and track processing statistics in real-time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5d628bae",
      "metadata": {
        "id": "5d628bae",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def run_workflow(workflow_id):\n",
        "    \"\"\"Run the workflow and return job information.\"\"\"\n",
        "    try:\n",
        "        print(\"🚀 Running workflow...\")\n",
        "\n",
        "        res = unstructured_client.workflows.run_workflow(\n",
        "            request={\n",
        "                \"workflow_id\": workflow_id,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        pretty_print_model(res.job_information)\n",
        "        job_id = res.job_information.id\n",
        "        print(f\"  ✅ Started job: {job_id}\")\n",
        "        return job_id\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Error running workflow: {e}\")\n",
        "        return None\n",
        "\n",
        "def poll_job_status(job_id, wait_time=30):\n",
        "    \"\"\"Poll job status until completion.\"\"\"\n",
        "    print(f\"⏳ Polling job status (checking every {wait_time} seconds)...\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            response = unstructured_client.jobs.get_job(\n",
        "                request={\n",
        "                    \"job_id\": job_id\n",
        "                }\n",
        "            )\n",
        "\n",
        "            job = response.job_information\n",
        "            status = job.status\n",
        "\n",
        "            if status == \"SCHEDULED\":\n",
        "                print(f\"  📅 Job is scheduled, checking again in {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            elif status == \"IN_PROGRESS\":\n",
        "                print(f\"  ⚙️ Job is in progress, checking again in {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            elif status == \"COMPLETED\":\n",
        "                print(f\"  ✅ Job completed successfully!\")\n",
        "                return job\n",
        "            elif status == \"FAILED\":\n",
        "                print(f\"  ❌ Job failed!\")\n",
        "                return job\n",
        "            else:\n",
        "                print(f\"  ❓ Unknown job status: {status}\")\n",
        "                return job\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error polling job status: {e}\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "def verify_mongodb_results():\n",
        "    \"\"\"Verify that documents were stored in MongoDB.\"\"\"\n",
        "    try:\n",
        "        print(\"🔍 Verifying MongoDB results...\")\n",
        "\n",
        "        client = MongoClient(MONGO_URI, tls=True)\n",
        "        db = client[\"unstructured\"]\n",
        "        collection = db[\"workflow_documents\"]\n",
        "\n",
        "        count = collection.count_documents({})\n",
        "        print(f\"  📊 Total documents in MongoDB: {count}\")\n",
        "\n",
        "        if count > 0:\n",
        "            # Show a sample document\n",
        "            sample = collection.find_one()\n",
        "            print(f\"  📄 Sample document keys: {list(sample.keys())}\")\n",
        "\n",
        "        client.close()\n",
        "        return count\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Error verifying MongoDB results: {e}\")\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b303ae2",
      "metadata": {
        "id": "4b303ae2"
      },
      "source": [
        "## Main Pipeline Execution\n",
        "\n",
        "This section contains the main pipeline logic that orchestrates all the steps:\n",
        "1. Download PDFs and upload to S3\n",
        "2. Create source and destination connectors\n",
        "3. Create and run the workflow\n",
        "4. Monitor job progress\n",
        "5. Verify results in MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "79aca879",
      "metadata": {
        "id": "79aca879"
      },
      "outputs": [],
      "source": [
        "# List of PDF URLs to process\n",
        "pdf_urls = [\n",
        "    \"https://core.ac.uk/download/616883802.pdf\",\n",
        "    # Add more URLs as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a3328d3",
      "metadata": {
        "id": "0a3328d3"
      },
      "source": [
        "### Step 1: Download and Upload PDFs to S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2a7fd339",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a7fd339",
        "outputId": "e8ff668b-8084-484d-d5a4-134ada8d7d97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📥 Step 1: Downloading and uploading PDFs to S3\n",
            "\n",
            "📄 Processing: https://core.ac.uk/download/616883802.pdf\n",
            "  📥 Downloading 616883802.pdf...\n",
            "  ✅ Downloaded 616883802.pdf (209,313 bytes)\n",
            "  ✅ Uploaded to S3: s3://nvannest-quickstart-sample-data/workflow-docs/pdf/616883802.pdf (209,313 bytes)\n",
            "  🗑️ Cleaned up local file: downloads/616883802.pdf\n",
            "\n",
            "✅ Successfully uploaded 1 files to S3\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Download and upload PDFs to S3\n",
        "print(\"\\n📥 Step 1: Downloading and uploading PDFs to S3\")\n",
        "uploaded_files = []\n",
        "\n",
        "for url in pdf_urls:\n",
        "    print(f\"\\n📄 Processing: {url}\")\n",
        "    filename, local_path = download_pdf(url)\n",
        "\n",
        "    if filename and local_path:\n",
        "        s3_key = upload_to_s3(local_path, filename)\n",
        "        if s3_key:\n",
        "            uploaded_files.append(s3_key)\n",
        "            # Clean up local file\n",
        "            os.remove(local_path)\n",
        "            print(f\"  🗑️ Cleaned up local file: {local_path}\")\n",
        "\n",
        "if not uploaded_files:\n",
        "    print(\"❌ No files were successfully uploaded to S3\")\n",
        "else:\n",
        "    print(f\"\\n✅ Successfully uploaded {len(uploaded_files)} files to S3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e5161fb",
      "metadata": {
        "id": "4e5161fb"
      },
      "source": [
        "### Step 2: Create Source and Destination Connectors\n",
        "\n",
        "Now we'll create the connectors that tell the Unstructured API where to read documents from (S3)\n",
        "and where to write the processed results (MongoDB)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "769fcde2",
      "metadata": {
        "id": "769fcde2"
      },
      "source": [
        "#### Create S3 Source Connector\n",
        "\n",
        "The S3 source connector configures the Unstructured API to read documents from our S3 bucket.\n",
        "It will recursively scan the `workflow-docs/` folder for PDF files to process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c605504b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c605504b",
        "outputId": "98474d8a-f049-4eb8-ee99-e78606b7723f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔗 Step 2: Creating S3 source connector\n",
            "🔗 Creating S3 source connector...\n",
            "  ✅ Created S3 source connector: ae88e100-ea66-464f-bc98-1c80eec9decc\n",
            "📋 Source connector ID: ae88e100-ea66-464f-bc98-1c80eec9decc\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n🔗 Step 2: Creating S3 source connector\")\n",
        "source_id = create_s3_source_connector()\n",
        "if not source_id:\n",
        "    print(\"❌ Failed to create S3 source connector\")\n",
        "    print(\"Please check your AWS credentials and S3 bucket permissions\")\n",
        "else:\n",
        "    print(f\"📋 Source connector ID: {source_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d48e6a2",
      "metadata": {
        "id": "9d48e6a2"
      },
      "source": [
        "#### Create MongoDB Destination Connector\n",
        "\n",
        "The MongoDB destination connector configures where the processed document data will be stored.\n",
        "Documents will be saved to the `unstructured.workflow_documents` collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a2c34dae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2c34dae",
        "outputId": "bf3923f2-368e-42f7-8051-8762b0c1973e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔗 Creating MongoDB destination connector\n",
            "🔗 Creating MongoDB destination connector...\n",
            "  ✅ Created MongoDB destination connector: f7c9ad8c-a92d-4503-8509-20a864ceaf7a\n",
            "📋 Destination connector ID: f7c9ad8c-a92d-4503-8509-20a864ceaf7a\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n🔗 Creating MongoDB destination connector\")\n",
        "destination_id = create_mongodb_destination_connector()\n",
        "if not destination_id:\n",
        "    print(\"❌ Failed to create MongoDB destination connector\")\n",
        "    print(\"Please check your MongoDB connection string and permissions\")\n",
        "else:\n",
        "    print(f\"📋 Destination connector ID: {destination_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d712a61a",
      "metadata": {
        "id": "d712a61a"
      },
      "source": [
        "### Step 3: Create and Run Workflow\n",
        "\n",
        "Now we'll create a custom workflow that defines how documents should be processed.\n",
        "Our workflow includes three key processing nodes:\n",
        "\n",
        "1. **Partitioner (VLM)**: Uses a Vision Language Model to intelligently segment documents\n",
        "2. **Chunker**: Breaks documents into smaller, manageable pieces based on titles\n",
        "3. **Embedder**: Generates vector embeddings for semantic search capabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63a897b2",
      "metadata": {
        "id": "63a897b2"
      },
      "source": [
        "#### Create Custom Workflow\n",
        "\n",
        "The workflow connects our S3 source to our MongoDB destination through a series of processing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "cf43125f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf43125f",
        "outputId": "2c0f7a18-fa09-4d09-c8f5-d07b06bd7527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "⚙️ Step 3: Creating custom workflow\n",
            "⚙️ Creating advanced document processing workflow...\n",
            "  🧠 Configured VLM Partitioner: GPT-4o will analyze document layout, tables, and structure\n",
            "  ✂️ Configured Smart Chunker: Will create 1500-2048 character chunks based on document structure\n",
            "  🔢 Configured Vector Embedder: Will generate embeddings using text-embedding-3-small\n",
            "  📊 Workflow created successfully! Here's the configuration:\n",
            "{\n",
            "    \"created_at\": \"2025-06-06T20:15:52.826756Z\",\n",
            "    \"destinations\": [\n",
            "        \"f7c9ad8c-a92d-4503-8509-20a864ceaf7a\"\n",
            "    ],\n",
            "    \"id\": \"632392a7-5edb-4936-8754-15c0bb27e12b\",\n",
            "    \"name\": \"S3-to-MongoDB-RAG-Pipeline_1749240952\",\n",
            "    \"sources\": [\n",
            "        \"ae88e100-ea66-464f-bc98-1c80eec9decc\"\n",
            "    ],\n",
            "    \"status\": \"active\",\n",
            "    \"workflow_nodes\": [\n",
            "        {\n",
            "            \"name\": \"VLM_Partitioner\",\n",
            "            \"subtype\": \"vlm\",\n",
            "            \"type\": \"partition\",\n",
            "            \"id\": \"21a26d1b-15f9-4bc5-8975-a4d392df0631\",\n",
            "            \"settings\": {\n",
            "                \"provider\": \"openai\",\n",
            "                \"provider_api_key\": null,\n",
            "                \"model\": \"gpt-4o\",\n",
            "                \"output_format\": \"text/html\",\n",
            "                \"prompt\": null,\n",
            "                \"format_html\": true,\n",
            "                \"unique_element_ids\": true,\n",
            "                \"is_dynamic\": false,\n",
            "                \"allow_fast\": true\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Smart_Chunker\",\n",
            "            \"subtype\": \"chunk_by_title\",\n",
            "            \"type\": \"chunk\",\n",
            "            \"id\": \"45e1b5ae-83dc-47a7-bdbf-4aafb42aab71\",\n",
            "            \"settings\": {\n",
            "                \"unstructured_api_url\": null,\n",
            "                \"unstructured_api_key\": null,\n",
            "                \"multipage_sections\": false,\n",
            "                \"combine_text_under_n_chars\": null,\n",
            "                \"include_orig_elements\": false,\n",
            "                \"new_after_n_chars\": 1500,\n",
            "                \"max_characters\": 2048,\n",
            "                \"overlap\": 0,\n",
            "                \"overlap_all\": false,\n",
            "                \"contextual_chunking_strategy\": null\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Vector_Embedder\",\n",
            "            \"subtype\": \"openai\",\n",
            "            \"type\": \"embed\",\n",
            "            \"id\": \"82705036-6e42-49b5-8fd6-1194abdd6d24\",\n",
            "            \"settings\": {\n",
            "                \"model_name\": \"text-embedding-3-small\"\n",
            "            }\n",
            "        }\n",
            "    ],\n",
            "    \"reprocess_all\": false,\n",
            "    \"schedule\": {\n",
            "        \"crontab_entries\": []\n",
            "    },\n",
            "    \"updated_at\": \"2025-06-06T20:15:52.839009Z\",\n",
            "    \"workflow_type\": \"custom\"\n",
            "}\n",
            "  ✅ Workflow ID: 632392a7-5edb-4936-8754-15c0bb27e12b\n",
            "  🎯 This workflow will process documents through: VLM Partition → Smart Chunk → Vector Embed\n",
            "📋 Workflow ID: 632392a7-5edb-4936-8754-15c0bb27e12b\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n⚙️ Step 3: Creating custom workflow\")\n",
        "workflow_id = create_workflow(source_id, destination_id)\n",
        "if not workflow_id:\n",
        "    print(\"❌ Failed to create workflow\")\n",
        "    print(\"Please check your source and destination connector IDs\")\n",
        "else:\n",
        "    print(f\"📋 Workflow ID: {workflow_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8097b527",
      "metadata": {
        "id": "8097b527"
      },
      "source": [
        "#### Run the Workflow\n",
        "\n",
        "This starts the actual processing job. The workflow will:\n",
        "1. Read PDF files from the S3 source\n",
        "2. Process them through the partition → chunk → embed pipeline\n",
        "3. Store the results in MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "08fc06a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08fc06a7",
        "outputId": "5f0e85e3-1602-4e7d-f48b-8d66e54da671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 Running the workflow\n",
            "🚀 Running workflow...\n",
            "{\n",
            "    \"created_at\": \"2025-06-06T20:15:53.469900Z\",\n",
            "    \"id\": \"b48d36ee-9613-42c2-9e57-25c721b534f4\",\n",
            "    \"status\": \"SCHEDULED\",\n",
            "    \"workflow_id\": \"632392a7-5edb-4936-8754-15c0bb27e12b\",\n",
            "    \"workflow_name\": \"S3-to-MongoDB-RAG-Pipeline_1749240952\",\n",
            "    \"job_type\": \"ephemeral\"\n",
            "}\n",
            "  ✅ Started job: b48d36ee-9613-42c2-9e57-25c721b534f4\n",
            "📋 Job ID: b48d36ee-9613-42c2-9e57-25c721b534f4\n",
            "🔄 Job has been submitted and is now processing...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n🚀 Running the workflow\")\n",
        "job_id = run_workflow(workflow_id)\n",
        "if not job_id:\n",
        "    print(\"❌ Failed to run workflow\")\n",
        "    print(\"Please check the workflow configuration and try again\")\n",
        "else:\n",
        "    print(f\"📋 Job ID: {job_id}\")\n",
        "    print(\"🔄 Job has been submitted and is now processing...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6c2864a",
      "metadata": {
        "id": "a6c2864a"
      },
      "source": [
        "### Step 4: Monitor Job Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "65c7f550",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65c7f550",
        "outputId": "e07d647e-20e6-4620-d230-e1a568f92e02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "⏳ Step 4: Monitoring job progress\n",
            "⏳ Polling job status (checking every 30 seconds)...\n",
            "  📅 Job is scheduled, checking again in 30 seconds...\n",
            "  ⚙️ Job is in progress, checking again in 30 seconds...\n",
            "  ✅ Job completed successfully!\n",
            "{\n",
            "    \"created_at\": \"2025-06-06T20:15:53.469900\",\n",
            "    \"id\": \"b48d36ee-9613-42c2-9e57-25c721b534f4\",\n",
            "    \"status\": \"COMPLETED\",\n",
            "    \"workflow_id\": \"632392a7-5edb-4936-8754-15c0bb27e12b\",\n",
            "    \"workflow_name\": \"S3-to-MongoDB-RAG-Pipeline_1749240952\",\n",
            "    \"job_type\": \"ephemeral\",\n",
            "    \"runtime\": \"PT0S\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Poll job status\n",
        "print(\"\\n⏳ Step 4: Monitoring job progress\")\n",
        "job = poll_job_status(job_id)\n",
        "pretty_print_model(job)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a00aaef",
      "metadata": {
        "id": "5a00aaef"
      },
      "source": [
        "### Step 5: Verify Results and Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f263f40c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f263f40c",
        "lines_to_next_cell": 1,
        "outputId": "305c5ae9-e425-4d36-e1a1-aaedadd47b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Step 5: Verifying results\n",
            "🔍 Verifying MongoDB results...\n",
            "  📊 Total documents in MongoDB: 38\n",
            "  📄 Sample document keys: ['_id', 'type', 'element_id', 'text', 'metadata', 'embeddings', 'record_id']\n",
            "\n",
            "======================================================================\n",
            "📊 PIPELINE SUMMARY\n",
            "======================================================================\n",
            "📁 Files uploaded to S3: 1\n",
            "🔗 Source connector ID: ae88e100-ea66-464f-bc98-1c80eec9decc\n",
            "🔗 Destination connector ID: f7c9ad8c-a92d-4503-8509-20a864ceaf7a\n",
            "⚙️ Workflow ID: 632392a7-5edb-4936-8754-15c0bb27e12b\n",
            "🚀 Job ID: b48d36ee-9613-42c2-9e57-25c721b534f4\n",
            "📊 Documents in MongoDB: 38\n",
            "✅ Job Status: JobStatus.COMPLETED\n",
            "\n",
            "🎉 Pipeline completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Verify results\n",
        "print(\"\\n🔍 Step 5: Verifying results\")\n",
        "doc_count = verify_mongodb_results()\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"📊 PIPELINE SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"📁 Files uploaded to S3: {len(uploaded_files)}\")\n",
        "print(f\"🔗 Source connector ID: {source_id}\")\n",
        "print(f\"🔗 Destination connector ID: {destination_id}\")\n",
        "print(f\"⚙️ Workflow ID: {workflow_id}\")\n",
        "print(f\"🚀 Job ID: {job_id}\")\n",
        "print(f\"📊 Documents in MongoDB: {doc_count}\")\n",
        "print(f\"✅ Job Status: {job.status if job else 'Unknown'}\")\n",
        "\n",
        "if job and job.status == \"COMPLETED\" and doc_count > 0:\n",
        "    print(\"\\n🎉 Pipeline completed successfully!\")\n",
        "else:\n",
        "    print(\"\\n⚠️ Pipeline completed with issues. Check the logs above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86b50562",
      "metadata": {
        "id": "86b50562"
      },
      "source": [
        "### Step 8: Retrieve and Display Sample Data\n",
        "\n",
        "This final section demonstrates how to retrieve and examine the processed data from MongoDB.\n",
        "We'll pull a few sample documents and display their structure and content to verify\n",
        "the pipeline worked correctly and show what data is available for downstream applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f550be0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f550be0f",
        "outputId": "1731643a-b33f-4a06-a3e4-f5aaf2155b7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📋 Step 6: Retrieving and displaying sample data from MongoDB\n",
            "======================================================================\n",
            "📊 Total documents in collection: 38\n",
            "\n",
            "🔍 Retrieving 3 sample document(s):\n",
            "--------------------------------------------------\n",
            "\n",
            "📄 DOCUMENT 1:\n",
            "==============================\n",
            "🆔 Document ID: 68430e8cd622acefdaf51281\n",
            "📁 Filename: 616883802.pdf\n",
            "📄 File Type: application/pdf\n",
            "📏 Page Number: 1\n",
            "🏷️ Element Type: CompositeElement\n",
            "📝 Text Content: GENESEO\n",
            "\n",
            "IDEAS THAT MATTER\n",
            "\n",
            "2024-2025 Artificial Intelligence\n",
            "\n",
            "AI and Academic Integrity Module\n",
            "\n",
            "Prepared by Max Sparkman, Research Instruction Librarian, and Brandon West, Head of Research Instructio...\n",
            "   (Full length: 1336 characters)\n",
            "🔢 Embedding Vector: [-0.0129, 0.0192, ..., 0.0255]\n",
            "   (Vector dimension: 1536)\n",
            "🔧 Additional Fields: element_id, record_id\n",
            "--------------------------------------------------\n",
            "\n",
            "📄 DOCUMENT 2:\n",
            "==============================\n",
            "🆔 Document ID: 68430e8cd622acefdaf51282\n",
            "📁 Filename: 616883802.pdf\n",
            "📄 File Type: application/pdf\n",
            "📏 Page Number: 2\n",
            "🏷️ Element Type: CompositeElement\n",
            "📝 Text Content: What is Generative AI?\n",
            "\n",
            "Generative AI, or Generative Artificial Intelligence, refers to a class of artificial intelligence techniques and models that are designed to generate new content, such as text...\n",
            "   (Full length: 1878 characters)\n",
            "🔢 Embedding Vector: [0.0084, -0.0150, ..., 0.0386]\n",
            "   (Vector dimension: 1536)\n",
            "🔧 Additional Fields: element_id, record_id\n",
            "--------------------------------------------------\n",
            "\n",
            "📄 DOCUMENT 3:\n",
            "==============================\n",
            "🆔 Document ID: 68430e8cd622acefdaf51283\n",
            "📁 Filename: 616883802.pdf\n",
            "📄 File Type: application/pdf\n",
            "📏 Page Number: 4\n",
            "🏷️ Element Type: CompositeElement\n",
            "📝 Text Content: Generative AI and the Academic Dishonesty Policy\n",
            "\n",
            "Tools such as ChatGPT and other generative AI models are able to create new text with very little user input. While there are some more legitimate use...\n",
            "   (Full length: 1932 characters)\n",
            "🔢 Embedding Vector: [0.0027, -0.0090, ..., 0.0378]\n",
            "   (Vector dimension: 1536)\n",
            "🔧 Additional Fields: element_id, record_id\n",
            "--------------------------------------------------\n",
            "\n",
            "📈 COLLECTION STATISTICS:\n",
            "==============================\n",
            "📊 Documents by Type:\n",
            "   • CompositeElement: 38\n",
            "\n",
            "📊 Documents by Category:\n",
            "   • Unknown: 38\n",
            "\n",
            "🔢 Documents with embeddings: 38/38\n",
            "\n",
            "✅ Sample data retrieval completed successfully!\n"
          ]
        }
      ],
      "source": [
        "def retrieve_and_display_sample_data(limit=3):\n",
        "    \"\"\"\n",
        "    Retrieve and pretty print sample documents from MongoDB to demonstrate\n",
        "    the structure and content of processed documents.\n",
        "\n",
        "    Args:\n",
        "        limit (int): Number of sample documents to retrieve and display\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\n📋 Step 6: Retrieving and displaying sample data from MongoDB\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Connect to MongoDB\n",
        "        client = MongoClient(MONGO_URI, tls=True)\n",
        "        db = client[\"unstructured\"]\n",
        "        collection = db[\"workflow_documents\"]\n",
        "\n",
        "        # Get total count\n",
        "        total_count = collection.count_documents({})\n",
        "        print(f\"📊 Total documents in collection: {total_count}\")\n",
        "\n",
        "        if total_count == 0:\n",
        "            print(\"❌ No documents found in the collection\")\n",
        "            client.close()\n",
        "            return\n",
        "\n",
        "        # Retrieve sample documents\n",
        "        print(f\"\\n🔍 Retrieving {min(limit, total_count)} sample document(s):\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        sample_docs = list(collection.find().limit(limit))\n",
        "\n",
        "        for i, doc in enumerate(sample_docs, 1):\n",
        "            print(f\"\\n📄 DOCUMENT {i}:\")\n",
        "            print(\"=\" * 30)\n",
        "\n",
        "            # Display key document metadata\n",
        "            if '_id' in doc:\n",
        "                print(f\"🆔 Document ID: {doc['_id']}\")\n",
        "\n",
        "            if 'metadata' in doc and doc['metadata']:\n",
        "                metadata = doc['metadata']\n",
        "                print(f\"📁 Filename: {metadata.get('filename', 'N/A')}\")\n",
        "                print(f\"📄 File Type: {metadata.get('filetype', 'N/A')}\")\n",
        "                print(f\"📏 Page Number: {metadata.get('page_number', 'N/A')}\")\n",
        "\n",
        "            # Display element information\n",
        "            if 'type' in doc:\n",
        "                print(f\"🏷️ Element Type: {doc['type']}\")\n",
        "\n",
        "            if 'category' in doc:\n",
        "                print(f\"📂 Category: {doc['category']}\")\n",
        "\n",
        "            # Display text content (truncated for readability)\n",
        "            if 'text' in doc and doc['text']:\n",
        "                text = doc['text']\n",
        "                if len(text) > 200:\n",
        "                    print(f\"📝 Text Content: {text[:200]}...\")\n",
        "                    print(f\"   (Full length: {len(text)} characters)\")\n",
        "                else:\n",
        "                    print(f\"📝 Text Content: {text}\")\n",
        "\n",
        "            # Display embedding information if available\n",
        "            if 'embeddings' in doc and doc['embeddings']:\n",
        "                embedding = doc['embeddings']\n",
        "                if isinstance(embedding, list) and len(embedding) > 0:\n",
        "                    print(f\"🔢 Embedding Vector: [{embedding[0]:.4f}, {embedding[1]:.4f}, ..., {embedding[-1]:.4f}]\")\n",
        "                    print(f\"   (Vector dimension: {len(embedding)})\")\n",
        "\n",
        "            # Display any additional fields\n",
        "            other_fields = [k for k in doc.keys() if k not in ['_id', 'metadata', 'type', 'category', 'text', 'embeddings']]\n",
        "            if other_fields:\n",
        "                print(f\"🔧 Additional Fields: {', '.join(other_fields)}\")\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        # Display collection statistics\n",
        "        print(f\"\\n📈 COLLECTION STATISTICS:\")\n",
        "        print(\"=\" * 30)\n",
        "\n",
        "        # Count by document type\n",
        "        pipeline = [\n",
        "            {\"$group\": {\"_id\": \"$type\", \"count\": {\"$sum\": 1}}},\n",
        "            {\"$sort\": {\"count\": -1}}\n",
        "        ]\n",
        "        type_counts = list(collection.aggregate(pipeline))\n",
        "\n",
        "        if type_counts:\n",
        "            print(\"📊 Documents by Type:\")\n",
        "            for type_info in type_counts:\n",
        "                doc_type = type_info['_id'] or 'Unknown'\n",
        "                count = type_info['count']\n",
        "                print(f\"   • {doc_type}: {count}\")\n",
        "\n",
        "        # Count by category\n",
        "        pipeline = [\n",
        "            {\"$group\": {\"_id\": \"$category\", \"count\": {\"$sum\": 1}}},\n",
        "            {\"$sort\": {\"count\": -1}}\n",
        "        ]\n",
        "        category_counts = list(collection.aggregate(pipeline))\n",
        "\n",
        "        if category_counts:\n",
        "            print(\"\\n📊 Documents by Category:\")\n",
        "            for cat_info in category_counts:\n",
        "                category = cat_info['_id'] or 'Unknown'\n",
        "                count = cat_info['count']\n",
        "                print(f\"   • {category}: {count}\")\n",
        "\n",
        "        # Check for embeddings\n",
        "        docs_with_embeddings = collection.count_documents({\"embeddings\": {\"$exists\": True, \"$ne\": None}})\n",
        "        print(f\"\\n🔢 Documents with embeddings: {docs_with_embeddings}/{total_count}\")\n",
        "\n",
        "        client.close()\n",
        "        print(f\"\\n✅ Sample data retrieval completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error retrieving sample data: {e}\")\n",
        "        if 'client' in locals():\n",
        "            client.close()\n",
        "\n",
        "# Execute the sample data retrieval\n",
        "retrieve_and_display_sample_data(limit=3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
