{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "03935b29",
      "metadata": {
        "id": "03935b29"
      },
      "source": [
        "# S3 to MongoDB Pipeline using Unstructured Workflow Endpoint\n",
        "\n",
        "This notebook demonstrates a complete end-to-end document processing pipeline using the **Unstructured Workflow Endpoint**. The pipeline processes PDF documents from URLs and stores the structured results in MongoDB, showcasing the power and flexibility of the Unstructured platform for enterprise document processing.\n",
        "\n",
        "## What is the Unstructured Workflow Endpoint?\n",
        "\n",
        "The **Unstructured Workflow Endpoint** is a powerful cloud-based platform that enables organizations to build sophisticated document processing pipelines without managing infrastructure. Key features include:\n",
        "\n",
        "- **üîó Source Connectors**: Connect to various data sources (S3, Azure Blob, Google Drive, etc.)\n",
        "- **üéØ Destination Connectors**: Output processed data to databases, data warehouses, or vector stores\n",
        "- **‚öôÔ∏è Processing Nodes**: Modular components for partitioning, chunking, embedding, and more\n",
        "- **ü§ñ AI-Powered Processing**: Leverage advanced models like Vision Language Models (VLMs) for intelligent document understanding\n",
        "- **üìä Workflow Orchestration**: Chain multiple processing steps into sophisticated pipelines\n",
        "- **üîÑ Job Management**: Monitor, track, and manage processing jobs at scale\n",
        "\n",
        "## Pipeline Architecture Overview\n",
        "\n",
        "Our pipeline demonstrates a real-world RAG (Retrieval-Augmented Generation) preparation workflow:\n",
        "\n",
        "```\n",
        "PDF URLs ‚Üí Download ‚Üí S3 Upload ‚Üí Unstructured Workflow Endpoint ‚Üí MongoDB\n",
        "                                        ‚Üì\n",
        "                                  [Partition] ‚Üí [Chunk] ‚Üí [Embed]\n",
        "```\n",
        "\n",
        "### Processing Steps Explained:\n",
        "\n",
        "1. **üì• Document Acquisition**: Downloads PDF files from specified URLs to demonstrate batch processing\n",
        "2. **‚òÅÔ∏è Cloud Storage**: Uploads documents to Amazon S3, the source for our workflow\n",
        "3. **üîå Source Configuration**: Creates an S3 source connector that tells the API where to find documents\n",
        "4. **üéØ Destination Setup**: Configures a MongoDB destination connector for storing processed results\n",
        "5. **‚öôÔ∏è Workflow Creation**: Builds a custom processing pipeline with three powerful nodes:\n",
        "   - **üß† VLM Partitioner**: Uses GPT-4o Vision to intelligently segment documents, understanding layout, tables, images, and text hierarchy\n",
        "   - **‚úÇÔ∏è Smart Chunker**: Breaks documents into semantically meaningful chunks based on document structure (titles, sections)\n",
        "   - **üî¢ Vector Embedder**: Generates high-quality embeddings using OpenAI's text-embedding-3-small for semantic search\n",
        "6. **üöÄ Execution & Monitoring**: Runs the workflow and provides real-time job monitoring with detailed status updates\n",
        "7. **‚úÖ Verification**: Validates that processed documents are correctly stored in MongoDB with proper structure\n",
        "\n",
        "## Why Use Unstructured Workflow Endpoint?\n",
        "\n",
        "- **üèóÔ∏è No Infrastructure Management**: Focus on your use case, not on managing servers or scaling\n",
        "- **üéØ Production-Ready**: Built for enterprise scale with reliability and monitoring\n",
        "- **üîß Flexible Architecture**: Mix and match processing nodes to create custom workflows\n",
        "- **üìà Scalable**: Process thousands of documents efficiently\n",
        "- **ü§ñ AI-Native**: Leverage cutting-edge AI models for document understanding\n",
        "- **üîó Integration-Friendly**: Easy integration with existing data infrastructure\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, ensure you have:\n",
        "- **Unstructured API key** (sign up at [unstructured.io](https://unstructured.io))\n",
        "- **AWS credentials** with S3 read/write permissions\n",
        "- **MongoDB instance** (MongoDB Atlas)\n",
        "- **Python environment** with required packages installed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ca4d57",
      "metadata": {
        "id": "c1ca4d57"
      },
      "source": [
        "## Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7yDyQmJB4e5r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yDyQmJB4e5r",
        "outputId": "f465a9e8-f95b-4afd-97fd-42221e3d4967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (1.38.31)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.11/dist-packages (4.13.0)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.11/dist-packages (0.36.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (2025.4.26)\n",
            "Requirement already satisfied: botocore<1.39.0,>=1.38.31 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.38.31)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3) (0.13.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (24.1.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (43.0.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (2.11.5)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (5.6.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.31->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client) (0.4.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.31->boto3) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3 python-dotenv pymongo unstructured-client requests certifi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9899746e",
      "metadata": {
        "id": "9899746e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import uuid\n",
        "import requests\n",
        "import boto3\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "from pymongo import MongoClient\n",
        "from unstructured_client import UnstructuredClient\n",
        "from unstructured_client.models.operations import (\n",
        "    CreateSourceRequest,\n",
        "    CreateDestinationRequest\n",
        ")\n",
        "from unstructured_client.models.shared import (\n",
        "    CreateSourceConnector,\n",
        "    SourceConnectorType,\n",
        "    S3SourceConnectorConfigInput,\n",
        "    CreateDestinationConnector,\n",
        "    DestinationConnectorType,\n",
        "    MongoDBConnectorConfigInput,\n",
        "    WorkflowNode,\n",
        "    WorkflowType,\n",
        "    Schedule\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6238a53",
      "metadata": {
        "id": "c6238a53"
      },
      "source": [
        "## Configuration and Environment Setup\n",
        "\n",
        "This section loads environment variables and initializes the necessary clients for our pipeline.\n",
        "We'll validate all required credentials before proceeding with the workflow.\n",
        "\n",
        "The code below gives an example of how to hard-code your environment variables, which is totally sufficient for the purposes of this notebook. As an alternative to hard-coding, we suggest storing credentials as google colab user data and loading these credentials into the notebook (i.e. `os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')`). This is helpful if you plan on making commits to github and want to avoid accidentally commiting sensitive credentials to your git history. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "aea7a1e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aea7a1e6",
        "outputId": "dafe7d0d-0909-40da-8f7e-7ba787c6150b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All required configuration values set successfully\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# Configuration - Set these explicitly\n",
        "AWS_ACCESS_KEY_ID = \"your-access-key-id\"\n",
        "AWS_SECRET_ACCESS_KEY = \"your-secret-access-key\"\n",
        "AWS_REGION = \"your-region\"\n",
        "S3_BUCKET = \"your-s3-bucket-name\"\n",
        "UNSTRUCTURED_API_KEY = \"your-unstructured-api-key\"\n",
        "MONGO_URI = \"your-mongodb-connection-string\"\n",
        "AWS_SESSION_TOKEN = \"\"  # Optional: provide if using temporary credentials\n",
        "\n",
        "# Validation\n",
        "REQUIRED_VARS = {\n",
        "    \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n",
        "    \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n",
        "    \"AWS_REGION\": AWS_REGION,\n",
        "    \"S3_BUCKET\": S3_BUCKET,\n",
        "    \"UNSTRUCTURED_API_KEY\": UNSTRUCTURED_API_KEY,\n",
        "    \"MONGO_URI\": MONGO_URI\n",
        "}\n",
        "\n",
        "missing_vars = [key for key, value in REQUIRED_VARS.items() if not value]\n",
        "if missing_vars:\n",
        "    print(f\"‚ùå Missing required configuration values: {', '.join(missing_vars)}\")\n",
        "    print(\"Please update the script with the required information.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"‚úÖ All required configuration values set successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "936b5ed1",
      "metadata": {
        "id": "936b5ed1"
      },
      "source": [
        "### Initialize Clients and Test Connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bcefb211",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcefb211",
        "lines_to_next_cell": 1,
        "outputId": "882f5987-77e6-441e-8a62-f2a53f368447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Initializing Unstructured client...\n",
            "üîß Initializing AWS S3 client...\n",
            "üîß Testing MongoDB connection...\n",
            "‚úÖ MongoDB connection successful\n",
            "üéâ All clients initialized and connections tested successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create downloads directory for temporary file storage\n",
        "DOWNLOADS_DIR = \"downloads\"\n",
        "Path(DOWNLOADS_DIR).mkdir(exist_ok=True)\n",
        "\n",
        "# Initialize Unstructured client\n",
        "print(\"üîß Initializing Unstructured client...\")\n",
        "unstructured_client = UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY)\n",
        "\n",
        "# Helper function to pretty print API responses\n",
        "def pretty_print_model(response_model):\n",
        "    \"\"\"Pretty print model responses for better readability\"\"\"\n",
        "    print(response_model.model_dump_json(indent=4))\n",
        "\n",
        "# Initialize AWS S3 client\n",
        "print(\"üîß Initializing AWS S3 client...\")\n",
        "s3 = boto3.client(\n",
        "    \"s3\",\n",
        "    region_name=AWS_REGION,\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        ")\n",
        "\n",
        "# Test MongoDB connection to ensure it's accessible\n",
        "print(\"üîß Testing MongoDB connection...\")\n",
        "try:\n",
        "    mongo_client = MongoClient(MONGO_URI, tls=True)\n",
        "    mongo_client.admin.command('ping')\n",
        "    print(\"‚úÖ MongoDB connection successful\")\n",
        "    mongo_client.close()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå MongoDB connection failed: {e}\")\n",
        "    print(\"Please check your MONGO_URI and ensure MongoDB is accessible\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"üéâ All clients initialized and connections tested successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "754a6d79",
      "metadata": {
        "id": "754a6d79"
      },
      "source": [
        "## Utility Functions\n",
        "\n",
        "This section defines the core functions that power our document processing pipeline. Each function serves a specific purpose in the workflow and demonstrates different aspects of the Unstructured Workflow Endpoint.\n",
        "\n",
        "### Function Categories:\n",
        "\n",
        "1. **üì• Document Management Functions**\n",
        "   - `download_pdf()`: Downloads PDF files from URLs with robust error handling\n",
        "   - `upload_to_s3()`: Uploads files to S3 with proper organization and validation\n",
        "\n",
        "2. **üîå Connector Management Functions**\n",
        "   - `create_s3_source_connector()`: Creates and configures S3 source connectors\n",
        "   - `create_mongodb_destination_connector()`: Sets up MongoDB destination connectors\n",
        "\n",
        "3. **‚öôÔ∏è Workflow Orchestration Functions**\n",
        "   - `create_workflow()`: Builds custom processing workflows with multiple nodes\n",
        "   - `run_workflow()`: Executes workflows and returns job information\n",
        "\n",
        "4. **üìä Monitoring and Verification Functions**\n",
        "   - `poll_job_status()`: Monitors job progress with real-time status updates\n",
        "   - `verify_mongodb_results()`: Validates processed data in the destination database\n",
        "\n",
        "These functions showcase the flexibility and power of the Unstructured Workflow Endpoint, demonstrating how to build production-ready document processing pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2936469",
      "metadata": {
        "id": "a2936469"
      },
      "source": [
        "### Document Management Functions\n",
        "\n",
        "These functions handle the initial stages of our pipeline: acquiring documents and preparing them for processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9dacb818",
      "metadata": {
        "id": "9dacb818",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def download_pdf(url):\n",
        "    \"\"\"Download a PDF from a given URL with robust error handling.\"\"\"\n",
        "    try:\n",
        "        filename = url.split(\"/\")[-1]\n",
        "        if not filename.endswith('.pdf'):\n",
        "            filename += '.pdf'\n",
        "\n",
        "        local_path = os.path.join(DOWNLOADS_DIR, filename)\n",
        "\n",
        "        print(f\"  üì• Downloading {filename}...\")\n",
        "        response = requests.get(url, timeout=30, stream=True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Validate content type\n",
        "        content_type = response.headers.get('content-type', '').lower()\n",
        "        if 'pdf' not in content_type and 'application/octet-stream' not in content_type:\n",
        "            print(f\"  ‚ö†Ô∏è Warning: Expected PDF but got content-type: {content_type}\")\n",
        "\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        # Verify download success\n",
        "        if not os.path.exists(local_path) or os.path.getsize(local_path) == 0:\n",
        "            print(f\"  ‚ùå Downloaded file is empty or doesn't exist: {filename}\")\n",
        "            return None, None\n",
        "\n",
        "        file_size = os.path.getsize(local_path)\n",
        "        print(f\"  ‚úÖ Downloaded {filename} ({file_size:,} bytes)\")\n",
        "        return filename, local_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error downloading {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def upload_to_s3(local_path, filename):\n",
        "    \"\"\"Upload a file to S3 with error handling.\"\"\"\n",
        "    try:\n",
        "        s3_key = f\"workflow-docs/pdf/{filename}\"\n",
        "\n",
        "        if not os.path.exists(local_path):\n",
        "            print(f\"  ‚ùå Local file not found: {local_path}\")\n",
        "            return None\n",
        "\n",
        "        file_size = os.path.getsize(local_path)\n",
        "        s3.upload_file(local_path, S3_BUCKET, s3_key)\n",
        "        print(f\"  ‚úÖ Uploaded to S3: s3://{S3_BUCKET}/{s3_key} ({file_size:,} bytes)\")\n",
        "        return s3_key\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error uploading {filename} to S3: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2542bb56",
      "metadata": {
        "id": "2542bb56"
      },
      "source": [
        "### Connector Management Functions\n",
        "\n",
        "**Source and Destination Connectors** are fundamental components of the Unstructured Workflow Endpoint. They define where your data comes from and where processed results go.\n",
        "\n",
        "#### Understanding Connectors:\n",
        "- **Source Connectors**: Tell the API where to find your documents (S3, Azure Blob, Google Drive, etc.)\n",
        "- **Destination Connectors**: Define where processed data should be stored (MongoDB, Snowflake, Pinecone, etc.)\n",
        "- **Reusability**: Once created, connectors can be reused across multiple workflows\n",
        "- **Security**: Credentials are securely stored and managed by the Unstructured platform, which is SOC 2 Type II, ISO 27001, and HIPAA compliant, ensuring industry-standard protections for confidentiality, integrity, and availability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "33ef9f0b",
      "metadata": {
        "id": "33ef9f0b",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def create_s3_source_connector():\n",
        "    \"\"\"\n",
        "    Create an S3 source connector for the Unstructured Workflow Endpoint.\n",
        "\n",
        "    This function demonstrates how to configure the API to read documents from Amazon S3.\n",
        "    The connector will scan the specified S3 path for documents to process.\n",
        "\n",
        "    Key Configuration Options:\n",
        "    - remote_url: S3 path where documents are stored\n",
        "    - recursive: Whether to scan subdirectories\n",
        "    - credentials: AWS access keys for authentication\n",
        "    - region: AWS region for the S3 bucket\n",
        "\n",
        "    Returns:\n",
        "        str: Source connector ID if successful, None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"üîó Creating S3 source connector...\")\n",
        "\n",
        "        response = unstructured_client.sources.create_source(\n",
        "            request=CreateSourceRequest(\n",
        "                create_source_connector=CreateSourceConnector(\n",
        "                    name=f\"s3_source_connector_{int(time.time())}\",  # Unique name with timestamp\n",
        "                    type=SourceConnectorType.S3,  # Specify S3 as the source type\n",
        "                    config=S3SourceConnectorConfigInput(\n",
        "                        remote_url=f\"s3://{S3_BUCKET}/workflow-docs/\",  # S3 path to scan\n",
        "                        recursive=True,  # Scan subdirectories\n",
        "                        key=AWS_ACCESS_KEY_ID,  # AWS credentials\n",
        "                        secret=AWS_SECRET_ACCESS_KEY,\n",
        "                        region=AWS_REGION  # AWS region\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "        source_id = response.source_connector_information.id\n",
        "        print(f\"  ‚úÖ Created S3 source connector: {source_id}\")\n",
        "        return source_id\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error creating S3 source connector: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_mongodb_destination_connector():\n",
        "    \"\"\"\n",
        "    Create a MongoDB destination connector for the Unstructured Workflow Endpoint.\n",
        "\n",
        "    This function demonstrates how to configure the API to store processed documents\n",
        "    in MongoDB. The connector handles the structured output from the workflow nodes.\n",
        "\n",
        "    Key Configuration Options:\n",
        "    - uri: MongoDB connection string (supports MongoDB Atlas)\n",
        "    - database: Target database name\n",
        "    - collection: Target collection name\n",
        "    - batch_size: Number of documents to write in each batch (optimizes performance)\n",
        "\n",
        "    Returns:\n",
        "        str: Destination connector ID if successful, None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"üîó Creating MongoDB destination connector...\")\n",
        "\n",
        "        response = unstructured_client.destinations.create_destination(\n",
        "            request=CreateDestinationRequest(\n",
        "                create_destination_connector=CreateDestinationConnector(\n",
        "                    name=f\"mongodb_dest_connector_{int(time.time())}\",  # Unique name with timestamp\n",
        "                    type=DestinationConnectorType.MONGODB,  # Specify MongoDB as destination\n",
        "                    config=MongoDBConnectorConfigInput(\n",
        "                        uri=MONGO_URI,  # MongoDB connection string\n",
        "                        database=\"unstructured\",  # Target database\n",
        "                        collection=\"workflow_documents\",  # Target collection\n",
        "                        batch_size=20  # Batch size for optimal performance\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "        destination_id = response.destination_connector_information.id\n",
        "        print(f\"  ‚úÖ Created MongoDB destination connector: {destination_id}\")\n",
        "        return destination_id\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error creating MongoDB destination connector: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e0def4",
      "metadata": {
        "id": "31e0def4"
      },
      "source": [
        "### Workflow Orchestration Functions\n",
        "\n",
        "**Workflows** are the heart of the Unstructured platform. They define how documents are processed through a series of connected nodes, each performing specific operations on the data.\n",
        "\n",
        "#### Understanding Workflow Nodes:\n",
        "- **Partition Nodes**: Break documents into logical elements (paragraphs, tables, images)\n",
        "- **Chunk Nodes**: Split large elements into smaller, manageable pieces\n",
        "- **Embed Nodes**: Generate vector embeddings for semantic search\n",
        "- **Enrichment Nodes**: Apply custom transformations to the data such as summarization of text, images, and tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a8a25386",
      "metadata": {
        "id": "a8a25386",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def create_workflow(source_id, destination_id):\n",
        "    \"\"\"\n",
        "    Create a sophisticated document processing workflow using the Unstructured Workflow Endpoint.\n",
        "\n",
        "    This function demonstrates the power of the platform by creating a three-stage pipeline\n",
        "    that transforms raw PDFs into searchable, embedded document chunks ready for RAG applications.\n",
        "\n",
        "    Workflow Architecture:\n",
        "    PDF ‚Üí [VLM Partition] ‚Üí [Smart Chunk] ‚Üí [Vector Embed] ‚Üí MongoDB\n",
        "\n",
        "    Returns:\n",
        "        str: Workflow ID if successful, None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"‚öôÔ∏è Creating advanced document processing workflow...\")\n",
        "\n",
        "        # üß† STAGE 1: VLM Partitioner Node\n",
        "        # Uses GPT-4o Vision to intelligently understand document structure\n",
        "        partition_node = WorkflowNode(\n",
        "            name=\"VLM_Partitioner\",\n",
        "            subtype=\"vlm\",  # Vision Language Model partitioning\n",
        "            type=\"partition\",\n",
        "            settings={\n",
        "                \"provider\": \"openai\",  # Use OpenAI's models\n",
        "                \"model\": \"gpt-4o\",     # GPT-4o with vision capabilities\n",
        "            }\n",
        "        )\n",
        "        print(\"  üß† Configured VLM Partitioner: GPT-4o will analyze document layout, tables, and structure\")\n",
        "\n",
        "        # ‚úÇÔ∏è STAGE 2: Smart Chunker Node\n",
        "        # Breaks documents into semantically meaningful pieces based on titles/sections\n",
        "        chunk_node = WorkflowNode(\n",
        "            name='Smart_Chunker',\n",
        "            subtype='chunk_by_title',  # Chunk based on document structure\n",
        "            type=\"chunk\",\n",
        "            settings={\n",
        "                'new_after_n_chars': 1500,    # Start new chunk after 1500 characters\n",
        "                'max_characters': 2048,       # Maximum chunk size (optimal for embeddings)\n",
        "                'overlap': 0                  # No overlap between chunks\n",
        "            }\n",
        "        )\n",
        "        print(\"  ‚úÇÔ∏è Configured Smart Chunker: Will create 1500-2048 character chunks based on document structure\")\n",
        "\n",
        "        # üî¢ STAGE 3: Vector Embedder Node\n",
        "        # Generates high-quality embeddings for semantic search and RAG\n",
        "        embedder_node = WorkflowNode(\n",
        "            name='Vector_Embedder',\n",
        "            subtype='openai',  # Use OpenAI's embedding models\n",
        "            type=\"embed\",\n",
        "            settings={\n",
        "                'model_name': 'text-embedding-3-small'  # Latest OpenAI embedding model\n",
        "            }\n",
        "        )\n",
        "        print(\"  üî¢ Configured Vector Embedder: Will generate embeddings using text-embedding-3-small\")\n",
        "\n",
        "        # üîó Create the complete workflow\n",
        "        response = unstructured_client.workflows.create_workflow(\n",
        "            request={\n",
        "                \"create_workflow\": {\n",
        "                    \"name\": f\"S3-to-MongoDB-RAG-Pipeline_{int(time.time())}\",\n",
        "                    \"source_id\": source_id,\n",
        "                    \"destination_id\": destination_id,\n",
        "                    \"workflow_type\": WorkflowType.CUSTOM,  # Custom workflow with our specific nodes\n",
        "                    \"workflow_nodes\": [\n",
        "                        partition_node,  # Process documents with VLM\n",
        "                        chunk_node,     # Break into manageable pieces\n",
        "                        embedder_node   # Generate vector embeddings\n",
        "                    ],\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(\"  üìä Workflow created successfully! Here's the configuration:\")\n",
        "        pretty_print_model(response.workflow_information)\n",
        "\n",
        "        workflow_id = response.workflow_information.id\n",
        "        print(f\"  ‚úÖ Workflow ID: {workflow_id}\")\n",
        "        print(f\"  üéØ This workflow will process documents through: VLM Partition ‚Üí Smart Chunk ‚Üí Vector Embed\")\n",
        "        return workflow_id\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error creating workflow: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db1acced",
      "metadata": {
        "id": "db1acced"
      },
      "source": [
        "### Job Management and Monitoring Functions\n",
        "\n",
        "The Unstructured Workflow Endpoint provides comprehensive job management capabilities, allowing you to monitor progress, handle failures, and track processing statistics in real-time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5d628bae",
      "metadata": {
        "id": "5d628bae",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def run_workflow(workflow_id):\n",
        "    \"\"\"Run the workflow and return job information.\"\"\"\n",
        "    try:\n",
        "        print(\"üöÄ Running workflow...\")\n",
        "\n",
        "        res = unstructured_client.workflows.run_workflow(\n",
        "            request={\n",
        "                \"workflow_id\": workflow_id,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        pretty_print_model(res.job_information)\n",
        "        job_id = res.job_information.id\n",
        "        print(f\"  ‚úÖ Started job: {job_id}\")\n",
        "        return job_id\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error running workflow: {e}\")\n",
        "        return None\n",
        "\n",
        "def poll_job_status(job_id, wait_time=30):\n",
        "    \"\"\"Poll job status until completion.\"\"\"\n",
        "    print(f\"‚è≥ Polling job status (checking every {wait_time} seconds)...\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            response = unstructured_client.jobs.get_job(\n",
        "                request={\n",
        "                    \"job_id\": job_id\n",
        "                }\n",
        "            )\n",
        "\n",
        "            job = response.job_information\n",
        "            status = job.status\n",
        "\n",
        "            if status == \"SCHEDULED\":\n",
        "                print(f\"  üìÖ Job is scheduled, checking again in {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            elif status == \"IN_PROGRESS\":\n",
        "                print(f\"  ‚öôÔ∏è Job is in progress, checking again in {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            elif status == \"COMPLETED\":\n",
        "                print(f\"  ‚úÖ Job completed successfully!\")\n",
        "                return job\n",
        "            elif status == \"FAILED\":\n",
        "                print(f\"  ‚ùå Job failed!\")\n",
        "                return job\n",
        "            else:\n",
        "                print(f\"  ‚ùì Unknown job status: {status}\")\n",
        "                return job\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error polling job status: {e}\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "def verify_mongodb_results():\n",
        "    \"\"\"Verify that documents were stored in MongoDB.\"\"\"\n",
        "    try:\n",
        "        print(\"üîç Verifying MongoDB results...\")\n",
        "\n",
        "        client = MongoClient(MONGO_URI, tls=True)\n",
        "        db = client[\"unstructured\"]\n",
        "        collection = db[\"workflow_documents\"]\n",
        "\n",
        "        count = collection.count_documents({})\n",
        "        print(f\"  üìä Total documents in MongoDB: {count}\")\n",
        "\n",
        "        if count > 0:\n",
        "            # Show a sample document\n",
        "            sample = collection.find_one()\n",
        "            print(f\"  üìÑ Sample document keys: {list(sample.keys())}\")\n",
        "\n",
        "        client.close()\n",
        "        return count\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error verifying MongoDB results: {e}\")\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b303ae2",
      "metadata": {
        "id": "4b303ae2"
      },
      "source": [
        "## Main Pipeline Execution\n",
        "\n",
        "This section contains the main pipeline logic that orchestrates all the steps:\n",
        "1. Download PDFs and upload to S3\n",
        "2. Create source and destination connectors\n",
        "3. Create and run the workflow\n",
        "4. Monitor job progress\n",
        "5. Verify results in MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "79aca879",
      "metadata": {
        "id": "79aca879"
      },
      "outputs": [],
      "source": [
        "# List of PDF URLs to process\n",
        "pdf_urls = [\n",
        "    \"https://core.ac.uk/download/616883802.pdf\",\n",
        "    # Add more URLs as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a3328d3",
      "metadata": {
        "id": "0a3328d3"
      },
      "source": [
        "### Step 1: Download and Upload PDFs to S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2a7fd339",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a7fd339",
        "outputId": "e8ff668b-8084-484d-d5a4-134ada8d7d97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üì• Step 1: Downloading and uploading PDFs to S3\n",
            "\n",
            "üìÑ Processing: https://core.ac.uk/download/616883802.pdf\n",
            "  üì• Downloading 616883802.pdf...\n",
            "  ‚úÖ Downloaded 616883802.pdf (209,313 bytes)\n",
            "  ‚úÖ Uploaded to S3: s3://nvannest-quickstart-sample-data/workflow-docs/pdf/616883802.pdf (209,313 bytes)\n",
            "  üóëÔ∏è Cleaned up local file: downloads/616883802.pdf\n",
            "\n",
            "‚úÖ Successfully uploaded 1 files to S3\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Download and upload PDFs to S3\n",
        "print(\"\\nüì• Step 1: Downloading and uploading PDFs to S3\")\n",
        "uploaded_files = []\n",
        "\n",
        "for url in pdf_urls:\n",
        "    print(f\"\\nüìÑ Processing: {url}\")\n",
        "    filename, local_path = download_pdf(url)\n",
        "\n",
        "    if filename and local_path:\n",
        "        s3_key = upload_to_s3(local_path, filename)\n",
        "        if s3_key:\n",
        "            uploaded_files.append(s3_key)\n",
        "            # Clean up local file\n",
        "            os.remove(local_path)\n",
        "            print(f\"  üóëÔ∏è Cleaned up local file: {local_path}\")\n",
        "\n",
        "if not uploaded_files:\n",
        "    print(\"‚ùå No files were successfully uploaded to S3\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Successfully uploaded {len(uploaded_files)} files to S3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e5161fb",
      "metadata": {
        "id": "4e5161fb"
      },
      "source": [
        "### Step 2: Create Source and Destination Connectors\n",
        "\n",
        "Now we'll create the connectors that tell the Unstructured API where to read documents from (S3)\n",
        "and where to write the processed results (MongoDB)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "769fcde2",
      "metadata": {
        "id": "769fcde2"
      },
      "source": [
        "#### Create S3 Source Connector\n",
        "\n",
        "The S3 source connector configures the Unstructured API to read documents from our S3 bucket.\n",
        "It will recursively scan the `workflow-docs/` folder for PDF files to process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c605504b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c605504b",
        "outputId": "98474d8a-f049-4eb8-ee99-e78606b7723f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîó Step 2: Creating S3 source connector\n",
            "üîó Creating S3 source connector...\n",
            "  ‚úÖ Created S3 source connector: ae88e100-ea66-464f-bc98-1c80eec9decc\n",
            "üìã Source connector ID: ae88e100-ea66-464f-bc98-1c80eec9decc\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüîó Step 2: Creating S3 source connector\")\n",
        "source_id = create_s3_source_connector()\n",
        "if not source_id:\n",
        "    print(\"‚ùå Failed to create S3 source connector\")\n",
        "    print(\"Please check your AWS credentials and S3 bucket permissions\")\n",
        "else:\n",
        "    print(f\"üìã Source connector ID: {source_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d48e6a2",
      "metadata": {
        "id": "9d48e6a2"
      },
      "source": [
        "#### Create MongoDB Destination Connector\n",
        "\n",
        "The MongoDB destination connector configures where the processed document data will be stored.\n",
        "Documents will be saved to the `unstructured.workflow_documents` collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a2c34dae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2c34dae",
        "outputId": "bf3923f2-368e-42f7-8051-8762b0c1973e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîó Creating MongoDB destination connector\n",
            "üîó Creating MongoDB destination connector...\n",
            "  ‚úÖ Created MongoDB destination connector: f7c9ad8c-a92d-4503-8509-20a864ceaf7a\n",
            "üìã Destination connector ID: f7c9ad8c-a92d-4503-8509-20a864ceaf7a\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüîó Creating MongoDB destination connector\")\n",
        "destination_id = create_mongodb_destination_connector()\n",
        "if not destination_id:\n",
        "    print(\"‚ùå Failed to create MongoDB destination connector\")\n",
        "    print(\"Please check your MongoDB connection string and permissions\")\n",
        "else:\n",
        "    print(f\"üìã Destination connector ID: {destination_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d712a61a",
      "metadata": {
        "id": "d712a61a"
      },
      "source": [
        "### Step 3: Create and Run Workflow\n",
        "\n",
        "Now we'll create a custom workflow that defines how documents should be processed.\n",
        "Our workflow includes three key processing nodes:\n",
        "\n",
        "1. **Partitioner (VLM)**: Uses a Vision Language Model to intelligently segment documents\n",
        "2. **Chunker**: Breaks documents into smaller, manageable pieces based on titles\n",
        "3. **Embedder**: Generates vector embeddings for semantic search capabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63a897b2",
      "metadata": {
        "id": "63a897b2"
      },
      "source": [
        "#### Create Custom Workflow\n",
        "\n",
        "The workflow connects our S3 source to our MongoDB destination through a series of processing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "cf43125f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf43125f",
        "outputId": "2c0f7a18-fa09-4d09-c8f5-d07b06bd7527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚öôÔ∏è Step 3: Creating custom workflow\n",
            "‚öôÔ∏è Creating advanced document processing workflow...\n",
            "  üß† Configured VLM Partitioner: GPT-4o will analyze document layout, tables, and structure\n",
            "  ‚úÇÔ∏è Configured Smart Chunker: Will create 1500-2048 character chunks based on document structure\n",
            "  üî¢ Configured Vector Embedder: Will generate embeddings using text-embedding-3-small\n",
            "  üìä Workflow created successfully! Here's the configuration:\n",
            "{\n",
            "    \"created_at\": \"2025-06-06T20:15:52.826756Z\",\n",
            "    \"destinations\": [\n",
            "        \"f7c9ad8c-a92d-4503-8509-20a864ceaf7a\"\n",
            "    ],\n",
            "    \"id\": \"632392a7-5edb-4936-8754-15c0bb27e12b\",\n",
            "    \"name\": \"S3-to-MongoDB-RAG-Pipeline_1749240952\",\n",
            "    \"sources\": [\n",
            "        \"ae88e100-ea66-464f-bc98-1c80eec9decc\"\n",
            "    ],\n",
            "    \"status\": \"active\",\n",
            "    \"workflow_nodes\": [\n",
            "        {\n",
            "            \"name\": \"VLM_Partitioner\",\n",
            "            \"subtype\": \"vlm\",\n",
            "            \"type\": \"partition\",\n",
            "            \"id\": \"21a26d1b-15f9-4bc5-8975-a4d392df0631\",\n",
            "            \"settings\": {\n",
            "                \"provider\": \"openai\",\n",
            "                \"provider_api_key\": null,\n",
            "                \"model\": \"gpt-4o\",\n",
            "                \"output_format\": \"text/html\",\n",
            "                \"prompt\": null,\n",
            "                \"format_html\": true,\n",
            "                \"unique_element_ids\": true,\n",
            "                \"is_dynamic\": false,\n",
            "                \"allow_fast\": true\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Smart_Chunker\",\n",
            "            \"subtype\": \"chunk_by_title\",\n",
            "            \"type\": \"chunk\",\n",
            "            \"id\": \"45e1b5ae-83dc-47a7-bdbf-4aafb42aab71\",\n",
            "            \"settings\": {\n",
            "                \"unstructured_api_url\": null,\n",
            "                \"unstructured_api_key\": null,\n",
            "                \"multipage_sections\": false,\n",
            "                \"combine_text_under_n_chars\": null,\n",
            "                \"include_orig_elements\": false,\n",
            "                \"new_after_n_chars\": 1500,\n",
            "                \"max_characters\": 2048,\n",
            "                \"overlap\": 0,\n",
            "                \"overlap_all\": false,\n",
            "                \"contextual_chunking_strategy\": null\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Vector_Embedder\",\n",
            "            \"subtype\": \"openai\",\n",
            "            \"type\": \"embed\",\n",
            "            \"id\": \"82705036-6e42-49b5-8fd6-1194abdd6d24\",\n",
            "            \"settings\": {\n",
            "                \"model_name\": \"text-embedding-3-small\"\n",
            "            }\n",
            "        }\n",
            "    ],\n",
            "    \"reprocess_all\": false,\n",
            "    \"schedule\": {\n",
            "        \"crontab_entries\": []\n",
            "    },\n",
            "    \"updated_at\": \"2025-06-06T20:15:52.839009Z\",\n",
            "    \"workflow_type\": \"custom\"\n",
            "}\n",
            "  ‚úÖ Workflow ID: 632392a7-5edb-4936-8754-15c0bb27e12b\n",
            "  üéØ This workflow will process documents through: VLM Partition ‚Üí Smart Chunk ‚Üí Vector Embed\n",
            "üìã Workflow ID: 632392a7-5edb-4936-8754-15c0bb27e12b\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n‚öôÔ∏è Step 3: Creating custom workflow\")\n",
        "workflow_id = create_workflow(source_id, destination_id)\n",
        "if not workflow_id:\n",
        "    print(\"‚ùå Failed to create workflow\")\n",
        "    print(\"Please check your source and destination connector IDs\")\n",
        "else:\n",
        "    print(f\"üìã Workflow ID: {workflow_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8097b527",
      "metadata": {
        "id": "8097b527"
      },
      "source": [
        "#### Run the Workflow\n",
        "\n",
        "This starts the actual processing job. The workflow will:\n",
        "1. Read PDF files from the S3 source\n",
        "2. Process them through the partition ‚Üí chunk ‚Üí embed pipeline\n",
        "3. Store the results in MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "08fc06a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08fc06a7",
        "outputId": "5f0e85e3-1602-4e7d-f48b-8d66e54da671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Running the workflow\n",
            "üöÄ Running workflow...\n",
            "{\n",
            "    \"created_at\": \"2025-06-06T20:15:53.469900Z\",\n",
            "    \"id\": \"b48d36ee-9613-42c2-9e57-25c721b534f4\",\n",
            "    \"status\": \"SCHEDULED\",\n",
            "    \"workflow_id\": \"632392a7-5edb-4936-8754-15c0bb27e12b\",\n",
            "    \"workflow_name\": \"S3-to-MongoDB-RAG-Pipeline_1749240952\",\n",
            "    \"job_type\": \"ephemeral\"\n",
            "}\n",
            "  ‚úÖ Started job: b48d36ee-9613-42c2-9e57-25c721b534f4\n",
            "üìã Job ID: b48d36ee-9613-42c2-9e57-25c721b534f4\n",
            "üîÑ Job has been submitted and is now processing...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüöÄ Running the workflow\")\n",
        "job_id = run_workflow(workflow_id)\n",
        "if not job_id:\n",
        "    print(\"‚ùå Failed to run workflow\")\n",
        "    print(\"Please check the workflow configuration and try again\")\n",
        "else:\n",
        "    print(f\"üìã Job ID: {job_id}\")\n",
        "    print(\"üîÑ Job has been submitted and is now processing...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6c2864a",
      "metadata": {
        "id": "a6c2864a"
      },
      "source": [
        "### Step 4: Monitor Job Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "65c7f550",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65c7f550",
        "outputId": "e07d647e-20e6-4620-d230-e1a568f92e02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚è≥ Step 4: Monitoring job progress\n",
            "‚è≥ Polling job status (checking every 30 seconds)...\n",
            "  üìÖ Job is scheduled, checking again in 30 seconds...\n",
            "  ‚öôÔ∏è Job is in progress, checking again in 30 seconds...\n",
            "  ‚úÖ Job completed successfully!\n",
            "{\n",
            "    \"created_at\": \"2025-06-06T20:15:53.469900\",\n",
            "    \"id\": \"b48d36ee-9613-42c2-9e57-25c721b534f4\",\n",
            "    \"status\": \"COMPLETED\",\n",
            "    \"workflow_id\": \"632392a7-5edb-4936-8754-15c0bb27e12b\",\n",
            "    \"workflow_name\": \"S3-to-MongoDB-RAG-Pipeline_1749240952\",\n",
            "    \"job_type\": \"ephemeral\",\n",
            "    \"runtime\": \"PT0S\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Poll job status\n",
        "print(\"\\n‚è≥ Step 4: Monitoring job progress\")\n",
        "job = poll_job_status(job_id)\n",
        "pretty_print_model(job)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a00aaef",
      "metadata": {
        "id": "5a00aaef"
      },
      "source": [
        "### Step 5: Verify Results and Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f263f40c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f263f40c",
        "lines_to_next_cell": 1,
        "outputId": "305c5ae9-e425-4d36-e1a1-aaedadd47b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Step 5: Verifying results\n",
            "üîç Verifying MongoDB results...\n",
            "  üìä Total documents in MongoDB: 38\n",
            "  üìÑ Sample document keys: ['_id', 'type', 'element_id', 'text', 'metadata', 'embeddings', 'record_id']\n",
            "\n",
            "======================================================================\n",
            "üìä PIPELINE SUMMARY\n",
            "======================================================================\n",
            "üìÅ Files uploaded to S3: 1\n",
            "üîó Source connector ID: ae88e100-ea66-464f-bc98-1c80eec9decc\n",
            "üîó Destination connector ID: f7c9ad8c-a92d-4503-8509-20a864ceaf7a\n",
            "‚öôÔ∏è Workflow ID: 632392a7-5edb-4936-8754-15c0bb27e12b\n",
            "üöÄ Job ID: b48d36ee-9613-42c2-9e57-25c721b534f4\n",
            "üìä Documents in MongoDB: 38\n",
            "‚úÖ Job Status: JobStatus.COMPLETED\n",
            "\n",
            "üéâ Pipeline completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Verify results\n",
        "print(\"\\nüîç Step 5: Verifying results\")\n",
        "doc_count = verify_mongodb_results()\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä PIPELINE SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üìÅ Files uploaded to S3: {len(uploaded_files)}\")\n",
        "print(f\"üîó Source connector ID: {source_id}\")\n",
        "print(f\"üîó Destination connector ID: {destination_id}\")\n",
        "print(f\"‚öôÔ∏è Workflow ID: {workflow_id}\")\n",
        "print(f\"üöÄ Job ID: {job_id}\")\n",
        "print(f\"üìä Documents in MongoDB: {doc_count}\")\n",
        "print(f\"‚úÖ Job Status: {job.status if job else 'Unknown'}\")\n",
        "\n",
        "if job and job.status == \"COMPLETED\" and doc_count > 0:\n",
        "    print(\"\\nüéâ Pipeline completed successfully!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Pipeline completed with issues. Check the logs above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86b50562",
      "metadata": {
        "id": "86b50562"
      },
      "source": [
        "### Step 8: Retrieve and Display Sample Data\n",
        "\n",
        "This final section demonstrates how to retrieve and examine the processed data from MongoDB.\n",
        "We'll pull a few sample documents and display their structure and content to verify\n",
        "the pipeline worked correctly and show what data is available for downstream applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f550be0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f550be0f",
        "outputId": "1731643a-b33f-4a06-a3e4-f5aaf2155b7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã Step 6: Retrieving and displaying sample data from MongoDB\n",
            "======================================================================\n",
            "üìä Total documents in collection: 38\n",
            "\n",
            "üîç Retrieving 3 sample document(s):\n",
            "--------------------------------------------------\n",
            "\n",
            "üìÑ DOCUMENT 1:\n",
            "==============================\n",
            "üÜî Document ID: 68430e8cd622acefdaf51281\n",
            "üìÅ Filename: 616883802.pdf\n",
            "üìÑ File Type: application/pdf\n",
            "üìè Page Number: 1\n",
            "üè∑Ô∏è Element Type: CompositeElement\n",
            "üìù Text Content: GENESEO\n",
            "\n",
            "IDEAS THAT MATTER\n",
            "\n",
            "2024-2025 Artificial Intelligence\n",
            "\n",
            "AI and Academic Integrity Module\n",
            "\n",
            "Prepared by Max Sparkman, Research Instruction Librarian, and Brandon West, Head of Research Instructio...\n",
            "   (Full length: 1336 characters)\n",
            "üî¢ Embedding Vector: [-0.0129, 0.0192, ..., 0.0255]\n",
            "   (Vector dimension: 1536)\n",
            "üîß Additional Fields: element_id, record_id\n",
            "--------------------------------------------------\n",
            "\n",
            "üìÑ DOCUMENT 2:\n",
            "==============================\n",
            "üÜî Document ID: 68430e8cd622acefdaf51282\n",
            "üìÅ Filename: 616883802.pdf\n",
            "üìÑ File Type: application/pdf\n",
            "üìè Page Number: 2\n",
            "üè∑Ô∏è Element Type: CompositeElement\n",
            "üìù Text Content: What is Generative AI?\n",
            "\n",
            "Generative AI, or Generative Artificial Intelligence, refers to a class of artificial intelligence techniques and models that are designed to generate new content, such as text...\n",
            "   (Full length: 1878 characters)\n",
            "üî¢ Embedding Vector: [0.0084, -0.0150, ..., 0.0386]\n",
            "   (Vector dimension: 1536)\n",
            "üîß Additional Fields: element_id, record_id\n",
            "--------------------------------------------------\n",
            "\n",
            "üìÑ DOCUMENT 3:\n",
            "==============================\n",
            "üÜî Document ID: 68430e8cd622acefdaf51283\n",
            "üìÅ Filename: 616883802.pdf\n",
            "üìÑ File Type: application/pdf\n",
            "üìè Page Number: 4\n",
            "üè∑Ô∏è Element Type: CompositeElement\n",
            "üìù Text Content: Generative AI and the Academic Dishonesty Policy\n",
            "\n",
            "Tools such as ChatGPT and other generative AI models are able to create new text with very little user input. While there are some more legitimate use...\n",
            "   (Full length: 1932 characters)\n",
            "üî¢ Embedding Vector: [0.0027, -0.0090, ..., 0.0378]\n",
            "   (Vector dimension: 1536)\n",
            "üîß Additional Fields: element_id, record_id\n",
            "--------------------------------------------------\n",
            "\n",
            "üìà COLLECTION STATISTICS:\n",
            "==============================\n",
            "üìä Documents by Type:\n",
            "   ‚Ä¢ CompositeElement: 38\n",
            "\n",
            "üìä Documents by Category:\n",
            "   ‚Ä¢ Unknown: 38\n",
            "\n",
            "üî¢ Documents with embeddings: 38/38\n",
            "\n",
            "‚úÖ Sample data retrieval completed successfully!\n"
          ]
        }
      ],
      "source": [
        "def retrieve_and_display_sample_data(limit=3):\n",
        "    \"\"\"\n",
        "    Retrieve and pretty print sample documents from MongoDB to demonstrate\n",
        "    the structure and content of processed documents.\n",
        "\n",
        "    Args:\n",
        "        limit (int): Number of sample documents to retrieve and display\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\nüìã Step 6: Retrieving and displaying sample data from MongoDB\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Connect to MongoDB\n",
        "        client = MongoClient(MONGO_URI, tls=True)\n",
        "        db = client[\"unstructured\"]\n",
        "        collection = db[\"workflow_documents\"]\n",
        "\n",
        "        # Get total count\n",
        "        total_count = collection.count_documents({})\n",
        "        print(f\"üìä Total documents in collection: {total_count}\")\n",
        "\n",
        "        if total_count == 0:\n",
        "            print(\"‚ùå No documents found in the collection\")\n",
        "            client.close()\n",
        "            return\n",
        "\n",
        "        # Retrieve sample documents\n",
        "        print(f\"\\nüîç Retrieving {min(limit, total_count)} sample document(s):\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        sample_docs = list(collection.find().limit(limit))\n",
        "\n",
        "        for i, doc in enumerate(sample_docs, 1):\n",
        "            print(f\"\\nüìÑ DOCUMENT {i}:\")\n",
        "            print(\"=\" * 30)\n",
        "\n",
        "            # Display key document metadata\n",
        "            if '_id' in doc:\n",
        "                print(f\"üÜî Document ID: {doc['_id']}\")\n",
        "\n",
        "            if 'metadata' in doc and doc['metadata']:\n",
        "                metadata = doc['metadata']\n",
        "                print(f\"üìÅ Filename: {metadata.get('filename', 'N/A')}\")\n",
        "                print(f\"üìÑ File Type: {metadata.get('filetype', 'N/A')}\")\n",
        "                print(f\"üìè Page Number: {metadata.get('page_number', 'N/A')}\")\n",
        "\n",
        "            # Display element information\n",
        "            if 'type' in doc:\n",
        "                print(f\"üè∑Ô∏è Element Type: {doc['type']}\")\n",
        "\n",
        "            if 'category' in doc:\n",
        "                print(f\"üìÇ Category: {doc['category']}\")\n",
        "\n",
        "            # Display text content (truncated for readability)\n",
        "            if 'text' in doc and doc['text']:\n",
        "                text = doc['text']\n",
        "                if len(text) > 200:\n",
        "                    print(f\"üìù Text Content: {text[:200]}...\")\n",
        "                    print(f\"   (Full length: {len(text)} characters)\")\n",
        "                else:\n",
        "                    print(f\"üìù Text Content: {text}\")\n",
        "\n",
        "            # Display embedding information if available\n",
        "            if 'embeddings' in doc and doc['embeddings']:\n",
        "                embedding = doc['embeddings']\n",
        "                if isinstance(embedding, list) and len(embedding) > 0:\n",
        "                    print(f\"üî¢ Embedding Vector: [{embedding[0]:.4f}, {embedding[1]:.4f}, ..., {embedding[-1]:.4f}]\")\n",
        "                    print(f\"   (Vector dimension: {len(embedding)})\")\n",
        "\n",
        "            # Display any additional fields\n",
        "            other_fields = [k for k in doc.keys() if k not in ['_id', 'metadata', 'type', 'category', 'text', 'embeddings']]\n",
        "            if other_fields:\n",
        "                print(f\"üîß Additional Fields: {', '.join(other_fields)}\")\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        # Display collection statistics\n",
        "        print(f\"\\nüìà COLLECTION STATISTICS:\")\n",
        "        print(\"=\" * 30)\n",
        "\n",
        "        # Count by document type\n",
        "        pipeline = [\n",
        "            {\"$group\": {\"_id\": \"$type\", \"count\": {\"$sum\": 1}}},\n",
        "            {\"$sort\": {\"count\": -1}}\n",
        "        ]\n",
        "        type_counts = list(collection.aggregate(pipeline))\n",
        "\n",
        "        if type_counts:\n",
        "            print(\"üìä Documents by Type:\")\n",
        "            for type_info in type_counts:\n",
        "                doc_type = type_info['_id'] or 'Unknown'\n",
        "                count = type_info['count']\n",
        "                print(f\"   ‚Ä¢ {doc_type}: {count}\")\n",
        "\n",
        "        # Count by category\n",
        "        pipeline = [\n",
        "            {\"$group\": {\"_id\": \"$category\", \"count\": {\"$sum\": 1}}},\n",
        "            {\"$sort\": {\"count\": -1}}\n",
        "        ]\n",
        "        category_counts = list(collection.aggregate(pipeline))\n",
        "\n",
        "        if category_counts:\n",
        "            print(\"\\nüìä Documents by Category:\")\n",
        "            for cat_info in category_counts:\n",
        "                category = cat_info['_id'] or 'Unknown'\n",
        "                count = cat_info['count']\n",
        "                print(f\"   ‚Ä¢ {category}: {count}\")\n",
        "\n",
        "        # Check for embeddings\n",
        "        docs_with_embeddings = collection.count_documents({\"embeddings\": {\"$exists\": True, \"$ne\": None}})\n",
        "        print(f\"\\nüî¢ Documents with embeddings: {docs_with_embeddings}/{total_count}\")\n",
        "\n",
        "        client.close()\n",
        "        print(f\"\\n‚úÖ Sample data retrieval completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error retrieving sample data: {e}\")\n",
        "        if 'client' in locals():\n",
        "            client.close()\n",
        "\n",
        "# Execute the sample data retrieval\n",
        "retrieve_and_display_sample_data(limit=3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
