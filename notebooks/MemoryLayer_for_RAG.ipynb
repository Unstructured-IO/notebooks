{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Beyond Retrieval: Adding a Memory Layer to RAG with Unstructured and Mem0\n",
        "\n",
        "Your RAG system works. It retrieves the right documents, chunks them properly, and generates accurate answers. But here's the problem: it treats every user the same.\n",
        "\n",
        "Ask it to explain a complex concept, and it gives you the same technical explanation whether you're an expert or a beginner. Query it again the next day about a related topic, and it's forgotten your knowledge level and preferences.\n",
        "\n",
        "**RAG systems are great at retrieval, but they're terrible at personalization.**\n",
        "\n",
        "The issue isn't with the documents or the vector search—it's that RAG has no memory of who's asking. Every query starts from scratch. Users have to re-establish their preferences every single time.\n",
        "\n",
        "What if your RAG system could remember not just the documents it processes, but the users it serves? What if it could adapt its responses based on what it's learned about each user over time?\n",
        "\n",
        "**That's what we're building in this notebook.**\n",
        "\n",
        "This notebook implements the concepts from our companion blog post, [Beyond Retrieval: Adding a Memory Layer to RAG](link). If you haven't read it yet, we recommend starting there for a conceptual overview. Here, we'll focus on the hands-on implementation.\n",
        "\n",
        "We'll take Unstructured's document processing capabilities and layer in Mem0's intelligent memory system. The result? A RAG application that doesn't just retrieve information—it personalizes how that information gets delivered to each user.\n",
        "\n",
        "By the end of this walkthrough, you'll have built a system that:\n",
        "- Remembers user preferences across sessions\n",
        "- Adapts explanations to individual knowledge levels\n",
        "\n",
        "\n",
        "Throughout this notebook, we'll use research papers on attention mechanisms as our example documents, building an AI assistant that learns how each user prefers to learn.\n",
        "\n",
        "Let's dive in!"
      ],
      "metadata": {
        "id": "9Zg-Af-g4N3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlAIeUoPOXIL"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"unstructured-client\" mem0ai openai weaviate-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from unstructured_client import UnstructuredClient\n",
        "\n",
        "\n",
        "def pretty_print_model(response_model):\n",
        "    print(response_model.model_dump_json(indent=4))"
      ],
      "metadata": {
        "id": "RBgfdgoDy9AX"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can start processing documents, we need to authenticate with Unstructured's API.\n",
        "\n",
        "If you haven't already, sign up for a free [Unstructured account](https://unstructured.io/?modal=try-for-free). Once you're signed in, you can navigate to the API Keys section in the platform to generate your API key.\n",
        "\n",
        "Store this key in your Colab secrets as `UNSTRUCTURED_API_KEY`, which we'll use below to authenticate:\n",
        "\n"
      ],
      "metadata": {
        "id": "yXyZ4wBq4rc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"UNSTRUCTURED_API_KEY\"] = userdata.get(\"UNSTRUCTURED_API_KEY\")\n",
        "client = UnstructuredClient(api_key_auth=os.environ[\"UNSTRUCTURED_API_KEY\"])"
      ],
      "metadata": {
        "id": "TCf9PJJ7zOCt"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the S3 Source Connector\n",
        "\n",
        "Now that we have our Unstructured client ready, we need to tell it where to find our research paper.\n",
        "\n",
        "Source connectors in Unstructured define where your documents live. In this example, we're using Amazon S3 to store the research paper PDF, but Unstructured supports [many other sources](https://docs.unstructured.io/api-reference/workflow/sources/overview) like Google Drive, Azure Blob Storage, and more.\n",
        "\n",
        "**What you'll need:**\n",
        "\n",
        "- `AWS_ACCESS`: Your AWS access key ID\n",
        "- `AWS_SECRET`: Your AWS secret access key  \n",
        "- `S3_REMOTE_URL`: The S3 URI to your bucket or folder, formatted as `s3://your-bucket-name/` or `s3://your-bucket-name/folder-path/`\n",
        "\n",
        "Store these in your Colab secrets, and we'll use them to create a source connector that points to the research paper:"
      ],
      "metadata": {
        "id": "-e2PRlh7zTYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"AWS_ACCESS\"] = userdata.get(\"AWS_ACCESS\")\n",
        "os.environ[\"AWS_SECRET\"] = userdata.get(\"AWS_SECRET\")\n",
        "os.environ[\"S3_REMOTE_URL\"] = userdata.get(\"S3_REMOTE_URL\")"
      ],
      "metadata": {
        "id": "bAy8V4wozQ7B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured_client.models.operations import CreateSourceRequest\n",
        "from unstructured_client.models.shared import CreateSourceConnector\n",
        "\n",
        "source_connector_response = client.sources.create_source(\n",
        "    request=CreateSourceRequest(\n",
        "        create_source_connector=CreateSourceConnector(\n",
        "            name=\"Memory Layer Demo - Source\",\n",
        "            type=\"s3\",\n",
        "            config={\n",
        "\n",
        "                # For AWS access key ID with AWS secret access key authentication:\n",
        "                \"key\": os.environ[\"AWS_ACCESS\"],\n",
        "                \"secret\": os.environ[\"AWS_SECRET\"],\n",
        "\n",
        "                \"remote_url\": os.environ[\"S3_REMOTE_URL\"],\n",
        "                \"recursive\": True\n",
        "            }\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "pretty_print_model(source_connector_response.source_connector_information)"
      ],
      "metadata": {
        "id": "tpqQ3jCm00Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Weaviate Destination Connector\n",
        "\n",
        "Now we need to configure where our processed data will go. We're using Weaviate as our destination because it's a vector database, which is perfect for storing our document chunks and their embeddings for semantic search.\n",
        "\n",
        "For this notebook, we're using Weaviate Cloud (WCD), which offers a free tier and handles all the infrastructure for you.\n",
        "\n",
        "If you don't have a Weaviate instance yet, [create a WCD account here](https://console.weaviate.cloud/) and set up a cluster. You can also check out [Unstructured's Weaviate destination documentation](https://docs.unstructured.io/api-reference/workflow/destinations/weaviate) for additional configuration options.\n",
        "\n",
        "Once your cluster is ready, grab these credentials:\n",
        "\n",
        "- `WEAVIATE_CLUSTER_URL`: Your Weaviate Cloud cluster URL\n",
        "- `WEAVIATE_API_KEY`: The authentication key for your cluster  \n",
        "- `WEAVIATE_COLLECTION_NAME`: The name of the collection where data will be stored\n",
        "\n",
        "**Before creating the connector**, you need to set up your Weaviate collection with a minimum schema. Weaviate requires at least a `record_id` property before it can accept data. In the Weaviate UI, add this basic schema to your collection—Weaviate will automatically generate additional properties based on the incoming data from Unstructured.\n",
        "\n",
        "Store these credentials in your Colab secrets, and we'll use them to create the destination connector:"
      ],
      "metadata": {
        "id": "NlXyMlngzVC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WEAVIATE_CLUSTER_URL\"] = userdata.get(\"WEAVIATE_CLUSTER_URL\")\n",
        "os.environ[\"WEAVIATE_API_KEY\"] = userdata.get(\"WEAVIATE_API_KEY\")\n",
        "os.environ[\"WEAVIATE_COLLECTION_NAME\"] = userdata.get(\"WEAVIATE_COLLECTION_NAME\")"
      ],
      "metadata": {
        "id": "3Zn3Ac--M2VK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured_client.models.operations import CreateDestinationRequest\n",
        "from unstructured_client.models.shared import CreateDestinationConnector\n",
        "\n",
        "\n",
        "destination_connector_response = client.destinations.create_destination(\n",
        "        request=CreateDestinationRequest(\n",
        "            create_destination_connector=CreateDestinationConnector(\n",
        "                name=\"Memory Layer Demo - Destination\",\n",
        "                type=\"weaviate-cloud\",\n",
        "                config={\n",
        "                    \"cluster_url\": os.environ[\"WEAVIATE_CLUSTER_URL\"],\n",
        "                    \"collection\": os.environ[\"WEAVIATE_COLLECTION_NAME\"],\n",
        "                    \"api_key\": os.environ[\"WEAVIATE_API_KEY\"]\n",
        "                }\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "pretty_print_model(destination_connector_response.destination_connector_information)"
      ],
      "metadata": {
        "id": "P2-YVq_OOR9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Document Processing Workflow\n",
        "\n",
        "Now that we have our source (S3) and destination (Weaviate) configured, we need to define **how** the document should be processed. This is where Unstructured's workflow system shines.\n",
        "\n",
        "A workflow is a pipeline of processing nodes, where each node performs a specific transformation on the document. The data flows through these nodes sequentially, with each step building on the previous one.\n",
        "\n",
        "For our RAG system, we need to transform raw PDFs into structured, enriched, embedded chunks that can be semantically searched.\n",
        "\n",
        "Here's the pipeline we'll build for our research papers:\n",
        "\n",
        "**1. Partitioner** (`hi_res` strategy)\n",
        "   - Extracts structured content from the PDF\n",
        "   - Uses Object Detection Models and OCR for better accuracy for PDFs with embedded text\n",
        "\n",
        "**2. Image Summarizer** (OpenAI vision model)\n",
        "   - Generates descriptions for figures and diagrams\n",
        "   - Converts visual information into text descriptions that become part of the searchable content\n",
        "\n",
        "**3. Table Summarizer** (Anthropic Claude)\n",
        "   - Creates natural language summaries of data tables\n",
        "   - Makes structured data queryable in natural language\n",
        "\n",
        "**4. Chunker** (`chunk_by_title`)\n",
        "   - Breaks documents into semantically meaningful pieces\n",
        "   - Keeps related content together based on document structure (sections under the same heading stay in the same chunk)\n",
        "   - This preserves context better than arbitrary character splits\n",
        "\n",
        "**5. Embedder** (OpenAI text-embedding-3-large)\n",
        "   - Generates vector representations of each chunk\n",
        "   - These embeddings enable semantic search in Weaviate—finding relevant content based on meaning, not just keywords\n",
        "\n",
        "Each node plays a critical role in making our documents retrieval-ready. Once this workflow completes, your vector database will be populated with structured, enriched and embedded chunks.\n",
        "\n",
        "Let's define these nodes:"
      ],
      "metadata": {
        "id": "uCpNEAQuzXI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured_client.models.shared import (\n",
        "    WorkflowNode,\n",
        "    WorkflowType,\n",
        "    Schedule\n",
        ")\n",
        "\n",
        "partition_node = WorkflowNode(\n",
        "    name=\"Partitioner\",\n",
        "    subtype=\"unstructured_api\",\n",
        "    type=\"partition\",\n",
        "    settings={\n",
        "        \"strategy\": \"hi_res\",\n",
        "        \"extract_image_block_types\": [\"Image\", \"Table\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "image_summarizer_node = WorkflowNode(\n",
        "    name=\"Image summarizer\",\n",
        "    subtype=\"openai_image_description\",\n",
        "    type=\"prompter\",\n",
        "    settings={}\n",
        ")\n",
        "\n",
        "table_summarizer_node = WorkflowNode(\n",
        "    name=\"Table summarizer\",\n",
        "    subtype=\"anthropic_table_description\",\n",
        "    type=\"prompter\",\n",
        "    settings={}\n",
        ")\n",
        "\n",
        "chunk_node = WorkflowNode(\n",
        "    name=\"Chunker\",\n",
        "    subtype=\"chunk_by_title\",\n",
        "    type=\"chunk\",\n",
        "    settings={\n",
        "        \"new_after_n_chars\": 200,\n",
        "        \"max_characters\": 2048,\n",
        "        \"overlap\": 50,\n",
        "        \"combine_text_under_n_chars\": 200,\n",
        "        \"multipage_sections\": True,\n",
        "        \"include_orig_elements\": True,\n",
        "    }\n",
        ")\n",
        "\n",
        "embedder_node = WorkflowNode(\n",
        "    name='Embedder',\n",
        "    subtype='azure_openai',\n",
        "    type=\"embed\",\n",
        "    settings={\n",
        "        'model_name': 'text-embedding-3-large'\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "response = client.workflows.create_workflow(\n",
        "    request={\n",
        "        \"create_workflow\": {\n",
        "            \"name\": f\"RAG with Memory Layer Workflow\",\n",
        "            \"source_id\": source_connector_response.source_connector_information.id,\n",
        "            \"destination_id\": destination_connector_response.destination_connector_information.id,\n",
        "            \"workflow_type\": WorkflowType.CUSTOM,\n",
        "            \"workflow_nodes\": [\n",
        "                partition_node,\n",
        "                image_summarizer_node,\n",
        "                table_summarizer_node,\n",
        "                chunk_node,\n",
        "                embedder_node,\n",
        "\n",
        "            ],\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow_id = response.workflow_information.id\n",
        "pretty_print_model(response.workflow_information)"
      ],
      "metadata": {
        "id": "pta3l-FPzYce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8a8ec9-d246-444f-e908-617920897f93"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"created_at\": \"2025-10-22T13:35:55.149375Z\",\n",
            "    \"destinations\": [\n",
            "        \"1e5750a3-7990-4607-94cb-7b74b11e2010\"\n",
            "    ],\n",
            "    \"id\": \"40013e49-53d4-499f-bce0-583375b00ec4\",\n",
            "    \"name\": \"RAG with Memory Layer Workflow\",\n",
            "    \"sources\": [\n",
            "        \"9e3be798-dbc2-465a-8794-1cb3957b99e9\"\n",
            "    ],\n",
            "    \"status\": \"active\",\n",
            "    \"workflow_nodes\": [\n",
            "        {\n",
            "            \"name\": \"Table summarizer\",\n",
            "            \"subtype\": \"anthropic_table_description\",\n",
            "            \"type\": \"prompter\",\n",
            "            \"id\": \"18d0aad4-ef9b-4fde-8059-358ec4c9b327\",\n",
            "            \"settings\": {\n",
            "                \"model\": \"claude-sonnet-4-20250514\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Image summarizer\",\n",
            "            \"subtype\": \"openai_image_description\",\n",
            "            \"type\": \"prompter\",\n",
            "            \"id\": \"cce19b2e-f6b8-47fd-a455-1df715c43d00\",\n",
            "            \"settings\": {\n",
            "                \"model\": \"gpt-4o\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Embedder\",\n",
            "            \"subtype\": \"azure_openai\",\n",
            "            \"type\": \"embed\",\n",
            "            \"id\": \"239c6aa7-f8d4-4927-bb48-bbce751a8f41\",\n",
            "            \"settings\": {\n",
            "                \"model_name\": \"text-embedding-3-large\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Partitioner\",\n",
            "            \"subtype\": \"unstructured_api\",\n",
            "            \"type\": \"partition\",\n",
            "            \"id\": \"4a9ffd43-f063-4c4f-b113-9f35afe2ac0d\",\n",
            "            \"settings\": {\n",
            "                \"strategy\": \"hi_res\",\n",
            "                \"include_page_breaks\": false,\n",
            "                \"pdf_infer_table_structure\": false,\n",
            "                \"exclude_elements\": null,\n",
            "                \"xml_keep_tags\": false,\n",
            "                \"encoding\": \"utf-8\",\n",
            "                \"extract_image_block_types\": [\n",
            "                    \"Image\",\n",
            "                    \"Table\"\n",
            "                ]\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Chunker\",\n",
            "            \"subtype\": \"chunk_by_title\",\n",
            "            \"type\": \"chunk\",\n",
            "            \"id\": \"b536d219-9679-4a48-806d-e8f14ec14182\",\n",
            "            \"settings\": {\n",
            "                \"unstructured_api_url\": null,\n",
            "                \"unstructured_api_key\": null,\n",
            "                \"multipage_sections\": true,\n",
            "                \"combine_text_under_n_chars\": 200,\n",
            "                \"include_orig_elements\": true,\n",
            "                \"new_after_n_chars\": 200,\n",
            "                \"max_characters\": 2048,\n",
            "                \"overlap\": 50,\n",
            "                \"overlap_all\": false,\n",
            "                \"contextual_chunking_strategy\": null\n",
            "            }\n",
            "        }\n",
            "    ],\n",
            "    \"reprocess_all\": false,\n",
            "    \"schedule\": {\n",
            "        \"crontab_entries\": []\n",
            "    },\n",
            "    \"updated_at\": \"2025-10-22T13:35:55.161628Z\",\n",
            "    \"workflow_type\": \"custom\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the workflow\n",
        "\n",
        "Now that we've defined how we want to process our research paper, let's start the workflow and wait for it to complete:"
      ],
      "metadata": {
        "id": "CTrsjVhIqvTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = client.workflows.run_workflow(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id,\n",
        "    }\n",
        ")\n",
        "\n",
        "pretty_print_model(res.job_information)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuX5R8p37Src",
        "outputId": "3d51f6f5-2e0f-4282-8e64-af8993173276"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"created_at\": \"2025-10-22T13:35:56.439961Z\",\n",
            "    \"id\": \"fd00503c-1ff3-481e-ade2-0fcdfb99e569\",\n",
            "    \"status\": \"SCHEDULED\",\n",
            "    \"workflow_id\": \"40013e49-53d4-499f-bce0-583375b00ec4\",\n",
            "    \"workflow_name\": \"RAG with Memory Layer Workflow\",\n",
            "    \"job_type\": \"ephemeral\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.jobs.list_jobs(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id\n",
        "    }\n",
        ")\n",
        "\n",
        "last_job = response.response_list_jobs[0]\n",
        "job_id = last_job.id\n",
        "print(f\"job_id: {job_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxc3G6DS7WDR",
        "outputId": "b9267b4a-5392-40a0-cd12-a647a013a521"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "job_id: fd00503c-1ff3-481e-ade2-0fcdfb99e569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've created and started a job, we can poll Unstructured's `get_job` endpoint and check for its status every 30s till completion"
      ],
      "metadata": {
        "id": "A1zTfzanq4Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def poll_job_status(job_id, wait_time=30):\n",
        "    while True:\n",
        "        response = client.jobs.get_job(\n",
        "            request={\n",
        "                \"job_id\": job_id\n",
        "            }\n",
        "        )\n",
        "\n",
        "        job = response.job_information\n",
        "\n",
        "        if job.status == \"SCHEDULED\":\n",
        "            print(f\"Job is scheduled, polling again in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        elif job.status == \"IN_PROGRESS\":\n",
        "            print(f\"Job is in progress, polling again in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        else:\n",
        "            print(\"Job is completed\")\n",
        "            break\n",
        "\n",
        "    return job\n",
        "\n",
        "job = poll_job_status(job_id)\n",
        "pretty_print_model(job)"
      ],
      "metadata": {
        "id": "7-DJdmDQ7nki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470728cc-50aa-4939-d6e2-f3bf28427468"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job is scheduled, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is completed\n",
            "{\n",
            "    \"created_at\": \"2025-10-22T13:35:56.439961\",\n",
            "    \"id\": \"fd00503c-1ff3-481e-ade2-0fcdfb99e569\",\n",
            "    \"status\": \"COMPLETED\",\n",
            "    \"workflow_id\": \"40013e49-53d4-499f-bce0-583375b00ec4\",\n",
            "    \"workflow_name\": \"RAG with Memory Layer Workflow\",\n",
            "    \"job_type\": \"ephemeral\",\n",
            "    \"runtime\": \"PT0S\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we've completed all the foundational steps:\n",
        "\n",
        "- Extracted structured elements from raw documents using a **Partitioner**\n",
        "- Generated descriptions for images and tables using **Enrichments**\n",
        "- Organized the content into semantically meaningful chunks with a **Chunker**\n",
        "- Generated vector embeddings for those chunks through an **Embedder**\n",
        "\n",
        "Our processed data is now stored in Weaviate, ready for retrieval.\n",
        "\n",
        "Next, we'll build the query system and add a memory layer on top. This is where we transform a standard RAG pipeline into one that remembers and adapts to each user."
      ],
      "metadata": {
        "id": "Xm79CIx3UD3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying with Memory\n",
        "\n",
        "Your documents are now successfully processed and sitting in Weaviate. Now comes the interesting part: building a query system that doesn't just retrieve information, but learns about each user and adapts to them.\n",
        "\n",
        "This is where we add [**Mem0**](https://memo.ai/) to create a memory layer on top of our RAG system. You can follow along with the complete implementation in the companion Colab notebook.\n",
        "\n",
        "**The architecture is straightforward.** When a user asks a question, we:\n",
        "\n",
        "1. Generate an embedding for their query\n",
        "2. Search Weaviate for relevant document chunks\n",
        "3. Check Mem0 for what we know about this specific user\n",
        "4. Generate a response that's grounded in the documents but personalized to the user's preferences\n",
        "\n",
        "**The difference?** Without memory, every user gets the same explanation for the same question. With memory, a beginner gets a simple explanation with examples, while an expert gets a technical deep dive. A user who prefers bullet points gets bullet points. Someone who wants markdown formatting gets markdown. And none of them have to repeat these preferences.\n",
        "\n",
        "Let's set up our connections and build the retrieval functions."
      ],
      "metadata": {
        "id": "CO5YHJVpJdQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "from openai import OpenAI\n",
        "from mem0 import MemoryClient"
      ],
      "metadata": {
        "id": "87dGsmo4HEz3"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need a Mem0 account to add the memory layer to our RAG system.\n",
        "\n",
        "If you don't already have one, [create a Mem0 account here](https://app.mem0.ai/). Once you're logged in, navigate to the **API Keys** section in the dashboard and generate a new key, you'll need it to authenticate with Mem0's platform.\n",
        "\n",
        "Store this key in your Colab secrets as `MEM0_API_KEY`, which we'll use below:"
      ],
      "metadata": {
        "id": "XVgaqoefVKAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"MEM0_API_KEY\"] = userdata.get(\"MEM0_API_KEY\")"
      ],
      "metadata": {
        "id": "J_UCFol3Uh6V"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also need an OpenAI API key for generating query embeddings and powering the LLM responses. If you don't have one, you can get it from [OpenAI's platform](https://platform.openai.com/api-keys).\n"
      ],
      "metadata": {
        "id": "ljvMP5KOVaEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "HTq1Xbj6Veqp"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connecting to all clients"
      ],
      "metadata": {
        "id": "6LOkzQ0ZWrgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=os.environ[\"WEAVIATE_CLUSTER_URL\"],\n",
        "    auth_credentials=weaviate.auth.AuthApiKey(os.environ[\"WEAVIATE_API_KEY\"])\n",
        ")\n",
        "\n",
        "collection = weaviate_client.collections.get(os.environ[\"WEAVIATE_COLLECTION_NAME\"])\n",
        "\n",
        "memory = MemoryClient(\n",
        "    api_key=os.environ[\"MEM0_API_KEY\"],\n",
        ")\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
        "LLM_MODEL = \"gpt-4o\"\n",
        "TOP_K = 5\n",
        "USER_ID = \"tutorial_user\""
      ],
      "metadata": {
        "id": "xvGMSJpvMMHl"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we add memory, let's set up the core retrieval functions. These handle the standard RAG operations:\n",
        "- embedding queries,\n",
        "- searching Weaviate for similar chunks,\n",
        "- formatting the retrieved context.\n"
      ],
      "metadata": {
        "id": "6yujPoffXAKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text: str):\n",
        "    \"\"\"Generate embedding using OpenAI's text-embedding-3-large\"\"\"\n",
        "    response = openai_client.embeddings.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=text\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "\n",
        "def retrieve_from_weaviate(query: str, limit: int = TOP_K):\n",
        "    \"\"\"\n",
        "    Retrieve relevant chunks from Weaviate using vector search\n",
        "    Returns: list of dicts with 'content' and 'source' (filename)\n",
        "    \"\"\"\n",
        "    # Generate query embedding\n",
        "    query_vector = get_embedding(query)\n",
        "\n",
        "    # Vector search in Weaviate\n",
        "    results = collection.query.near_vector(\n",
        "        near_vector=query_vector,\n",
        "        limit=limit,\n",
        "        return_metadata=weaviate.classes.query.MetadataQuery(distance=True)\n",
        "    )\n",
        "\n",
        "    # Extract relevant info\n",
        "    retrieved_docs = []\n",
        "    sources = set()\n",
        "\n",
        "    for item in results.objects:\n",
        "        content = item.properties.get('text', '')\n",
        "\n",
        "        retrieved_docs.append({\n",
        "            'content': content,\n",
        "        })\n",
        "\n",
        "    return retrieved_docs\n",
        "\n",
        "\n",
        "def format_context(retrieved_docs):\n",
        "    \"\"\"Format retrieved documents into context string\"\"\"\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"\\n{doc['content']}\"\n",
        "        for doc in retrieved_docs\n",
        "    )\n",
        "    return context\n"
      ],
      "metadata": {
        "id": "pfzRhXAfNbjK"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by implementing a vanilla RAG query function consisting of just retrieval and generation. This serves as our baseline to demonstrate what changes when we add the memory layer.\n",
        "\n",
        "This function retrieves relevant chunks from Weaviate and generates an answer based purely on the retrieved context. Every user gets the same response for the same question."
      ],
      "metadata": {
        "id": "MDykQOmQXHYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_without_memory(question: str):\n",
        "    \"\"\"\n",
        "    Query the documents without using memory\n",
        "    Returns generic response based only on retrieved context\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    retrieved_docs = retrieve_from_weaviate(question)\n",
        "\n",
        "    context = format_context(retrieved_docs)\n",
        "\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=LLM_MODEL,\n",
        "        temperature=0.0,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful AI assistant. Answer the question based on the provided context.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content\n",
        "\n",
        "    print(f\"Answer:\\n{answer}\")\n",
        "    print(f\"\\n{'='*80}\\n\")\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1YigcML5N8ND"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1 = \"What are the key innovations in the Attention Is All You Need paper?\"\n",
        "response_q1 = query_without_memory(q1)\n",
        "\n",
        "q2 = \"How does sparse attention work compared to standard attention?\"\n",
        "response_q2_no_memory = query_without_memory(q2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiwS8A2CROev",
        "outputId": "8ce4e571-0db9-4b20-dc4c-ea999b0664ac"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Question: What are the key innovations in the Attention Is All You Need paper?\n",
            "================================================================================\n",
            "\n",
            "Answer:\n",
            "The \"Attention Is All You Need\" paper introduces several key innovations:\n",
            "\n",
            "1. **Transformer Architecture**: The paper proposes the Transformer model, which relies entirely on self-attention mechanisms, dispensing with the need for recurrent or convolutional neural networks. This allows for more parallelization and efficiency in training.\n",
            "\n",
            "2. **Self-Attention Mechanism**: The Transformer uses self-attention to compute representations of input and output sequences, enabling the model to draw global dependencies without regard to the distance between elements in the sequence.\n",
            "\n",
            "3. **Multi-Head Attention**: The paper introduces multi-head attention, which allows the model to focus on different parts of the input sequence simultaneously, improving the model's ability to capture complex patterns.\n",
            "\n",
            "4. **Positional Encoding**: To handle the order of sequences, the paper uses sinusoidal positional encodings, which help the model learn to attend by relative positions and potentially extrapolate to longer sequences than those seen during training.\n",
            "\n",
            "5. **Scalability and Efficiency**: The Transformer architecture allows for significant parallelization, reducing training time and computational resources compared to traditional recurrent models. The model achieves state-of-the-art results in machine translation tasks with less training time and resources.\n",
            "\n",
            "These innovations collectively contribute to the Transformer's superior performance in sequence transduction tasks, such as machine translation, compared to previous models.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Question: How does sparse attention work compared to standard attention?\n",
            "================================================================================\n",
            "\n",
            "Answer:\n",
            "Sparse attention works by reducing the number of connections or interactions between elements in the attention mechanism, compared to standard attention which typically considers all possible interactions. In standard attention, each element in a sequence can attend to every other element, leading to a quadratic scaling in time and memory with respect to the sequence length. This can be computationally expensive and inefficient, especially for long sequences.\n",
            "\n",
            "Sparse attention, on the other hand, introduces a structured pattern of sparsity, where only a subset of interactions are considered. This can be achieved through various methods, such as block-sparse patterns, strided patterns, or fixed patterns, which limit the number of elements each position attends to. By doing so, sparse attention reduces the number of computations and memory accesses required, leading to faster execution and lower memory usage.\n",
            "\n",
            "For example, block-sparse FlashAttention, as described in the context, uses a block-sparse pattern that allows it to perform fewer high-bandwidth memory (HBM) accesses compared to dense FlashAttention. This results in a speedup proportional to the sparsity of the pattern. Sparse attention can also be combined with other techniques, such as factorized attention, where different attention heads attend to different subsets of the input, further improving efficiency.\n",
            "\n",
            "Overall, sparse attention aims to maintain the performance benefits of attention mechanisms while reducing computational costs, making it particularly useful for handling long sequences in models like Transformers.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding the Memory Layer\n",
        "\n",
        "Now let's implement the memory-enabled version. This function works identically to our vanilla RAG but with one critical addition: it checks Mem0 for stored information about the user.\n",
        "\n",
        "**Here's how it works:**\n",
        "\n",
        "When `use_memory=True`, the function:\n",
        "\n",
        "1. **Retrieves from Weaviate** (standard RAG retrieval)\n",
        "2. **Queries Mem0** for any stored memories about this user\n",
        "3. **Injects user preferences** into the system prompt if memories exist\n",
        "4. **Generates the response** with both document context and user context\n",
        "5. **Updates memory** by sending the conversation to Mem0 with instructions to extract and store relevant preferences\n",
        "\n",
        "The key difference is in the system prompt. Without memory, every user gets: `\"You are a helpful AI assistant. Answer based on the provided context.\"`\n",
        "\n",
        "With memory, users get: `\"You are a helpful AI assistant. Answer based on the context and user preferences. User Preferences: [their specific preferences]\"`\n",
        "\n",
        "The `memory.add()` call at the end is what makes the system learn over time. We send the user messages to Mem0 with specific instructions about what to extract—format preferences, knowledge level, learning style and it uses its own LLM to parse and store that information for future queries.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CWDFkxxzYcP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_papers(question: str, use_memory: bool = False):\n",
        "    \"\"\"\n",
        "    Query papers with automatic memory management\n",
        "    \"\"\"\n",
        "    # Retrieve from Weaviate\n",
        "    retrieved_docs = retrieve_from_weaviate(question)\n",
        "\n",
        "    context = format_context(retrieved_docs)\n",
        "\n",
        "\n",
        "    # Build system prompt\n",
        "    if use_memory:\n",
        "        # Retrieve memories with filters\n",
        "        filters = {\"AND\": [{\"user_id\": USER_ID}]}\n",
        "        memory_response = memory.search(query=question, filters=filters, version=\"v2\")\n",
        "\n",
        "        memories = memory_response.get('results', [])\n",
        "\n",
        "        if len(memories) > 0:\n",
        "            memories_text = \"\\n\".join([f\"- {m.get('memory', '')}\" for m in memories])\n",
        "            system_prompt = f\"\"\"You are a helpful AI assistant. Answer based on the context and user preferences.\n",
        "\n",
        "User Preferences:\n",
        "{memories_text}\n",
        "\n",
        "\"\"\"\n",
        "        else:\n",
        "            system_prompt = \"You are a helpful AI assistant. Answer based on the provided context.\"\n",
        "    else:\n",
        "        system_prompt = \"You are a helpful AI assistant. Answer based on the provided context.\"\n",
        "\n",
        "    # Generate response\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=LLM_MODEL,\n",
        "        temperature=0.0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content\n",
        "\n",
        "    system_prompt = \"\"\"Extract and store ONLY: response output format preferences (which formatting the model response should be in), knowledge baseline (what user knows), and learning style.\n",
        "    Do NOT store anything from the questions user asks, store ONLY from preferences user EXPLICITLY states.\"\"\"\n",
        "\n",
        "\n",
        "    # Update memory with system instruction for Mem0's LLM\n",
        "    if use_memory:\n",
        "        memory.add(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": system_prompt\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": question},\n",
        "            ],\n",
        "            user_id=USER_ID,\n",
        "            metadata={\"category\": \"preferences\"}\n",
        "        )\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "3ufB5fGlcZLk"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the difference in action. We'll start by querying without memory to establish our baseline, then demonstrate how memory changes the interaction."
      ],
      "metadata": {
        "id": "JJCM0wGgZGQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q1: What is scaled dot-product attention and how does it work?\\n\")\n",
        "response_q1 = query_papers(q1, use_memory=False)\n",
        "print(f\"\\nAnswer:\\n{response_q1}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t_s2RBKUo60",
        "outputId": "ecc0809e-0748-4343-faee-b9a411c8de10"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: What is scaled dot-product attention and how does it work?\n",
            "\n",
            "\n",
            "Answer:\n",
            "The \"Attention Is All You Need\" paper introduces several key innovations:\n",
            "\n",
            "1. **Transformer Architecture**: The paper proposes the Transformer model, which relies entirely on attention mechanisms, dispensing with recurrence and convolution. This allows for more parallelization and efficiency in training compared to traditional RNN-based models.\n",
            "\n",
            "2. **Self-Attention Mechanism**: The Transformer uses self-attention to compute representations of input and output sequences. This mechanism relates different positions of a single sequence to compute a representation, enabling the model to capture dependencies regardless of their distance in the sequence.\n",
            "\n",
            "3. **Multi-Head Attention**: To counteract the reduced effective resolution due to averaging attention-weighted positions, the paper introduces Multi-Head Attention. This allows the model to focus on different parts of the sequence simultaneously, improving its ability to capture complex dependencies.\n",
            "\n",
            "4. **Positional Encoding**: Since the Transformer does not use recurrence, it incorporates positional encodings to provide information about the position of tokens in the sequence. The paper uses sinusoidal functions for positional encoding, which may allow the model to generalize to sequence lengths not seen during training.\n",
            "\n",
            "5. **Scalability and Efficiency**: The Transformer architecture allows for significant parallelization, reducing training time and computational resources needed. The model achieves state-of-the-art results in machine translation tasks with less training time compared to previous models.\n",
            "\n",
            "These innovations collectively contribute to the Transformer's superior performance and efficiency in sequence transduction tasks.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q2: How does sparse attention work compared to standard attention?\\n\")\n",
        "response_q2_no_memory = query_papers(q2, use_memory=False)\n",
        "print(f\"\\nAnswer:\\n{response_q2_no_memory}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkHpPkOAVlco",
        "outputId": "850543a7-eae2-404b-8528-47b7c350a497"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q2: How does sparse attention work compared to standard attention?\n",
            "\n",
            "\n",
            "Answer:\n",
            "Sparse attention works by reducing the number of connections or interactions between elements in the attention mechanism, compared to standard attention which typically considers all possible interactions. In standard attention, each element in a sequence can attend to every other element, leading to a quadratic scaling of time and memory with respect to the sequence length. This can be computationally expensive and inefficient, especially for long sequences.\n",
            "\n",
            "Sparse attention, on the other hand, introduces a sparsity pattern that limits the number of elements each position attends to. This can be achieved through various methods, such as:\n",
            "\n",
            "1. **Block-Sparse Patterns**: Dividing the attention matrix into blocks and only computing attention within certain blocks, as seen in block-sparse FlashAttention. This reduces the number of computations by focusing only on non-zero blocks.\n",
            "\n",
            "2. **Strided or Fixed Patterns**: Using fixed attention patterns where only specific positions are attended to, such as attending to every nth position or using a fixed set of positions for each element.\n",
            "\n",
            "3. **Factorized Attention**: Splitting the attention heads into separate groups, each attending to a subset of positions, which can be more efficient while still capturing important information.\n",
            "\n",
            "These sparse patterns reduce the number of high-bandwidth memory (HBM) accesses and computational load, leading to faster execution and lower memory usage. Sparse attention can still provide global context by carefully designing the sparsity pattern to ensure that important information is not lost, even though it does not consider all possible interactions like standard attention.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test the memory system. This time, we'll include explicit preferences in the query—format preferences, knowledge level, and learning style.\n",
        "\n",
        "When `use_memory=True`, Mem0 will automatically extract and store these preferences. Future queries won't need to repeat them."
      ],
      "metadata": {
        "id": "8ZAHwMt3Zks5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preference_question = \"\"\"I'd like my responses in markdown format. Explain concepts like I only know\n",
        "about the vanilla attention mechanism and nothing else. I'm willing to learn as long as there\n",
        "are comparisons and bridges to more complex topics from what I know already.\n",
        "\n",
        "Now answer: How does sparse attention work compared to standard attention?\"\"\"\n",
        "\n",
        "print(f\"User: {preference_question}\\n\")\n",
        "response_q2_with_memory = query_papers(preference_question, use_memory=True)\n",
        "print(f\"\\nAnswer:\\n{response_q2_with_memory}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNCDnbwtVnvX",
        "outputId": "2e6210ca-5901-4f2c-fc78-1b473b803155"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: I'd like my responses in markdown format. Explain concepts like I only know\n",
            "about the vanilla attention mechanism and nothing else. I'm willing to learn as long as there\n",
            "are comparisons and bridges to more complex topics from what I know already.\n",
            "\n",
            "Now answer: How does sparse attention work compared to standard attention?\n",
            "\n",
            "\n",
            "Answer:\n",
            "Certainly! Let's dive into the concept of sparse attention by building on your understanding of the vanilla attention mechanism.\n",
            "\n",
            "### Vanilla Attention Mechanism\n",
            "\n",
            "In the vanilla attention mechanism, every token in a sequence attends to every other token. This means that if you have a sequence of length \\( n \\), the attention mechanism computes interactions between all pairs of tokens, resulting in a time and memory complexity of \\( O(n^2) \\). This is because you need to calculate attention scores for each pair of tokens, which can become computationally expensive for long sequences.\n",
            "\n",
            "### Sparse Attention\n",
            "\n",
            "Sparse attention, on the other hand, aims to reduce the computational burden by limiting the number of interactions between tokens. Instead of every token attending to every other token, sparse attention introduces a pattern or structure that determines which tokens can attend to which others. This reduces the number of computations needed, leading to more efficient processing.\n",
            "\n",
            "#### Key Differences and Comparisons\n",
            "\n",
            "1. **Connectivity Pattern**:\n",
            "   - **Vanilla Attention**: Fully connected. Every token can attend to every other token.\n",
            "   - **Sparse Attention**: Partially connected. Only a subset of tokens attend to each other based on a predefined pattern.\n",
            "\n",
            "2. **Computational Complexity**:\n",
            "   - **Vanilla Attention**: \\( O(n^2) \\) due to the full pairwise interactions.\n",
            "   - **Sparse Attention**: Reduced complexity, often \\( O(n \\sqrt{n}) \\) or similar, depending on the sparsity pattern used.\n",
            "\n",
            "3. **Sparsity Patterns**:\n",
            "   - **Strided**: Tokens attend to others at regular intervals (e.g., every 2nd or 3rd token).\n",
            "   - **Fixed**: A predetermined pattern, such as a butterfly pattern, is used throughout the model.\n",
            "\n",
            "4. **Performance**:\n",
            "   - **Vanilla Attention**: Generally provides rich connectivity and can capture complex dependencies but at a high computational cost.\n",
            "   - **Sparse Attention**: Aims to maintain performance while reducing computational requirements. It may not capture all dependencies but can be nearly as effective with the right pattern.\n",
            "\n",
            "5. **Use Cases**:\n",
            "   - **Vanilla Attention**: Suitable for shorter sequences where computational resources are not a constraint.\n",
            "   - **Sparse Attention**: Ideal for longer sequences or when computational efficiency is crucial, such as in real-time applications or when working with limited hardware resources.\n",
            "\n",
            "### Bridging to More Complex Topics\n",
            "\n",
            "Sparse attention is a stepping stone to understanding more advanced techniques like block-sparse FlashAttention, which further optimizes the attention mechanism by using specific sparsity patterns (e.g., butterfly patterns) to achieve even greater efficiency. These methods are part of a broader effort to make Transformers more scalable and applicable to tasks involving very long sequences.\n",
            "\n",
            "By understanding sparse attention, you can appreciate how researchers are innovating to overcome the limitations of the vanilla attention mechanism, making it feasible to apply Transformers to a wider range of problems.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The response came back exactly as requested—markdown format, starting from basic attention concepts, with comparisons bridging to more complex ideas.\n",
        "\n",
        "Now let's test with a completely different question, without repeating any of those preferences.\n",
        "\n",
        "The below queries don't mention formatting preferences, knowledge level, or explanation styles. Let's see what happens:"
      ],
      "metadata": {
        "id": "J0sNgxwYatMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q3: What is Flash Attention?\\n\")\n",
        "q3 = \"What is Flash Attention?\"\n",
        "response_q3 = query_papers(q3, use_memory=True)\n",
        "print(f\"\\nAnswer:\\n{response_q3}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcdtoYuTXBEi",
        "outputId": "eaebad88-83de-4f8b-8327-5eb8b5049a46"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q3: What is Flash Attention?\n",
            "\n",
            "\n",
            "Answer:\n",
            "FlashAttention is an advanced attention algorithm designed to improve the efficiency of the attention mechanism in Transformers by reducing memory accesses, which are a primary bottleneck in computation speed on GPUs. Here's a breakdown of its key features and how it compares to the standard attention mechanism:\n",
            "\n",
            "### Key Features of FlashAttention\n",
            "\n",
            "1. **Memory Access Reduction**: \n",
            "   - FlashAttention focuses on minimizing the number of memory reads and writes, which are critical for performance due to the disparity between compute speed and memory speed on GPUs.\n",
            "   - By reducing memory accesses, FlashAttention not only speeds up computation but also reduces the total memory requirement.\n",
            "\n",
            "2. **Block Processing and Tiling**:\n",
            "   - The algorithm restructures attention computation by splitting the input into blocks and processing these blocks incrementally. This approach, known as tiling, allows for efficient softmax computation without needing access to the entire input at once.\n",
            "\n",
            "3. **Efficient Backward Pass**:\n",
            "   - FlashAttention simplifies the backward pass by analytically recomputing the attention matrix without storing large intermediate matrices, which reduces memory usage and increases speed.\n",
            "\n",
            "4. **Implementation in CUDA**:\n",
            "   - The algorithm is implemented in CUDA, allowing for fine-grained control over memory access and fusing all attention operations into a single GPU kernel.\n",
            "\n",
            "5. **Performance Improvements**:\n",
            "   - FlashAttention is significantly faster (up to 7.6x on GPT-2) and more memory-efficient than standard attention, especially for longer sequence lengths.\n",
            "\n",
            "### Comparison to Standard Attention\n",
            "\n",
            "- **Standard Attention**: Involves computing and storing large attention matrices, which leads to high memory access and storage requirements.\n",
            "- **FlashAttention**: Avoids storing these large matrices by recomputing necessary values on-the-fly, thus reducing memory access and improving speed.\n",
            "\n",
            "### Applications and Extensions\n",
            "\n",
            "- **Block-Sparse FlashAttention**: An extension that further improves performance by introducing sparsity, making it 2-4x faster than FlashAttention itself.\n",
            "- **Scalability**: FlashAttention enables Transformers to handle longer sequences, improving model quality and enabling new capabilities.\n",
            "\n",
            "### Empirical Validation\n",
            "\n",
            "- FlashAttention has been shown to speed up model training and improve model quality by allowing for longer context modeling. It also benchmarks favorably against other attention implementations in terms of runtime and memory footprint.\n",
            "\n",
            "In summary, FlashAttention is a memory-efficient and faster alternative to the standard attention mechanism, particularly beneficial for handling long sequences in Transformer models.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q4: Compare standard, sparse, and Flash Attention. What are the trade-offs?\\n\")\n",
        "q4 = \"Compare standard, sparse, and Flash Attention. What are the trade-offs?\"\n",
        "response_q4 = query_papers(q4, use_memory=True)\n",
        "print(f\"\\nAnswer:\\n{response_q4}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xovL5oNXdo1",
        "outputId": "6e6cfd49-51f0-4cc0-9986-ff5df021df17"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q4: Compare standard, sparse, and Flash Attention. What are the trade-offs?\n",
            "\n",
            "\n",
            "Answer:\n",
            "To compare standard, sparse, and FlashAttention, let's first establish a baseline understanding of the vanilla attention mechanism, which you are familiar with. Vanilla attention involves computing a weighted sum of values, where the weights are determined by the similarity between queries and keys. This process can be computationally intensive, especially for long sequences, due to the quadratic complexity in terms of sequence length.\n",
            "\n",
            "### Standard Attention\n",
            "\n",
            "- **Complexity**: Standard attention has a complexity of \\( \\Theta(N^2) \\) in terms of memory accesses, where \\( N \\) is the sequence length. This is because it computes a full attention matrix for all pairs of input tokens.\n",
            "- **Memory Usage**: It requires storing the entire attention matrix, which can be memory-intensive for long sequences.\n",
            "- **Speed**: The runtime grows quadratically with sequence length, making it slower for longer sequences.\n",
            "\n",
            "### Sparse Attention\n",
            "\n",
            "- **Complexity**: Sparse attention reduces the number of computations by only considering a subset of the attention matrix. This can lead to linear complexity in some implementations.\n",
            "- **Memory Usage**: By focusing on a subset of the attention matrix, sparse attention reduces memory usage compared to standard attention.\n",
            "- **Speed**: Sparse attention can be faster than standard attention, especially for long sequences, due to fewer computations. However, the speedup depends on the sparsity pattern and implementation.\n",
            "\n",
            "### FlashAttention\n",
            "\n",
            "- **Complexity**: FlashAttention significantly reduces the number of memory accesses to \\( \\Theta(N^2d^2M^{-1}) \\), where \\( d \\) is the head dimension and \\( M \\) is the size of SRAM. This is achieved by optimizing memory access patterns.\n",
            "- **Memory Usage**: It requires fewer memory accesses and has a smaller memory footprint compared to standard attention. FlashAttention is up to 20× more memory-efficient than standard attention.\n",
            "- **Speed**: FlashAttention is 2-4× faster than standard attention due to reduced memory accesses. It is also faster than many approximate and sparse attention methods for shorter sequences.\n",
            "- **Trade-offs**: While FlashAttention is highly efficient, it may not always outperform approximate methods for very long sequences. However, it provides a good balance of speed and memory efficiency for a wide range of sequence lengths.\n",
            "\n",
            "### Block-Sparse FlashAttention\n",
            "\n",
            "- **Complexity**: This variant further optimizes FlashAttention by introducing sparsity, leading to even fewer memory accesses.\n",
            "- **Memory Usage**: It maintains the same memory efficiency as FlashAttention but benefits from the sparsity to reduce computational overhead.\n",
            "- **Speed**: Block-sparse FlashAttention is faster than all known implementations of exact, sparse, and approximate attention, especially for very long sequences.\n",
            "\n",
            "### Trade-offs Summary\n",
            "\n",
            "- **Standard Attention**: Best for small sequences where memory and computational resources are not a constraint.\n",
            "- **Sparse Attention**: Useful for very long sequences where full attention is not feasible, but the sparsity pattern must be carefully chosen.\n",
            "- **FlashAttention**: Offers a balanced approach with significant speed and memory improvements for a wide range of sequence lengths.\n",
            "- **Block-Sparse FlashAttention**: Ideal for extremely long sequences, providing the best performance by leveraging sparsity.\n",
            "\n",
            "In summary, the choice between these methods depends on the specific requirements of your application, such as sequence length, available memory, and desired speed. FlashAttention and its block-sparse variant provide substantial improvements over standard attention, especially in terms of memory efficiency and speed.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The system remembered. Without repeating any preferences, the response automatically came back in markdown format, explained at a beginner level starting from attention mechanisms, and used comparisons throughout.\n",
        "\n",
        "The memory layer is working, Mem0 retrieved the stored preferences and applied them to these questions."
      ],
      "metadata": {
        "id": "Ax4ocVYabJV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check what's actually stored in the memory layer"
      ],
      "metadata": {
        "id": "DXG_3pSgdJyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final Memory State - What Mem0 Learned About You\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "filters = {\"AND\": [{\"user_id\": USER_ID}]}\n",
        "all_memories = memory.get_all(filters=filters)['results']\n",
        "print(f\"Total memories stored: {len(all_memories)}\\n\")\n",
        "for idx, m in enumerate(all_memories):\n",
        "    print(f\"{idx}. {m.get('memory', '')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M2On5mLXlar",
        "outputId": "49bd47f6-a89e-4d56-e7b5-a95bd4a53634"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Memory State - What Mem0 Learned About You\n",
            "================================================================================\n",
            "\n",
            "Total memories stored: 3\n",
            "\n",
            "0. User only knows vanilla attention mechanism\n",
            "1. User wants responses in markdown format\n",
            "2. User is willing to learn with comparisons and bridges to more complex topics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that what's stored is exactly what we instructed Mem0 to extract. The individual questions you asked (about sparse attention, Flash Attention, trade-offs) aren't in there. Those were just queries, not preferences.\n",
        "\n",
        "This is the key difference. The memory layer isn't recording your conversation history, it's distilling the patterns that matter for personalization.\n",
        "\n",
        "Here's what happened across the demo:\n",
        "\n",
        "**Q1 (baseline)**: Asked about the base paper without memory → generic response\n",
        "\n",
        "**Q2 (set preferences)**: Stated your preferences once in the query itself → Mem0 extracted and stored them\n",
        "\n",
        "**Q3, Q4**: Asked completely different questions without mentioning preferences → system automatically applied them every time\n",
        "\n",
        "The result? You stated your preferences once, and the system remembered them across multiple questions. No repetition needed.\n",
        "\n",
        "This is what makes memory-enabled RAG different from standard RAG. Same documents, same retrieval, same LLM but the delivery adapts to each user.\n",
        "\n",
        "**Ready to build your own memory-enabled RAG system?**\n",
        "\n",
        "Swap in your own documents, customize what memories to extract, and see how personalization changes your AI's responses.\n",
        "\n",
        "If you're working with enterprise data at scale, [sign up for Unstructured](https://unstructured.io/?modal=try-for-free) to access the full platform with workflow scheduling, multiple source connectors, and production-grade processing.\n"
      ],
      "metadata": {
        "id": "4BDgnQvadxGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q3HoT-8GZ8X6"
      },
      "execution_count": 134,
      "outputs": []
    }
  ]
}