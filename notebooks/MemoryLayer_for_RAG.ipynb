{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Beyond Retrieval: Adding a Memory Layer to RAG with Unstructured and Mem0\n",
        "\n",
        "Your RAG system works. It retrieves the right documents, chunks them properly, and generates accurate answers. But here's the problem: it treats every user the same.\n",
        "\n",
        "Ask it to explain a complex concept, and it gives you the same technical explanation whether you're an expert or a beginner. Query it again the next day about a related topic, and it's forgotten your knowledge level and preferences.\n",
        "\n",
        "**RAG systems are great at retrieval, but they're terrible at personalization.**\n",
        "\n",
        "The issue isn't with the documents or the vector search—it's that RAG has no memory of who's asking. Every query starts from scratch. Users have to re-establish their preferences every single time.\n",
        "\n",
        "What if your RAG system could remember not just the documents it processes, but the users it serves? What if it could adapt its responses based on what it's learned about each user over time?\n",
        "\n",
        "**That's what we're building in this notebook.**\n",
        "\n",
        "This notebook implements the concepts from our companion blog post, [Beyond Retrieval: Adding a Memory Layer to RAG](link). If you haven't read it yet, we recommend starting there for a conceptual overview. Here, we'll focus on the hands-on implementation.\n",
        "\n",
        "We'll take Unstructured's document processing capabilities and layer in Mem0's intelligent memory system. The result? A RAG application that doesn't just retrieve information—it personalizes how that information gets delivered to each user.\n",
        "\n",
        "By the end of this walkthrough, you'll have built a system that:\n",
        "- Remembers user preferences across sessions\n",
        "- Adapts explanations to individual knowledge levels\n",
        "\n",
        "\n",
        "Throughout this notebook, we'll use research papers on attention mechanisms as our example documents, building an AI assistant that learns how each user prefers to learn.\n",
        "\n",
        "Let's dive in!"
      ],
      "metadata": {
        "id": "9Zg-Af-g4N3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlAIeUoPOXIL"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"unstructured-client\" mem0ai openai weaviate-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from unstructured_client import UnstructuredClient\n",
        "\n",
        "\n",
        "def pretty_print_model(response_model):\n",
        "    print(response_model.model_dump_json(indent=4))"
      ],
      "metadata": {
        "id": "RBgfdgoDy9AX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can start processing documents, we need to authenticate with Unstructured's API.\n",
        "\n",
        "If you haven't already, sign up for a free [Unstructured account](https://unstructured.io/?modal=try-for-free). Once you're signed in, you can navigate to the API Keys section in the platform to generate your API key.\n",
        "\n",
        "Store this key in your Colab secrets as `UNSTRUCTURED_API_KEY`, which we'll use below to authenticate:\n",
        "\n"
      ],
      "metadata": {
        "id": "yXyZ4wBq4rc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"UNSTRUCTURED_API_KEY\"] = userdata.get(\"UNSTRUCTURED_API_KEY\")\n",
        "client = UnstructuredClient(api_key_auth=os.environ[\"UNSTRUCTURED_API_KEY\"])"
      ],
      "metadata": {
        "id": "TCf9PJJ7zOCt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the S3 Source Connector\n",
        "\n",
        "Now that we have our Unstructured client ready, we need to tell it where to find our research paper.\n",
        "\n",
        "Source connectors in Unstructured define where your documents live. In this example, we're using Amazon S3 to store the research paper PDF, but Unstructured supports [many other sources](https://docs.unstructured.io/api-reference/workflow/sources/overview) like Google Drive, Azure Blob Storage, and more.\n",
        "\n",
        "**What you'll need:**\n",
        "\n",
        "- `AWS_ACCESS`: Your AWS access key ID\n",
        "- `AWS_SECRET`: Your AWS secret access key  \n",
        "- `S3_REMOTE_URL`: The S3 URI to your bucket or folder, formatted as `s3://your-bucket-name/` or `s3://your-bucket-name/folder-path/`\n",
        "\n",
        "Store these in your Colab secrets, and we'll use them to create a source connector that points to the research paper:"
      ],
      "metadata": {
        "id": "-e2PRlh7zTYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"AWS_ACCESS\"] = userdata.get(\"AWS_ACCESS\")\n",
        "os.environ[\"AWS_SECRET\"] = userdata.get(\"AWS_SECRET\")\n",
        "os.environ[\"S3_REMOTE_URL\"] = userdata.get(\"S3_REMOTE_URL\")"
      ],
      "metadata": {
        "id": "bAy8V4wozQ7B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured_client.models.operations import CreateSourceRequest\n",
        "from unstructured_client.models.shared import CreateSourceConnector\n",
        "\n",
        "source_connector_response = client.sources.create_source(\n",
        "    request=CreateSourceRequest(\n",
        "        create_source_connector=CreateSourceConnector(\n",
        "            name=\"Memory Layer Demo - Source\",\n",
        "            type=\"s3\",\n",
        "            config={\n",
        "\n",
        "                # For AWS access key ID with AWS secret access key authentication:\n",
        "                \"key\": os.environ[\"AWS_ACCESS\"],\n",
        "                \"secret\": os.environ[\"AWS_SECRET\"],\n",
        "\n",
        "                \"remote_url\": os.environ[\"S3_REMOTE_URL\"],\n",
        "                \"recursive\": True\n",
        "            }\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "pretty_print_model(source_connector_response.source_connector_information)"
      ],
      "metadata": {
        "id": "tpqQ3jCm00Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Weaviate Destination Connector\n",
        "\n",
        "Now we need to configure where our processed data will go. We're using Weaviate as our destination because it's a vector database, which is perfect for storing our document chunks and their embeddings for semantic search.\n",
        "\n",
        "For this notebook, we're using Weaviate Cloud (WCD), which offers a free tier and handles all the infrastructure for you.\n",
        "\n",
        "If you don't have a Weaviate instance yet, [create a WCD account here](https://console.weaviate.cloud/) and set up a cluster. You can also check out [Unstructured's Weaviate destination documentation](https://docs.unstructured.io/api-reference/workflow/destinations/weaviate) for additional configuration options.\n",
        "\n",
        "Once your cluster is ready, grab these credentials:\n",
        "\n",
        "- `WEAVIATE_CLUSTER_URL`: Your Weaviate Cloud cluster URL\n",
        "- `WEAVIATE_API_KEY`: The authentication key for your cluster  \n",
        "- `WEAVIATE_COLLECTION_NAME`: The name of the collection where data will be stored\n",
        "\n",
        "**Before creating the connector**, you need to set up your Weaviate collection with a minimum schema. Weaviate requires at least a `record_id` property before it can accept data. In the Weaviate UI, add this basic schema to your collection—Weaviate will automatically generate additional properties based on the incoming data from Unstructured.\n",
        "\n",
        "Store these credentials in your Colab secrets, and we'll use them to create the destination connector:"
      ],
      "metadata": {
        "id": "NlXyMlngzVC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WEAVIATE_CLUSTER_URL\"] = userdata.get(\"WEAVIATE_CLUSTER_URL\")\n",
        "os.environ[\"WEAVIATE_API_KEY\"] = userdata.get(\"WEAVIATE_API_KEY\")\n",
        "os.environ[\"WEAVIATE_COLLECTION_NAME\"] = userdata.get(\"WEAVIATE_COLLECTION_NAME\")"
      ],
      "metadata": {
        "id": "3Zn3Ac--M2VK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured_client.models.operations import CreateDestinationRequest\n",
        "from unstructured_client.models.shared import CreateDestinationConnector\n",
        "\n",
        "\n",
        "destination_connector_response = client.destinations.create_destination(\n",
        "        request=CreateDestinationRequest(\n",
        "            create_destination_connector=CreateDestinationConnector(\n",
        "                name=\"Memory Layer Demo - Destination\",\n",
        "                type=\"weaviate-cloud\",\n",
        "                config={\n",
        "                    \"cluster_url\": os.environ[\"WEAVIATE_CLUSTER_URL\"],\n",
        "                    \"collection\": os.environ[\"WEAVIATE_COLLECTION_NAME\"],\n",
        "                    \"api_key\": os.environ[\"WEAVIATE_API_KEY\"]\n",
        "                }\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "pretty_print_model(destination_connector_response.destination_connector_information)"
      ],
      "metadata": {
        "id": "P2-YVq_OOR9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Document Processing Workflow\n",
        "\n",
        "Now that we have our source (S3) and destination (Weaviate) configured, we need to define **how** the document should be processed. This is where Unstructured's workflow system shines.\n",
        "\n",
        "A workflow is a pipeline of processing nodes, where each node performs a specific transformation on the document. The data flows through these nodes sequentially, with each step building on the previous one.\n",
        "\n",
        "For our RAG system, we need to transform raw PDFs into structured, enriched, embedded chunks that can be semantically searched.\n",
        "\n",
        "Here's the pipeline we'll build for our research papers:\n",
        "\n",
        "**1. Partitioner** (`hi_res` strategy)\n",
        "   - Extracts structured content from the PDF\n",
        "   - Uses Object Detection Models and OCR for better accuracy for PDFs with embedded text\n",
        "\n",
        "**2. Image Summarizer** (OpenAI vision model)\n",
        "   - Generates descriptions for figures and diagrams\n",
        "   - Converts visual information into text descriptions that become part of the searchable content\n",
        "\n",
        "**3. Table Summarizer** (Anthropic Claude)\n",
        "   - Creates natural language summaries of data tables\n",
        "   - Makes structured data queryable in natural language\n",
        "\n",
        "**4. Chunker** (`chunk_by_title`)\n",
        "   - Breaks documents into semantically meaningful pieces\n",
        "   - Keeps related content together based on document structure (sections under the same heading stay in the same chunk)\n",
        "   - This preserves context better than arbitrary character splits\n",
        "\n",
        "**5. Embedder** (OpenAI text-embedding-3-large)\n",
        "   - Generates vector representations of each chunk\n",
        "   - These embeddings enable semantic search in Weaviate—finding relevant content based on meaning, not just keywords\n",
        "\n",
        "Each node plays a critical role in making our documents retrieval-ready. Once this workflow completes, your vector database will be populated with structured, enriched and embedded chunks.\n",
        "\n",
        "Let's define these nodes:"
      ],
      "metadata": {
        "id": "uCpNEAQuzXI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured_client.models.shared import (\n",
        "    WorkflowNode,\n",
        "    WorkflowType,\n",
        "    Schedule\n",
        ")\n",
        "\n",
        "partition_node = WorkflowNode(\n",
        "    name=\"Partitioner\",\n",
        "    subtype=\"unstructured_api\",\n",
        "    type=\"partition\",\n",
        "    settings={\n",
        "        \"strategy\": \"hi_res\",\n",
        "        \"extract_image_block_types\": [\"Image\", \"Table\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "image_summarizer_node = WorkflowNode(\n",
        "    name=\"Image summarizer\",\n",
        "    subtype=\"openai_image_description\",\n",
        "    type=\"prompter\",\n",
        "    settings={}\n",
        ")\n",
        "\n",
        "table_summarizer_node = WorkflowNode(\n",
        "    name=\"Table summarizer\",\n",
        "    subtype=\"anthropic_table_description\",\n",
        "    type=\"prompter\",\n",
        "    settings={}\n",
        ")\n",
        "\n",
        "chunk_node = WorkflowNode(\n",
        "    name=\"Chunker\",\n",
        "    subtype=\"chunk_by_title\",\n",
        "    type=\"chunk\",\n",
        "    settings={\n",
        "        \"new_after_n_chars\": 200,\n",
        "        \"max_characters\": 2048,\n",
        "        \"overlap\": 50,\n",
        "        \"combine_text_under_n_chars\": 200,\n",
        "        \"multipage_sections\": True,\n",
        "        \"include_orig_elements\": True,\n",
        "    }\n",
        ")\n",
        "\n",
        "embedder_node = WorkflowNode(\n",
        "    name='Embedder',\n",
        "    subtype='azure_openai',\n",
        "    type=\"embed\",\n",
        "    settings={\n",
        "        'model_name': 'text-embedding-3-large'\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "response = client.workflows.create_workflow(\n",
        "    request={\n",
        "        \"create_workflow\": {\n",
        "            \"name\": f\"RAG with Memory Layer Workflow\",\n",
        "            \"source_id\": source_connector_response.source_connector_information.id,\n",
        "            \"destination_id\": destination_connector_response.destination_connector_information.id,\n",
        "            \"workflow_type\": WorkflowType.CUSTOM,\n",
        "            \"workflow_nodes\": [\n",
        "                partition_node,\n",
        "                image_summarizer_node,\n",
        "                table_summarizer_node,\n",
        "                chunk_node,\n",
        "                embedder_node,\n",
        "\n",
        "            ],\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow_id = response.workflow_information.id\n",
        "pretty_print_model(response.workflow_information)"
      ],
      "metadata": {
        "id": "pta3l-FPzYce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8a8ec9-d246-444f-e908-617920897f93"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"created_at\": \"2025-10-22T13:35:55.149375Z\",\n",
            "    \"destinations\": [\n",
            "        \"1e5750a3-7990-4607-94cb-7b74b11e2010\"\n",
            "    ],\n",
            "    \"id\": \"40013e49-53d4-499f-bce0-583375b00ec4\",\n",
            "    \"name\": \"RAG with Memory Layer Workflow\",\n",
            "    \"sources\": [\n",
            "        \"9e3be798-dbc2-465a-8794-1cb3957b99e9\"\n",
            "    ],\n",
            "    \"status\": \"active\",\n",
            "    \"workflow_nodes\": [\n",
            "        {\n",
            "            \"name\": \"Table summarizer\",\n",
            "            \"subtype\": \"anthropic_table_description\",\n",
            "            \"type\": \"prompter\",\n",
            "            \"id\": \"18d0aad4-ef9b-4fde-8059-358ec4c9b327\",\n",
            "            \"settings\": {\n",
            "                \"model\": \"claude-sonnet-4-20250514\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Image summarizer\",\n",
            "            \"subtype\": \"openai_image_description\",\n",
            "            \"type\": \"prompter\",\n",
            "            \"id\": \"cce19b2e-f6b8-47fd-a455-1df715c43d00\",\n",
            "            \"settings\": {\n",
            "                \"model\": \"gpt-4o\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Embedder\",\n",
            "            \"subtype\": \"azure_openai\",\n",
            "            \"type\": \"embed\",\n",
            "            \"id\": \"239c6aa7-f8d4-4927-bb48-bbce751a8f41\",\n",
            "            \"settings\": {\n",
            "                \"model_name\": \"text-embedding-3-large\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Partitioner\",\n",
            "            \"subtype\": \"unstructured_api\",\n",
            "            \"type\": \"partition\",\n",
            "            \"id\": \"4a9ffd43-f063-4c4f-b113-9f35afe2ac0d\",\n",
            "            \"settings\": {\n",
            "                \"strategy\": \"hi_res\",\n",
            "                \"include_page_breaks\": false,\n",
            "                \"pdf_infer_table_structure\": false,\n",
            "                \"exclude_elements\": null,\n",
            "                \"xml_keep_tags\": false,\n",
            "                \"encoding\": \"utf-8\",\n",
            "                \"extract_image_block_types\": [\n",
            "                    \"Image\",\n",
            "                    \"Table\"\n",
            "                ]\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Chunker\",\n",
            "            \"subtype\": \"chunk_by_title\",\n",
            "            \"type\": \"chunk\",\n",
            "            \"id\": \"b536d219-9679-4a48-806d-e8f14ec14182\",\n",
            "            \"settings\": {\n",
            "                \"unstructured_api_url\": null,\n",
            "                \"unstructured_api_key\": null,\n",
            "                \"multipage_sections\": true,\n",
            "                \"combine_text_under_n_chars\": 200,\n",
            "                \"include_orig_elements\": true,\n",
            "                \"new_after_n_chars\": 200,\n",
            "                \"max_characters\": 2048,\n",
            "                \"overlap\": 50,\n",
            "                \"overlap_all\": false,\n",
            "                \"contextual_chunking_strategy\": null\n",
            "            }\n",
            "        }\n",
            "    ],\n",
            "    \"reprocess_all\": false,\n",
            "    \"schedule\": {\n",
            "        \"crontab_entries\": []\n",
            "    },\n",
            "    \"updated_at\": \"2025-10-22T13:35:55.161628Z\",\n",
            "    \"workflow_type\": \"custom\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the workflow\n",
        "\n",
        "Now that we've defined how we want to process our research paper, let's start the workflow and wait for it to complete:"
      ],
      "metadata": {
        "id": "CTrsjVhIqvTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = client.workflows.run_workflow(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id,\n",
        "    }\n",
        ")\n",
        "\n",
        "pretty_print_model(res.job_information)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuX5R8p37Src",
        "outputId": "3d51f6f5-2e0f-4282-8e64-af8993173276"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"created_at\": \"2025-10-22T13:35:56.439961Z\",\n",
            "    \"id\": \"fd00503c-1ff3-481e-ade2-0fcdfb99e569\",\n",
            "    \"status\": \"SCHEDULED\",\n",
            "    \"workflow_id\": \"40013e49-53d4-499f-bce0-583375b00ec4\",\n",
            "    \"workflow_name\": \"RAG with Memory Layer Workflow\",\n",
            "    \"job_type\": \"ephemeral\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.jobs.list_jobs(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id\n",
        "    }\n",
        ")\n",
        "\n",
        "last_job = response.response_list_jobs[0]\n",
        "job_id = last_job.id\n",
        "print(f\"job_id: {job_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxc3G6DS7WDR",
        "outputId": "b9267b4a-5392-40a0-cd12-a647a013a521"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "job_id: fd00503c-1ff3-481e-ade2-0fcdfb99e569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've created and started a job, we can poll Unstructured's `get_job` endpoint and check for its status every 30s till completion"
      ],
      "metadata": {
        "id": "A1zTfzanq4Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def poll_job_status(job_id, wait_time=30):\n",
        "    while True:\n",
        "        response = client.jobs.get_job(\n",
        "            request={\n",
        "                \"job_id\": job_id\n",
        "            }\n",
        "        )\n",
        "\n",
        "        job = response.job_information\n",
        "\n",
        "        if job.status == \"SCHEDULED\":\n",
        "            print(f\"Job is scheduled, polling again in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        elif job.status == \"IN_PROGRESS\":\n",
        "            print(f\"Job is in progress, polling again in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        else:\n",
        "            print(\"Job is completed\")\n",
        "            break\n",
        "\n",
        "    return job\n",
        "\n",
        "job = poll_job_status(job_id)\n",
        "pretty_print_model(job)"
      ],
      "metadata": {
        "id": "7-DJdmDQ7nki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470728cc-50aa-4939-d6e2-f3bf28427468"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job is scheduled, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is completed\n",
            "{\n",
            "    \"created_at\": \"2025-10-22T13:35:56.439961\",\n",
            "    \"id\": \"fd00503c-1ff3-481e-ade2-0fcdfb99e569\",\n",
            "    \"status\": \"COMPLETED\",\n",
            "    \"workflow_id\": \"40013e49-53d4-499f-bce0-583375b00ec4\",\n",
            "    \"workflow_name\": \"RAG with Memory Layer Workflow\",\n",
            "    \"job_type\": \"ephemeral\",\n",
            "    \"runtime\": \"PT0S\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we've completed all the foundational steps:\n",
        "\n",
        "- Extracted structured elements from raw documents using a **Partitioner**\n",
        "- Generated descriptions for images and tables using **Enrichments**\n",
        "- Organized the content into semantically meaningful chunks with a **Chunker**\n",
        "- Generated vector embeddings for those chunks through an **Embedder**\n",
        "\n",
        "Our processed data is now stored in Weaviate, ready for retrieval.\n",
        "\n",
        "Next, we'll build the query system and add a memory layer on top. This is where we transform a standard RAG pipeline into one that remembers and adapts to each user."
      ],
      "metadata": {
        "id": "Xm79CIx3UD3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying with Memory\n",
        "\n",
        "Your documents are now successfully processed and sitting in Weaviate. Now comes the interesting part: building a query system that doesn't just retrieve information, but learns about each user and adapts to them.\n",
        "\n",
        "This is where we add [**Mem0**](https://memo.ai/) to create a memory layer on top of our RAG system. You can follow along with the complete implementation in the companion Colab notebook.\n",
        "\n",
        "**The architecture is straightforward.** When a user asks a question, we:\n",
        "\n",
        "1. Generate an embedding for their query\n",
        "2. Search Weaviate for relevant document chunks\n",
        "3. Check Mem0 for what we know about this specific user\n",
        "4. Generate a response that's grounded in the documents but personalized to the user's preferences\n",
        "\n",
        "**The difference?** Without memory, every user gets the same explanation for the same question. With memory, a beginner gets a simple explanation with examples, while an expert gets a technical deep dive. A user who prefers bullet points gets bullet points. Someone who wants markdown formatting gets markdown. And none of them have to repeat these preferences.\n",
        "\n",
        "Let's set up our connections and build the retrieval functions."
      ],
      "metadata": {
        "id": "CO5YHJVpJdQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "from openai import OpenAI\n",
        "from mem0 import MemoryClient"
      ],
      "metadata": {
        "id": "87dGsmo4HEz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae352366-15d1-44cb-b52c-6e9f19321ab2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need a Mem0 account to add the memory layer to our RAG system.\n",
        "\n",
        "If you don't already have one, [create a Mem0 account here](https://app.mem0.ai/). Once you're logged in, navigate to the **API Keys** section in the dashboard and generate a new key, you'll need it to authenticate with Mem0's platform.\n",
        "\n",
        "Store this key in your Colab secrets as `MEM0_API_KEY`, which we'll use below:"
      ],
      "metadata": {
        "id": "XVgaqoefVKAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"MEM0_API_KEY\"] = userdata.get(\"MEM0_API_KEY\")"
      ],
      "metadata": {
        "id": "J_UCFol3Uh6V"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also need an OpenAI API key for generating query embeddings and powering the LLM responses. If you don't have one, you can get it from [OpenAI's platform](https://platform.openai.com/api-keys).\n"
      ],
      "metadata": {
        "id": "ljvMP5KOVaEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "HTq1Xbj6Veqp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connecting to all clients"
      ],
      "metadata": {
        "id": "6LOkzQ0ZWrgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=os.environ[\"WEAVIATE_CLUSTER_URL\"],\n",
        "    auth_credentials=weaviate.auth.AuthApiKey(os.environ[\"WEAVIATE_API_KEY\"])\n",
        ")\n",
        "\n",
        "collection = weaviate_client.collections.get(os.environ[\"WEAVIATE_COLLECTION_NAME\"])\n",
        "\n",
        "memory = MemoryClient(\n",
        "    api_key=os.environ[\"MEM0_API_KEY\"],\n",
        ")\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
        "LLM_MODEL = \"gpt-4o\"\n",
        "TOP_K = 5\n",
        "USER_ID = \"tutorial_user\""
      ],
      "metadata": {
        "id": "xvGMSJpvMMHl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we add memory, let's set up the core retrieval functions. These handle the standard RAG operations:\n",
        "- embedding queries,\n",
        "- searching Weaviate for similar chunks,\n",
        "- formatting the retrieved context.\n"
      ],
      "metadata": {
        "id": "6yujPoffXAKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text: str):\n",
        "    \"\"\"Generate embedding using OpenAI's text-embedding-3-large\"\"\"\n",
        "    response = openai_client.embeddings.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=text\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "\n",
        "def retrieve_from_weaviate(query: str, limit: int = TOP_K):\n",
        "    \"\"\"\n",
        "    Retrieve relevant chunks from Weaviate using vector search\n",
        "    Returns: list of dicts with 'content' and 'source' (filename)\n",
        "    \"\"\"\n",
        "    # Generate query embedding\n",
        "    query_vector = get_embedding(query)\n",
        "\n",
        "    # Vector search in Weaviate\n",
        "    results = collection.query.near_vector(\n",
        "        near_vector=query_vector,\n",
        "        limit=limit,\n",
        "        return_metadata=weaviate.classes.query.MetadataQuery(distance=True)\n",
        "    )\n",
        "\n",
        "    # Extract relevant info\n",
        "    retrieved_docs = []\n",
        "    sources = set()\n",
        "\n",
        "    for item in results.objects:\n",
        "        content = item.properties.get('text', '')\n",
        "\n",
        "        retrieved_docs.append({\n",
        "            'content': content,\n",
        "        })\n",
        "\n",
        "    return retrieved_docs\n",
        "\n",
        "\n",
        "def format_context(retrieved_docs):\n",
        "    \"\"\"Format retrieved documents into context string\"\"\"\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"\\n{doc['content']}\"\n",
        "        for doc in retrieved_docs\n",
        "    )\n",
        "    return context\n"
      ],
      "metadata": {
        "id": "pfzRhXAfNbjK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by implementing a vanilla RAG query function consisting of just retrieval and generation. This serves as our baseline to demonstrate what changes when we add the memory layer.\n",
        "\n",
        "This function retrieves relevant chunks from Weaviate and generates an answer based purely on the retrieved context. Every user gets the same response for the same question."
      ],
      "metadata": {
        "id": "MDykQOmQXHYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_without_memory(question: str):\n",
        "    \"\"\"\n",
        "    Query the documents without using memory\n",
        "    Returns generic response based only on retrieved context\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    retrieved_docs = retrieve_from_weaviate(question)\n",
        "\n",
        "    context = format_context(retrieved_docs)\n",
        "\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=LLM_MODEL,\n",
        "        temperature=0.3,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful AI assistant. Answer the question based on the provided context.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content\n",
        "\n",
        "    print(f\"Answer:\\n{answer}\")\n",
        "    print(f\"\\n{'='*80}\\n\")\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1YigcML5N8ND"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1 = \"What are the key innovations in the Attention Is All You Need paper?\"\n",
        "response_q1 = query_without_memory(q1)\n",
        "\n",
        "q2 = \"How does sparse attention work compared to standard attention?\"\n",
        "response_q2_no_memory = query_without_memory(q2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiwS8A2CROev",
        "outputId": "a269ed77-ec3b-488e-98d2-9d5973deaf3b"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Question: What are the key innovations in the Attention Is All You Need paper?\n",
            "================================================================================\n",
            "\n",
            "Answer:\n",
            "The \"Attention Is All You Need\" paper introduces several key innovations:\n",
            "\n",
            "1. **Transformer Architecture**: The paper proposes the Transformer, a novel model architecture that relies entirely on attention mechanisms, specifically self-attention, to compute representations of input and output sequences. This is a departure from traditional models that use recurrent or convolutional neural networks.\n",
            "\n",
            "2. **Self-Attention Mechanism**: The Transformer uses self-attention to relate different positions of a single sequence to compute a representation of the sequence. This allows the model to draw global dependencies between input and output without regard to their distance in the sequence.\n",
            "\n",
            "3. **Multi-Head Attention**: To counteract the potential loss of resolution due to averaging in self-attention, the paper introduces multi-head attention, which allows the model to focus on different parts of the input sequence simultaneously.\n",
            "\n",
            "4. **Positional Encoding**: The paper uses sinusoidal positional encodings to provide the model with information about the position of tokens in the sequence. This helps the model to learn position information without using recurrent networks.\n",
            "\n",
            "5. **Parallelization**: By eliminating recurrence, the Transformer allows for significant parallelization, making it more efficient to train compared to RNN-based models. This results in faster training times and the ability to handle longer sequences more effectively.\n",
            "\n",
            "6. **State-of-the-Art Performance**: The Transformer achieves state-of-the-art results in machine translation tasks, demonstrating its effectiveness and efficiency compared to previous models.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Question: How does sparse attention work compared to standard attention?\n",
            "================================================================================\n",
            "\n",
            "Answer:\n",
            "Sparse attention works by reducing the number of connections between elements in the attention mechanism, compared to standard dense attention which considers all possible connections. In standard attention, every element in a sequence can attend to every other element, leading to a quadratic scaling in time and memory with respect to the sequence length. Sparse attention, on the other hand, introduces a structured sparsity pattern that limits the number of connections, thereby reducing computational and memory requirements.\n",
            "\n",
            "For example, in the context of block-sparse FlashAttention, the attention mechanism only considers a subset of possible connections, organized in blocks, which can significantly speed up computation by reducing the number of high-bandwidth memory (HBM) accesses. This is because fewer connections mean fewer data transfers between memory and processing units, which is a major factor affecting runtime.\n",
            "\n",
            "Additionally, sparse attention can be implemented using fixed patterns, such as strided or fixed attention, where specific positions are pre-determined to attend to certain other positions. This can maintain some level of global context while still benefiting from the efficiency of sparsity. Factorized attention heads, another form of sparse attention, use multiple attention heads that attend to different subsets of the input, further optimizing the computation.\n",
            "\n",
            "Overall, sparse attention aims to maintain the performance of dense attention while improving efficiency by leveraging structured sparsity, making it particularly useful for handling long sequences in models like Transformers.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding the Memory Layer\n",
        "\n",
        "Now let's implement the memory-enabled version. This function works identically to our vanilla RAG but with one critical addition: it checks Mem0 for stored information about the user.\n",
        "\n",
        "**Here's how it works:**\n",
        "\n",
        "When `use_memory=True`, the function:\n",
        "\n",
        "1. **Retrieves from Weaviate** (standard RAG retrieval)\n",
        "2. **Queries Mem0** for any stored memories about this user\n",
        "3. **Injects user preferences** into the system prompt if memories exist\n",
        "4. **Generates the response** with both document context and user context\n",
        "5. **Updates memory** by sending the conversation to Mem0 with instructions to extract and store relevant preferences\n",
        "\n",
        "The key difference is in the system prompt. Without memory, every user gets: `\"You are a helpful AI assistant. Answer based on the provided context.\"`\n",
        "\n",
        "With memory, users get: `\"You are a helpful AI assistant. Answer based on the context and user preferences. User Preferences: [their specific preferences]\"`\n",
        "\n",
        "The `memory.add()` call at the end is what makes the system learn over time. We send the user messages to Mem0 with specific instructions about what to extract—format preferences, knowledge level, learning style and it uses its own LLM to parse and store that information for future queries.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CWDFkxxzYcP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_papers(question: str, use_memory: bool = False):\n",
        "    \"\"\"\n",
        "    Query papers with automatic memory management\n",
        "    \"\"\"\n",
        "    # Retrieve from Weaviate\n",
        "    query_vector = get_embedding(question)\n",
        "    results = collection.query.near_vector(\n",
        "        near_vector=query_vector,\n",
        "        limit=TOP_K\n",
        "    )\n",
        "\n",
        "    retrieved_docs = []\n",
        "    for item in results.objects:\n",
        "        retrieved_docs.append({\n",
        "            'content': item.properties.get('text', ''),\n",
        "        })\n",
        "\n",
        "\n",
        "    context = \"\\n\\n\".join(f\"[Source: \\n{d['content']}\" for d in retrieved_docs)\n",
        "\n",
        "    # Build system prompt\n",
        "    if use_memory:\n",
        "        # Retrieve memories with filters\n",
        "        filters = {\"AND\": [{\"user_id\": USER_ID}]}\n",
        "        memory_response = memory.search(query=question, filters=filters, version=\"v2\")\n",
        "\n",
        "        memories = memory_response.get('results', [])\n",
        "\n",
        "        if len(memories) > 0:\n",
        "            memories_text = \"\\n\".join([f\"- {m.get('memory', '')}\" for m in memories])\n",
        "            system_prompt = f\"\"\"You are a helpful AI assistant. Answer based on the context and user preferences.\n",
        "\n",
        "User Preferences:\n",
        "{memories_text}\n",
        "\n",
        "\"\"\"\n",
        "        else:\n",
        "            system_prompt = \"You are a helpful AI assistant. Answer based on the provided context.\"\n",
        "    else:\n",
        "        system_prompt = \"You are a helpful AI assistant. Answer based on the provided context.\"\n",
        "\n",
        "    # Generate response\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=LLM_MODEL,\n",
        "        temperature=0.3,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content\n",
        "\n",
        "    system_prompt = \"\"\"Extract and store ONLY: response output format preferences (which formatting the model response should be in), knowledge baseline (what user knows), and learning style.\n",
        "    Do NOT store anything from the questions user asks, store ONLY from preferences user EXPLICITLY states.\"\"\"\n",
        "\n",
        "\n",
        "    # Update memory with system instruction for Mem0's LLM\n",
        "    if use_memory:\n",
        "        memory.add(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": system_prompt\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": question},\n",
        "                # {\"role\": \"assistant\", \"content\": answer}\n",
        "            ],\n",
        "            user_id=USER_ID,\n",
        "            metadata={\"category\": \"preferences\"}\n",
        "        )\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "3ufB5fGlcZLk"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the difference in action. We'll start by querying without memory to establish our baseline, then demonstrate how memory changes the interaction."
      ],
      "metadata": {
        "id": "JJCM0wGgZGQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q1: What is scaled dot-product attention and how does it work?\\n\")\n",
        "response_q1 = query_papers(q1, use_memory=False)\n",
        "print(f\"\\nAnswer:\\n{response_q1}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t_s2RBKUo60",
        "outputId": "7abb70bc-f3be-4f5b-b87b-58a4874a357e"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: What is scaled dot-product attention and how does it work?\n",
            "\n",
            "\n",
            "Answer:\n",
            "The \"Attention Is All You Need\" paper introduces several key innovations:\n",
            "\n",
            "1. **Transformer Architecture**: The paper proposes the Transformer model, which relies entirely on attention mechanisms, eliminating the need for recurrence and convolution. This allows for more efficient parallelization and faster training times.\n",
            "\n",
            "2. **Self-Attention Mechanism**: The Transformer uses self-attention to compute representations of input and output sequences. This mechanism relates different positions of a single sequence to compute a representation of the sequence, enabling the model to capture dependencies regardless of their distance.\n",
            "\n",
            "3. **Multi-Head Attention**: To counteract the reduced effective resolution due to averaging attention-weighted positions, the paper introduces Multi-Head Attention. This allows the model to focus on different parts of the sequence simultaneously, improving its ability to capture complex dependencies.\n",
            "\n",
            "4. **Positional Encoding**: The paper introduces sinusoidal positional encodings to provide the model with information about the position of each token in the sequence. This helps the model to learn positional relationships without relying on recurrence.\n",
            "\n",
            "5. **Scalability and Efficiency**: The Transformer architecture allows for significant parallelization, reducing training time and computational costs compared to traditional recurrent models. The paper demonstrates that the Transformer can achieve state-of-the-art results in machine translation tasks with less training time and resources.\n",
            "\n",
            "Overall, these innovations contribute to the Transformer's ability to efficiently model long-range dependencies and achieve high performance in sequence transduction tasks.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q2: How does sparse attention work compared to standard attention?\\n\")\n",
        "response_q2_no_memory = query_papers(q2, use_memory=False)\n",
        "print(f\"\\nAnswer:\\n{response_q2_no_memory}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkHpPkOAVlco",
        "outputId": "2edcbfd5-f612-43ed-ebef-29f33d2058a4"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q2: How does sparse attention work compared to standard attention?\n",
            "\n",
            "\n",
            "Answer:\n",
            "Sparse attention works by selectively focusing on a subset of the input data, rather than attending to all possible pairs of positions as in standard attention. This approach reduces the computational complexity and memory usage, which are typically quadratic in the sequence length for standard attention.\n",
            "\n",
            "In sparse attention, only certain positions in the input sequence are attended to, based on a predefined sparsity pattern. This can be achieved through various methods, such as block-sparse patterns, strided patterns, or fixed patterns, where specific cells summarize previous locations and propagate that information to future cells. These patterns allow for reduced HBM (High Bandwidth Memory) accesses, leading to faster execution and lower memory footprint.\n",
            "\n",
            "Sparse attention can be implemented using factorized self-attention, where multiple attention heads attend to distinct subblocks of the input, rather than the entire sequence. This allows for efficient computation while still maintaining a level of global context.\n",
            "\n",
            "Overall, sparse attention aims to maintain the performance of standard attention while improving efficiency by reducing the number of computations and memory accesses required.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test the memory system. This time, we'll include explicit preferences in the query—format preferences, knowledge level, and learning style.\n",
        "\n",
        "When `use_memory=True`, Mem0 will automatically extract and store these preferences. Future queries won't need to repeat them."
      ],
      "metadata": {
        "id": "8ZAHwMt3Zks5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preference_question = \"\"\"I'd like my responses in markdown format. Explain concepts like I only know\n",
        "about the vanilla attention mechanism and nothing else. I'm willing to learn as long as there\n",
        "are comparisons and bridges to more complex topics from what I know already.\n",
        "\n",
        "Now answer: How does sparse attention work compared to standard attention?\"\"\"\n",
        "\n",
        "print(f\"User: {preference_question}\\n\")\n",
        "response_q2_with_memory = query_papers(preference_question, use_memory=True)\n",
        "print(f\"\\nAnswer:\\n{response_q2_with_memory}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNCDnbwtVnvX",
        "outputId": "d8a16281-a4a2-4f74-c296-b93e3841b46f"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: I'd like my responses in markdown format. Explain concepts like I only know\n",
            "about the attention mechanism and nothing else. I'm willing to learn as long as there\n",
            "are comparisons and bridges to more complex topics from what I know already.\n",
            "\n",
            "Now answer: How does sparse attention work compared to standard attention?\n",
            "\n",
            "\n",
            "Answer:\n",
            "Certainly! Let's break down the concept of sparse attention compared to standard attention, using the attention mechanism as our starting point.\n",
            "\n",
            "### Standard Attention\n",
            "\n",
            "In a standard attention mechanism, every element in a sequence (like words in a sentence) attends to every other element. This means that for a sequence of length \\( n \\), the attention mechanism computes \\( n \\times n \\) interactions. This is often visualized as a full grid or matrix where each element is connected to every other element. This approach can be computationally expensive and memory-intensive, especially for long sequences, because the operations scale quadratically with the sequence length.\n",
            "\n",
            "### Sparse Attention\n",
            "\n",
            "Sparse attention, on the other hand, reduces the number of interactions by limiting which elements attend to each other. Instead of every element attending to every other element, sparse attention introduces a pattern or structure that dictates which elements can interact. This results in a matrix that is not fully filled, hence \"sparse.\" \n",
            "\n",
            "#### Key Differences:\n",
            "\n",
            "1. **Connectivity**:\n",
            "   - **Standard Attention**: Every element is connected to every other element.\n",
            "   - **Sparse Attention**: Only a subset of elements are connected, based on a predefined pattern.\n",
            "\n",
            "2. **Computational Efficiency**:\n",
            "   - **Standard Attention**: Computational cost grows quadratically with sequence length.\n",
            "   - **Sparse Attention**: By reducing the number of interactions, the computational cost can be reduced, often growing linearly with sequence length.\n",
            "\n",
            "3. **Memory Usage**:\n",
            "   - **Standard Attention**: Requires a lot of memory to store all pairwise interactions.\n",
            "   - **Sparse Attention**: Uses less memory because fewer interactions are stored.\n",
            "\n",
            "### Example Patterns in Sparse Attention\n",
            "\n",
            "- **Strided Patterns**: Elements attend to every \\( k \\)-th element, creating a pattern similar to skipping steps.\n",
            "- **Fixed Patterns**: A predetermined pattern, like a butterfly pattern, is used throughout the model.\n",
            "\n",
            "### Benefits and Trade-offs\n",
            "\n",
            "- **Benefits**: Sparse attention can significantly speed up computations and reduce memory usage, making it feasible to handle longer sequences.\n",
            "- **Trade-offs**: The challenge is to design sparse patterns that still capture the essential information needed for the task, as some information might be lost compared to the full connectivity of standard attention.\n",
            "\n",
            "### Bridging to More Complex Topics\n",
            "\n",
            "Sparse attention can be seen as a step towards more efficient models, similar to how pruning works in model compression. It also aligns with the idea of the \"lottery ticket hypothesis,\" which suggests that smaller sub-networks can perform as well as larger ones if they are structured correctly.\n",
            "\n",
            "By understanding sparse attention, you can explore more advanced topics like block-sparse attention, which uses specific block patterns to further optimize performance, or hybrid approaches that combine sparse and low-rank approximations for even greater efficiency.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The response came back exactly as requested—markdown format, starting from basic attention concepts, with comparisons bridging to more complex ideas.\n",
        "\n",
        "Now let's test with a completely different question, without repeating any of those preferences.\n",
        "\n",
        "The below queries don't mention formatting preferences, knowledge level, or explanation styles. Let's see what happens:"
      ],
      "metadata": {
        "id": "J0sNgxwYatMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q3: What is Flash Attention?\\n\")\n",
        "q3 = \"What is Flash Attention?\"\n",
        "response_q3 = query_papers(q3, use_memory=True)\n",
        "print(f\"\\nAnswer:\\n{response_q3}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcdtoYuTXBEi",
        "outputId": "57be65d5-b95c-42f6-8d8c-352bdd01effc"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q3: What is Flash Attention?\n",
            "\n",
            "\n",
            "Answer:\n",
            "FlashAttention is an advanced attention algorithm designed to enhance the efficiency of the attention mechanism in Transformers, particularly by addressing the bottleneck caused by memory accesses. Here's a breakdown of its key features and how it compares to standard attention:\n",
            "\n",
            "1. **Memory Access Optimization**: \n",
            "   - FlashAttention reduces the number of memory reads and writes, which are the primary factors affecting runtime in GPU computations. By minimizing these memory accesses, FlashAttention achieves faster performance compared to standard attention.\n",
            "\n",
            "2. **Block Processing and Tiling**:\n",
            "   - The algorithm splits the input into blocks and processes these blocks incrementally. This approach allows for the computation of the softmax reduction without needing access to the entire input at once, thus avoiding the storage of large intermediate attention matrices.\n",
            "\n",
            "3. **Efficient Backward Pass**:\n",
            "   - Unlike some other methods that recompute both the attention matrix and temporary outputs during the backward pass, FlashAttention simplifies this process by only recomputing the attention matrix. This reduces memory requirements and speeds up the backward pass.\n",
            "\n",
            "4. **Implementation and Performance**:\n",
            "   - FlashAttention is implemented in CUDA, allowing for fine-grained control over memory access and the fusion of all attention operations into a single GPU kernel. Despite the increased floating-point operations (FLOPs) due to recomputation, it runs significantly faster (up to 7.6x on models like GPT-2) and uses less memory, scaling linearly with sequence length.\n",
            "\n",
            "5. **Comparison with Other Methods**:\n",
            "   - Compared to Rabe and Staats' method, which focuses on reducing the total memory footprint, FlashAttention is faster because it reduces memory accesses. Both methods offer substantial memory savings, but FlashAttention's approach to incrementally updating outputs and simplifying the backward pass provides additional speed and efficiency benefits.\n",
            "\n",
            "6. **Extensions and Applications**:\n",
            "   - FlashAttention can serve as a foundation for approximate attention algorithms by mitigating memory access overhead. It has been extended to block-sparse FlashAttention, which is even faster and scales to longer sequences (up to 64K).\n",
            "\n",
            "Overall, FlashAttention enhances model training speed and quality by enabling Transformers to handle longer sequences more efficiently, making it a valuable tool for improving the performance of deep learning models.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q4: Compare standard, sparse, and Flash Attention. What are the trade-offs?\\n\")\n",
        "q4 = \"Compare standard, sparse, and Flash Attention. What are the trade-offs?\"\n",
        "response_q4 = query_papers(q4, use_memory=True)\n",
        "print(f\"\\nAnswer:\\n{response_q4}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xovL5oNXdo1",
        "outputId": "e4f4dc2d-583d-4e2a-9e54-ed8e2a90929b"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q4: Compare standard, sparse, and Flash Attention. What are the trade-offs?\n",
            "\n",
            "\n",
            "Answer:\n",
            "When comparing standard, sparse, and FlashAttention, several trade-offs emerge in terms of speed, memory efficiency, and scalability:\n",
            "\n",
            "1. **Standard Attention:**\n",
            "   - **Speed:** Standard attention is generally slower due to higher memory access requirements. It requires Θ(𝑁𝑑 + 𝑁²) HBM accesses, which can be a bottleneck for runtime.\n",
            "   - **Memory Efficiency:** It has a larger memory footprint compared to FlashAttention, making it less efficient for longer sequences.\n",
            "   - **Scalability:** Standard attention struggles to scale efficiently to very long sequences due to its quadratic growth in both runtime and memory usage.\n",
            "\n",
            "2. **Sparse Attention:**\n",
            "   - **Speed:** Sparse attention mechanisms can be faster than standard attention for certain sequence lengths because they reduce the number of computations by focusing only on a subset of the attention matrix.\n",
            "   - **Memory Efficiency:** Sparse attention can be more memory-efficient than standard attention since it does not need to compute or store the entire attention matrix.\n",
            "   - **Scalability:** While sparse attention can handle longer sequences more efficiently than standard attention, its performance can vary depending on the sparsity pattern and implementation.\n",
            "\n",
            "3. **FlashAttention:**\n",
            "   - **Speed:** FlashAttention is significantly faster than standard attention, up to 3× faster for common sequence lengths, due to reduced memory accesses (Θ(𝑁²𝑑²𝑀⁻¹) HBM accesses). It also outperforms many sparse and approximate attention methods for shorter sequences.\n",
            "   - **Memory Efficiency:** FlashAttention is up to 20× more memory-efficient than standard attention and more efficient than many approximate methods. It maintains a linear growth in memory footprint with sequence length.\n",
            "   - **Scalability:** FlashAttention scales well to longer sequences (up to 64K), and block-sparse FlashAttention further enhances scalability by being 2-4× faster than FlashAttention itself, due to improved IO complexity proportional to the sparsity ratio.\n",
            "\n",
            "**Trade-offs:**\n",
            "- **Standard Attention** is straightforward but becomes inefficient for long sequences due to high memory and computational demands.\n",
            "- **Sparse Attention** offers a trade-off between speed and accuracy, potentially sacrificing some accuracy for improved efficiency.\n",
            "- **FlashAttention** provides a balance of speed and memory efficiency without sacrificing accuracy, making it suitable for both short and long sequences. Block-sparse FlashAttention further optimizes performance for very long sequences by leveraging sparsity.\n",
            "\n",
            "In summary, FlashAttention and its block-sparse variant offer superior performance and efficiency over standard and sparse attention methods, especially for longer sequences, by reducing memory accesses and optimizing memory usage.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The system remembered. Without repeating any preferences, the response automatically came back in markdown format, explained at a beginner level starting from attention mechanisms, and used comparisons throughout.\n",
        "\n",
        "The memory layer is working, Mem0 retrieved the stored preferences and applied them to these questions."
      ],
      "metadata": {
        "id": "Ax4ocVYabJV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check what's actually stored in the memory layer"
      ],
      "metadata": {
        "id": "DXG_3pSgdJyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final Memory State - What Mem0 Learned About You\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "filters = {\"AND\": [{\"user_id\": USER_ID}]}\n",
        "all_memories = memory.get_all(filters=filters)['results']\n",
        "print(f\"Total memories stored: {len(all_memories)}\\n\")\n",
        "for idx, m in enumerate(all_memories):\n",
        "    print(f\"{idx}. {m.get('memory', '')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M2On5mLXlar",
        "outputId": "ebb712a3-4f19-4924-fd0c-01fe9c6303d2"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Memory State - What Mem0 Learned About You\n",
            "================================================================================\n",
            "\n",
            "Total memories stored: 3\n",
            "\n",
            "0. User is willing to learn as long as there are comparisons and bridges to more complex topics\n",
            "1. User only knows about the attention mechanism\n",
            "2. User prefers responses in markdown format\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that what's stored is exactly what we instructed Mem0 to extract. The individual questions you asked (about sparse attention, Flash Attention, trade-offs) aren't in there. Those were just queries, not preferences.\n",
        "\n",
        "This is the key difference. The memory layer isn't recording your conversation history, it's distilling the patterns that matter for personalization.\n",
        "\n",
        "Here's what happened across the demo:\n",
        "\n",
        "**Q1 (baseline)**: Asked about the base paper without memory → generic response\n",
        "\n",
        "**Q2 (set preferences)**: Stated your preferences once in the query itself → Mem0 extracted and stored them\n",
        "\n",
        "**Q3, Q4**: Asked completely different questions without mentioning preferences → system automatically applied them every time\n",
        "\n",
        "The result? You stated your preferences once, and the system remembered them across multiple questions. No repetition needed.\n",
        "\n",
        "This is what makes memory-enabled RAG different from standard RAG. Same documents, same retrieval, same LLM but the delivery adapts to each user.\n",
        "\n",
        "**Ready to build your own memory-enabled RAG system?**\n",
        "\n",
        "Swap in your own documents, customize what memories to extract, and see how personalization changes your AI's responses.\n",
        "\n",
        "If you're working with enterprise data at scale, [sign up for Unstructured](https://unstructured.io/?modal=try-for-free) to access the full platform with workflow scheduling, multiple source connectors, and production-grade processing.\n"
      ],
      "metadata": {
        "id": "4BDgnQvadxGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q3HoT-8GZ8X6"
      },
      "execution_count": 134,
      "outputs": []
    }
  ]
}