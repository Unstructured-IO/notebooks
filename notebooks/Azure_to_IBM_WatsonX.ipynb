{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started with Unstructured API and IBM watsonx.data\n",
        "[Unstructured](https://unstructured.io) is an ETL+ platform purpose-built for preprocessing unstructured data for GenAI and retrieval-based applications. It helps teams:\n",
        "\n",
        "* Connect to a wide range of enterprise systems—from cloud storage providers like Azure Blob Storage or Amazon S3, to collaboration platforms like Confluence and Dropbox, to business tools like Salesforce, Jira, and more.\n",
        "* Continuously ingest data from these systems in a scalable, automated way.\n",
        "* Preprocess the raw content using a unified, modular pipeline: partitioning, enriching, chunking, and embedding your documents in a consistent format.\n",
        "* Output clean, structured results into your downstream stack—such as a vector database, search engine, or data warehouse.\n",
        "\n",
        "You can manage connectors and workflows via the Unstructured UI or the headless API.\n",
        "\n",
        "In this hands-on notebook, we’ll walk through how to use the Unstructured Python SDK to define and run a full data processing workflow—taking unstructured files from **Azure Blob Storage** and landing the structured output into **IBM watsonx.data**.\n",
        "\n",
        "While we’re using Azure Blob Storage as our data source, you can substitute any of the [supported sources](https://docs.unstructured.io/api-reference/workflow/sources/overview) to fit your stack. The pipeline we'll build is modular and extensible by design.\n",
        "\n",
        "So, let's get started!"
      ],
      "metadata": {
        "id": "bVEaaXVak-l_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 Install the Unstructured API Python SDK\n",
        "\n",
        "All functionality available in the the UI of the [Unstructured](https://unstructured.io/) product is also available programmatically via Unstructured API. You can interact with Unstructured API either by sending direct requests via curl or postman, or using Unstructured API [Python SDK](https://docs.unstructured.io/api-reference/workflow/overview#unstructured-python-sdk). Here, we'll be using the latter.\n",
        "\n",
        "\n",
        "> **Note:**\n",
        "The Unstructured API has two endpoints:\n",
        "* The Unstructured Partition Endpoint: intended for rapid prototyping of Unstructured's various partitioning strategies. It works only with processing of local files, one file at a time.\n",
        "* The Unstructured Workflow Endpoint: enables a full range of partitioning, chunking, embedding, and enrichment options for your data. It is designed to batch-process data from any data source to any destination. This is what we're using in this notebook.\n",
        "\n",
        "\n",
        "Run the following cell to install the Unstructured API Python SDK."
      ],
      "metadata": {
        "id": "Os8xmINhKtWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade \"unstructured-client>=0.30.6\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP4GzlTxIfL8",
        "outputId": "6e8f95ff-cae9-4ade-c006-10c072b96138",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured-client>=0.30.6\n",
            "  Downloading unstructured_client-0.34.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting aiofiles>=24.1.0 (from unstructured-client>=0.30.6)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client>=0.30.6) (43.0.3)\n",
            "Collecting eval-type-backport>=0.2.0 (from unstructured-client>=0.30.6)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client>=0.30.6) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client>=0.30.6) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client>=0.30.6) (2.11.4)\n",
            "Collecting pypdf>=4.0 (from unstructured-client>=0.30.6)\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client>=0.30.6) (1.0.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client>=0.30.6) (0.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client>=0.30.6) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client>=0.30.6) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client>=0.30.6) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client>=0.30.6) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client>=0.30.6) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client>=0.30.6) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client>=0.30.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client>=0.30.6) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client>=0.30.6) (4.13.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from requests-toolbelt>=1.0.0->unstructured-client>=0.30.6) (2.32.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client>=0.30.6) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt>=1.0.0->unstructured-client>=0.30.6) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt>=1.0.0->unstructured-client>=0.30.6) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client>=0.30.6) (1.3.1)\n",
            "Downloading unstructured_client-0.34.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.4/189.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, eval-type-backport, aiofiles, unstructured-client\n",
            "Successfully installed aiofiles-24.1.0 eval-type-backport-0.2.2 pypdf-5.4.0 unstructured-client-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Create all Connectors"
      ],
      "metadata": {
        "id": "p905Ez5mKypD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, let's set up the Unstructured API. For this, you will reqiure an Unstructured API Key.\n",
        "\n",
        "[Learn how to get one](https://docs.unstructured.io/platform-api/api/overview)."
      ],
      "metadata": {
        "id": "rZeZc_gTK6M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from unstructured_client import UnstructuredClient\n",
        "\n",
        "os.environ[\"UNSTRUCTURED_API_KEY\"] = userdata.get(\"UNSTRUCTURED_API_KEY\")\n",
        "client = UnstructuredClient(api_key_auth=os.environ[\"UNSTRUCTURED_API_KEY\"])\n",
        "\n",
        "# helper function to format outputs\n",
        "def pretty_print_model(response_model):\n",
        "    print(response_model.model_dump_json(indent=4))"
      ],
      "metadata": {
        "id": "Wimcsi4tIfJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Microsoft Azure Blob Storage Source Connector\n",
        "\n",
        "You'll need an **Azure account** with access to **Azure Blob Storage**, along with your **storage account name** and a **shared access signature (SAS)** token for authentication. Make sure you've created a **container** within your storage account and that it has the appropriate access permissions. Upload a few files to your blob container so there's something to play with! 😉 Take a look at [this list](https://docs.unstructured.io/api-reference/supported-file-types) of supported file types and [this video](https://www.youtube.com/watch?time_continue=211&v=Vl3KCphlh9Y&embeds_referring_euri=https%3A%2F%2Fdocs.unstructured.io%2F&source_ve_path=MjM4NTE) on how you can set yours up.\n"
      ],
      "metadata": {
        "id": "cebSZH8WK87I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['AZURE_REMOTE_URL'] = userdata.get('AZURE_REMOTE_URL')\n",
        "os.environ['AZURE_ACCOUNT_NAME'] = userdata.get('AZURE_ACCOUNT_NAME')\n",
        "os.environ['AZURE_SAS_TOKEN'] = userdata.get('AZURE_SAS_TOKEN')"
      ],
      "metadata": {
        "id": "n0dZDSdiQ0Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured_client.models.operations import CreateSourceRequest\n",
        "from unstructured_client.models.shared import (\n",
        "    CreateSourceConnector,\n",
        "    SourceConnectorType,\n",
        "    AzureSourceConnectorConfigInput\n",
        ")\n",
        "\n",
        "\n",
        "response = client.sources.create_source(\n",
        "    request=CreateSourceRequest(\n",
        "        create_source_connector=CreateSourceConnector(\n",
        "            name=\"azure_souce_connector\",\n",
        "            type=SourceConnectorType.AZURE,\n",
        "            config=AzureSourceConnectorConfigInput(\n",
        "                remote_url=os.environ['AZURE_REMOTE_URL'],\n",
        "                recursive=True,\n",
        "                account_name=os.environ['AZURE_ACCOUNT_NAME'],\n",
        "                sas_token=os.environ['AZURE_SAS_TOKEN']\n",
        "\n",
        "            )\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "source_connector_id = response.source_connector_information.id"
      ],
      "metadata": {
        "id": "Tn7B1eZlQyY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create IBM WatsonX Destination Connector\n",
        "\n",
        "\n",
        "To use IBM watsonx.data as your destination in a Unstructured workflow, you’ll need a few components configured ahead of time. This setup allows Unstructured to push processed, structured content directly into a table within your watsonx.data environment.\n",
        "\n",
        "Here’s what you’ll need to have ready:\n",
        "\n",
        "- An **IBM Cloud account** with an active **watsonx.data instance**\n",
        "- An **IBM Cloud API key** for authentication\n",
        "- A **Cloud Object Storage (COS)** instance, including:\n",
        "  - A target **bucket** for data storage\n",
        "  - The **public endpoint**, **region**, and **bucket name**\n",
        "  - **HMAC credentials** (access key ID and secret key)\n",
        "- An **Apache Iceberg catalog** associated with your watsonx.data instance and linked to the COS bucket\n",
        "- A **namespace** (also called a schema) and a **target table** inside the catalog\n",
        "- A column in the table that uniquely identifies records (usually `record_id`)\n",
        "\n",
        "You’ll also need to configure your catalog in the watsonx.data console via the Infrastructure Manager, ensuring it's:\n",
        "- Connected to COS with correct access credentials\n",
        "- Associated with an engine (e.g., IBM Presto)\n",
        "- Activated and tested with a successful connection status\n",
        "\n",
        "Make sure your table matches the expected Unstructured output schema—any extra fields in the data that don’t map to table columns will be dropped.\n",
        "\n",
        "> 📘 **Tip:** For performance, it's recommended to enable regular metadata cleanup on the table using a small Python script (provided in the official docs).\n",
        "\n",
        "Once your watsonx.data environment is configured, you can create a destination connector using the Python SDK, UI, or API.\n",
        "\n",
        "\n",
        "| **Key** | **Required** | **Description** |\n",
        "|--------|------------|----------------|\n",
        "| `iceberg_endpoint` | ✅ Required | The metastore REST endpoint of the Iceberg catalog (exclude `https://`) |\n",
        "| `object_storage_endpoint` | ✅ Required | Public endpoint of the COS bucket (exclude `https://`) |\n",
        "| `object_storage_region` | ✅ Required | Short region ID of the COS bucket (e.g. `us-east`) |\n",
        "| `iam_api_key` | ✅ Required | API key for your IBM Cloud account |\n",
        "| `access_key_id` | ✅ Required | HMAC access key ID for the COS instance |\n",
        "| `secret_access_key` | ✅ Required | HMAC secret access key paired with `access_key_id` |\n",
        "| `catalog` | ✅ Required | Name of the Iceberg catalog in watsonx.data |\n",
        "| `namespace` | ✅ Required | The schema (namespace) within the catalog |\n",
        "| `table` | ✅ Required | Name of the destination table within the namespace |\n",
        "| `record_id_key` | ❌ Optional | Name of the table column that uniquely identifies records (default: `record_id`) |\n",
        "| `max_retries` | ❌ Optional | Max upload retries (default: 50; allowed: 2–500) |\n",
        "| `max_retries_connection` | ❌ Optional | Max connection retries (default: 10; allowed: 2–100) |\n",
        "\n",
        "> 📘 **Tip:** All endpoint values should be raw (e.g., `s3.us-east.cloud-object-storage.appdomain.cloud`) without any URL schemes (`https://`).\n",
        "\n",
        "To review the full configuration steps, including the table schema and example connector creation code, refer to the [Unstructured IBM watsonx.data destination documentation](https://docs.unstructured.io/api-reference/workflow/destinations/ibm-watsonxdata).\n",
        "\n",
        "\n",
        "\n",
        "Fetching all credentials from Secrets!"
      ],
      "metadata": {
        "id": "QPvcpmm7VoYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"IBM_WX_ICEBERG_ENDPOINT\"] = userdata.get(\"IBM_WX_ICEBERG_ENDPOINT\")\n",
        "os.environ[\"IBM_WX_OBJECT_STORAGE_ENDPOINT\"] = userdata.get(\"IBM_WX_OBJECT_STORAGE_ENDPOINT\")\n",
        "os.environ[\"IBM_WX_OBJECT_STORAGE_REGION\"] = userdata.get(\"IBM_WX_OBJECT_STORAGE_REGION\")\n",
        "os.environ[\"IBM_WX_IAM_API_KEY\"] = userdata.get(\"IBM_WX_IAM_API_KEY\")\n",
        "os.environ[\"IBM_WX_ACCESS_KEY_ID\"] = userdata.get(\"IBM_WX_ACCESS_KEY_ID\")\n",
        "os.environ[\"IBM_WX_SECRET_ACCESS_KEY\"] = userdata.get(\"IBM_WX_SECRET_ACCESS_KEY\")\n",
        "os.environ[\"IBM_WX_CATALOG\"] = userdata.get(\"IBM_WX_CATALOG\")\n",
        "os.environ[\"IBM_WX_NAMESPACE\"] = userdata.get(\"IBM_WX_NAMESPACE\")\n",
        "os.environ[\"IBM_WX_TABLE\"] = userdata.get(\"IBM_WX_TABLE\")"
      ],
      "metadata": {
        "id": "9-uJIfkcswH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured_client.models.operations import CreateDestinationRequest\n",
        "from unstructured_client.models.shared import (\n",
        "    CreateDestinationConnector,\n",
        "    DestinationConnectorType,\n",
        "    IbmWatsonxDestinationConnectorConfigInput\n",
        ")\n",
        "\n",
        "\n",
        "response = client.destinations.create_destination(\n",
        "    request=CreateDestinationRequest(\n",
        "        create_destination_connector=CreateDestinationConnector(\n",
        "            name=f\"IBM watsonx.data destination {time.time()}\",\n",
        "            type=DestinationConnectorType.IBM_WATSONX_S3,\n",
        "            config=IbmWatsonxDestinationConnectorConfigInput(\n",
        "                iceberg_endpoint=os.environ[\"IBM_WX_ICEBERG_ENDPOINT\"],\n",
        "                object_storage_endpoint=os.environ[\"IBM_WX_OBJECT_STORAGE_ENDPOINT\"],\n",
        "                object_storage_region=os.environ[\"IBM_WX_OBJECT_STORAGE_REGION\"],\n",
        "                iam_api_key=os.environ[\"IBM_WX_IAM_API_KEY\"],\n",
        "                access_key_id=os.environ[\"IBM_WX_ACCESS_KEY_ID\"],\n",
        "                secret_access_key=os.environ[\"IBM_WX_SECRET_ACCESS_KEY\"],\n",
        "                catalog=os.environ[\"IBM_WX_CATALOG\"],\n",
        "                namespace=os.environ[\"IBM_WX_NAMESPACE\"],\n",
        "                table=os.environ[\"IBM_WX_TABLE\"],\n",
        "                max_retries=50,\n",
        "                max_retries_connection=10,\n",
        "                record_id_key=\"record_id\"\n",
        "            )\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "destination_connector_id = response.destination_connector_information.id"
      ],
      "metadata": {
        "id": "-H40eCZGecqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Designing Your Data Workflow\n",
        "\n",
        "Once your connectors are in place, it's time to define *how* your data will be processed. This is where workflows come in.\n",
        "\n",
        "In the Unstructured platform, a **workflow** is a directed acyclic graph (DAG) that connects a series of processing steps—each one represented by a `WorkflowNode`. Think of each node as a small, focused operation in a larger data prep pipeline. These steps can include things like turning a PDF into structured JSON, generating image captions, or creating embeddings for search.\n",
        "\n",
        "Let’s walk through the most common node types you’ll use to shape your workflow.\n",
        "\n",
        "### Partitioning the Raw Data\n",
        "\n",
        "Every workflow starts with a `PARTITION` node. This is the required first step and forms the foundation for everything else. Its job is to take in raw documents—PDFs, markdown files, emails, you name it—and convert them into a standardized JSON format that the rest of the pipeline can understand.\n",
        "\n",
        "Under the hood, Unstructured offers several partitioning strategies. Here’s a quick overview:\n",
        "\n",
        "- **Auto** : A smart mode that chooses the best strategy based on the page. It balances performance and cost by dynamically selecting between VLM, High Res, or Fast.\n",
        "- **VLM**: Uses vision-language models to extract content from hard-to-read documents—like scans with handwriting or complex layouts.\n",
        "- **High Res**: A solid choice for scanned image-based documents that need strong OCR plus layout understanding.\n",
        "- **Fast**: Ideal for well-structured text files like markdown or Word docs. Lightweight and efficient.\n",
        "\n",
        "If you’re curious about the structure of the output, you can explore the JSON schema [here](https://docs.unstructured.io/api-reference/partition/document-elements).\n",
        "\n",
        "\n",
        "### Breaking It Down with Chunking\n",
        "\n",
        "The next node type you’ll usually add is the `CHUNK` node. This node helps divide the document into smaller, coherent pieces of text. Why do this? Because most embedding models (and downstream tools like vector databases) work best with bite-sized chunks that fit within token limits.\n",
        "\n",
        "You can read more about chunking strategies [here](https://docs.unstructured.io/ui/chunking).\n",
        "\n",
        "### Generating Embeddings\n",
        "\n",
        "Finally, there’s the `EMBED` node. This is where your clean, chunked content gets converted into numerical vector representations—aka *embeddings*. These vectors can then power similarity search, clustering, or RAG pipelines.\n",
        "\n",
        "To go deeper on how embeddings work in Unstructured, check out the guide [here](https://docs.unstructured.io/ui/embedding).\n",
        "\n",
        "---\n",
        "\n",
        "Once you’ve defined your workflow steps, it’s time to create the workflow and send it off to the platform.\n",
        "\n",
        "Run the next cell to spin it up.\n"
      ],
      "metadata": {
        "id": "LZ982QSwfOQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured_client.models.shared import (\n",
        "    WorkflowNode,\n",
        "    WorkflowType,\n",
        "    Schedule\n",
        ")\n",
        "\n",
        "parition_node = WorkflowNode(\n",
        "    name=\"Partitioner\",\n",
        "    subtype=\"vlm\",\n",
        "    type=\"partition\",\n",
        "    settings={\n",
        "        \"provider\": \"anthropic\",\n",
        "        \"model\": \"claude-3-7-sonnet-20250219\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "chunk_node = WorkflowNode(\n",
        "    name='Chunker',\n",
        "    subtype='chunk_by_title',\n",
        "    type=\"chunk\",\n",
        "    settings={\n",
        "        'new_after_n_chars': 1500,\n",
        "        'max_characters': 2048,\n",
        "        'overlap': 0\n",
        "        }\n",
        "    )\n",
        "\n",
        "embedder_node = WorkflowNode(\n",
        "    name='Embedder',\n",
        "    subtype='azure_openai',\n",
        "    type=\"embed\",\n",
        "    settings={\n",
        "        'model_name': 'text-embedding-3-small'\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "response = client.workflows.create_workflow(\n",
        "    request={\n",
        "        \"create_workflow\": {\n",
        "            \"name\": \"Azure-to-snowflake-table-custom-workflow_1373\",\n",
        "            \"source_id\": source_connector_id,\n",
        "            \"destination_id\": destination_connector_id,\n",
        "            \"workflow_type\": WorkflowType.CUSTOM,\n",
        "            \"workflow_nodes\": [\n",
        "                parition_node,\n",
        "                chunk_node,\n",
        "                embedder_node\n",
        "            ],\n",
        "            \"schedule\": Schedule(\"monthly\")\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow_id = response.workflow_information.id"
      ],
      "metadata": {
        "id": "GUsSPMedfe-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Run the workflow\n",
        "\n",
        "Run the following cell to start running the workflow."
      ],
      "metadata": {
        "id": "V5JfHDo9f5ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = client.workflows.run_workflow(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "lD-IZs0af7u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Get the workflow run's job ID\n",
        "\n",
        "Run the next cell to get the workflow run's job ID, which is needed to poll for job completion later. If successful, Unstructured prints the job's ID."
      ],
      "metadata": {
        "id": "UbOwxNb0gA9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.jobs.list_jobs(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id\n",
        "    }\n",
        ")\n",
        "\n",
        "last_job = response.response_list_jobs[0]\n",
        "job_id = last_job.id"
      ],
      "metadata": {
        "id": "OXeZ9_aOgD7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Poll for job completion\n",
        "\n",
        "Run the below cell to confirm the job has finished running. If successful, Unstructured prints `\"status\": \"COMPLETED\"` within the information about the job."
      ],
      "metadata": {
        "id": "b7UZADKXgGO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def poll_job_status(job_id, wait_time=30):\n",
        "    while True:\n",
        "        response = client.jobs.get_job(\n",
        "            request={\n",
        "                \"job_id\": job_id\n",
        "            }\n",
        "        )\n",
        "\n",
        "        job = response.job_information\n",
        "\n",
        "        if job.status == \"SCHEDULED\":\n",
        "            print(f\"Job is scheduled, polling again in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        elif job.status == \"IN_PROGRESS\":\n",
        "            print(f\"Job is in progress, polling again in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        else:\n",
        "            print(\"Job is completed\")\n",
        "            break\n",
        "\n",
        "    return job\n",
        "\n",
        "job = poll_job_status(job_id)"
      ],
      "metadata": {
        "id": "cfjWzqFbgIXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: View the processed data\n",
        "\n",
        "\n",
        "Once the job is completed, your data is processed, and you can find it in your table under the schema you've specified:\n",
        "\n",
        "![](https://framerusercontent.com/images/V9MIETDCnhERvPsqJmSEaTYEw.png)"
      ],
      "metadata": {
        "id": "cdz4TkllgK-P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3S127fl3sIbd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}