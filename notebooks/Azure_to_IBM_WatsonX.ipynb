{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVEaaXVak-l_"
      },
      "source": [
        "# Getting Started with Unstructured API and IBM watsonx.data\n",
        "[Unstructured](https://unstructured.io) is an ETL+ platform purpose-built for preprocessing unstructured data for GenAI and retrieval-based applications. It helps teams:\n",
        "\n",
        "* Connect to a wide range of enterprise systemsâ€”from cloud storage providers like Azure Blob Storage or Amazon S3, to collaboration platforms like Confluence and Dropbox, to business tools like Salesforce, Jira, and more.\n",
        "* Continuously ingest data from these systems in a scalable, automated way.\n",
        "* Preprocess the raw content using a unified, modular pipeline: partitioning, enriching, chunking, and embedding your documents in a consistent format.\n",
        "* Output clean, structured results into your downstream stackâ€”such as a vector database, search engine, or data warehouse.\n",
        "\n",
        "You can manage connectors and workflows via the Unstructured UI or the headless API.\n",
        "\n",
        "In this hands-on notebook, weâ€™ll walk through how to use the Unstructured Python SDK to define and run a full data processing workflowâ€”taking unstructured files from **Azure Blob Storage** and landing the structured output into **IBM watsonx.data**.\n",
        "\n",
        "While weâ€™re using Azure Blob Storage as our data source, you can substitute any of the [supported sources](https://docs.unstructured.io/api-reference/workflow/sources/overview) to fit your stack. The pipeline we'll build is modular and extensible by design.\n",
        "\n",
        "So, let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os8xmINhKtWQ"
      },
      "source": [
        "## Step 1 Install the Unstructured API Python SDK\n",
        "\n",
        "All functionality available in the the UI of the [Unstructured](https://unstructured.io/) product is also available programmatically via Unstructured API. The Unstructured API provides a set of programmatic operations that enable you to have Unstructured:\n",
        "\n",
        "* Perform a full range of partitioning, enrichment, chunking, and embedding of your files and data, producing results that are AI-ready.\n",
        "* Batch-process files locally; and batch-process files and data that are stored in remote storage locations, databases, and vector stores.\n",
        "\n",
        "\n",
        "Run the following cell to install the Unstructured API Python SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NP4GzlTxIfL8",
        "outputId": "6e8f95ff-cae9-4ade-c006-10c072b96138"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade \"unstructured-client>=0.30.6\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p905Ez5mKypD"
      },
      "source": [
        "## Step 2: Create all Connectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZeZc_gTK6M8"
      },
      "source": [
        "Firstly, let's set up the Unstructured API. For this, you will reqiure an Unstructured API Key.\n",
        "\n",
        "[Learn how to get one](https://docs.unstructured.io/platform-api/api/overview)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wimcsi4tIfJV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from unstructured_client import UnstructuredClient\n",
        "\n",
        "os.environ[\"UNSTRUCTURED_API_KEY\"] = userdata.get(\"UNSTRUCTURED_API_KEY\")\n",
        "client = UnstructuredClient(api_key_auth=os.environ[\"UNSTRUCTURED_API_KEY\"])\n",
        "\n",
        "# helper function to format outputs\n",
        "def pretty_print_model(response_model):\n",
        "    print(response_model.model_dump_json(indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cebSZH8WK87I"
      },
      "source": [
        "### Create Microsoft Azure Blob Storage Source Connector\n",
        "\n",
        "You'll need an **Azure account** with access to **Azure Blob Storage**, along with your **storage account name** and a **shared access signature (SAS)** token for authentication. Make sure you've created a **container** within your storage account and that it has the appropriate access permissions. Upload a few files to your blob container so there's something to play with! ðŸ˜‰ Take a look at [this list](https://docs.unstructured.io/api-reference/supported-file-types) of supported file types and [this video](https://www.youtube.com/watch?time_continue=211&v=Vl3KCphlh9Y&embeds_referring_euri=https%3A%2F%2Fdocs.unstructured.io%2F&source_ve_path=MjM4NTE) on how you can set yours up.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0dZDSdiQ0Lg"
      },
      "outputs": [],
      "source": [
        "os.environ['AZURE_REMOTE_URL'] = userdata.get('AZURE_REMOTE_URL')\n",
        "os.environ['AZURE_ACCOUNT_NAME'] = userdata.get('AZURE_ACCOUNT_NAME')\n",
        "os.environ['AZURE_SAS_TOKEN'] = userdata.get('AZURE_SAS_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn7B1eZlQyY6"
      },
      "outputs": [],
      "source": [
        "from unstructured_client.models.operations import CreateSourceRequest\n",
        "from unstructured_client.models.shared import CreateSourceConnector\n",
        "\n",
        "\n",
        "\n",
        "response = client.sources.create_source(\n",
        "    request=CreateSourceRequest(\n",
        "        create_source_connector=CreateSourceConnector(\n",
        "            name=\"azure_souce_connector\",\n",
        "            type=\"azure\",\n",
        "            config={\n",
        "                \"remote_url\": os.environ['AZURE_REMOTE_URL'],\n",
        "                \"account_name\": os.environ['AZURE_ACCOUNT_NAME'],\n",
        "                \"sas_token\": os.environ['AZURE_SAS_TOKEN'],\n",
        "                \"recursive\": True\n",
        "            }\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "source_connector_id = response.source_connector_information.id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPvcpmm7VoYo"
      },
      "source": [
        "### Create IBM WatsonX Destination Connector\n",
        "\n",
        "\n",
        "To use IBM watsonx.data as your destination, youâ€™ll need a few components configured ahead of time. This setup allows Unstructured to push processed, structured content directly into a table within your watsonx.data environment.\n",
        "\n",
        "Hereâ€™s what youâ€™ll need to have ready:\n",
        "\n",
        "- An **IBM Cloud account** with an active **watsonx.data instance**\n",
        "- An **IBM Cloud API key** for authentication\n",
        "- A **Cloud Object Storage (COS)** instance, including:\n",
        "  - A target **bucket** for data storage\n",
        "  - The **public endpoint**, **region**, and **bucket name**\n",
        "  - **HMAC credentials** (access key ID and secret key)\n",
        "- An **Apache Iceberg catalog** associated with your watsonx.data instance and linked to the COS bucket\n",
        "- A **namespace** (also called a schema) and a **target table** inside the catalog\n",
        "- A column in the table that uniquely identifies records (usually `record_id`)\n",
        "\n",
        "Youâ€™ll also need to configure your catalog in the watsonx.data console via the Infrastructure Manager, ensuring it's:\n",
        "- Connected to COS with correct access credentials\n",
        "- Associated with an engine (e.g., IBM Presto)\n",
        "- Activated and tested with a successful connection status\n",
        "\n",
        "Make sure your table matches the expected Unstructured output schemaâ€”any extra fields in the data that donâ€™t map to table columns will be dropped.\n",
        "\n",
        "> ðŸ“˜ **Tip:** For performance, it's recommended to enable regular metadata cleanup on the table using a small Python script (provided in the official docs).\n",
        "\n",
        "Once your watsonx.data environment is configured, you can create a destination connector using the Python SDK, UI, or API.\n",
        "\n",
        "\n",
        "| **Key** | **Required** | **Description** |\n",
        "|--------|------------|----------------|\n",
        "| `iceberg_endpoint` | âœ… Required | The metastore REST endpoint of the Iceberg catalog (exclude `https://`) |\n",
        "| `object_storage_endpoint` | âœ… Required | Public endpoint of the COS bucket (exclude `https://`) |\n",
        "| `object_storage_region` | âœ… Required | Short region ID of the COS bucket (e.g. `us-east`) |\n",
        "| `iam_api_key` | âœ… Required | API key for your IBM Cloud account |\n",
        "| `access_key_id` | âœ… Required | HMAC access key ID for the COS instance |\n",
        "| `secret_access_key` | âœ… Required | HMAC secret access key paired with `access_key_id` |\n",
        "| `catalog` | âœ… Required | Name of the Iceberg catalog in watsonx.data |\n",
        "| `namespace` | âœ… Required | The schema (namespace) within the catalog |\n",
        "| `table` | âœ… Required | Name of the destination table within the namespace |\n",
        "| `record_id_key` | âŒ Optional | Name of the table column that uniquely identifies records (default: `record_id`) |\n",
        "| `max_retries` | âŒ Optional | Max upload retries (default: 50; allowed: 2â€“500) |\n",
        "| `max_retries_connection` | âŒ Optional | Max connection retries (default: 10; allowed: 2â€“100) |\n",
        "\n",
        "> ðŸ“˜ **Tip:** All endpoint values should be raw (e.g., `s3.us-east.cloud-object-storage.appdomain.cloud`) without any URL schemes (`https://`).\n",
        "\n",
        "To review the full configuration steps, including the table schema and example connector creation code, refer to the [Unstructured IBM watsonx.data destination documentation](https://docs.unstructured.io/api-reference/workflow/destinations/ibm-watsonxdata).\n",
        "\n",
        "\n",
        "\n",
        "Fetching all credentials from Secrets!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-uJIfkcswH3"
      },
      "outputs": [],
      "source": [
        "os.environ[\"IBM_WX_ICEBERG_ENDPOINT\"] = userdata.get(\"IBM_WX_ICEBERG_ENDPOINT\")\n",
        "os.environ[\"IBM_WX_OBJECT_STORAGE_ENDPOINT\"] = userdata.get(\"IBM_WX_OBJECT_STORAGE_ENDPOINT\")\n",
        "os.environ[\"IBM_WX_OBJECT_STORAGE_REGION\"] = userdata.get(\"IBM_WX_OBJECT_STORAGE_REGION\")\n",
        "os.environ[\"IBM_WX_IAM_API_KEY\"] = userdata.get(\"IBM_WX_IAM_API_KEY\")\n",
        "os.environ[\"IBM_WX_ACCESS_KEY_ID\"] = userdata.get(\"IBM_WX_ACCESS_KEY_ID\")\n",
        "os.environ[\"IBM_WX_SECRET_ACCESS_KEY\"] = userdata.get(\"IBM_WX_SECRET_ACCESS_KEY\")\n",
        "os.environ[\"IBM_WX_CATALOG\"] = userdata.get(\"IBM_WX_CATALOG\")\n",
        "os.environ[\"IBM_WX_NAMESPACE\"] = userdata.get(\"IBM_WX_NAMESPACE\")\n",
        "os.environ[\"IBM_WX_TABLE\"] = userdata.get(\"IBM_WX_TABLE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H40eCZGecqo"
      },
      "outputs": [],
      "source": [
        "from unstructured_client.models.operations import CreateDestinationRequest\n",
        "from unstructured_client.models.shared import CreateDestinationConnector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "response = client.destinations.create_destination(\n",
        "    request=CreateDestinationRequest(\n",
        "        create_destination_connector=CreateDestinationConnector(\n",
        "            name=f\"IBM watsonx.data destination {time.time()}\",\n",
        "            type=\"ibm_watsonx_s3\",\n",
        "            config={\n",
        "                \"iceberg_endpoint\": os.environ[\"IBM_WX_ICEBERG_ENDPOINT\"],\n",
        "                \"object_storage_endpoint\": os.environ[\"IBM_WX_OBJECT_STORAGE_ENDPOINT\"],\n",
        "                \"object_storage_region\": os.environ[\"IBM_WX_OBJECT_STORAGE_REGION\"],\n",
        "                \"iam_api_key\": os.environ[\"IBM_WX_IAM_API_KEY\"],\n",
        "                \"access_key_id\": os.environ[\"IBM_WX_ACCESS_KEY_ID\"],\n",
        "                \"secret_access_key\": os.environ[\"IBM_WX_SECRET_ACCESS_KEY\"],\n",
        "                \"catalog\": os.environ[\"IBM_WX_CATALOG\"],\n",
        "                \"namespace\": os.environ[\"IBM_WX_NAMESPACE\"],\n",
        "                \"table\": os.environ[\"IBM_WX_TABLE\"],\n",
        "                \"max_retries\": 50,\n",
        "                \"max_retries_connection\": 10,\n",
        "                \"record_id_key\": \"record_id\"\n",
        "            }\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "destination_connector_id = response.destination_connector_information.id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ982QSwfOQW"
      },
      "source": [
        "## Step 3: Designing Your Data Workflow\n",
        "\n",
        "Once your connectors are in place, it's time to define *how* your data will be processed. This is where workflows come in.\n",
        "\n",
        "In the Unstructured platform, a **workflow** is a directed acyclic graph (DAG) that connects a series of processing stepsâ€”each one represented by a `WorkflowNode`. Think of each node as a small, focused operation in a larger data prep pipeline. These steps can include things like turning a PDF into structured JSON, generating image captions, or creating embeddings for search.\n",
        "\n",
        "Letâ€™s walk through the most common node types youâ€™ll use to shape your workflow.\n",
        "\n",
        "### Partitioning the Raw Data\n",
        "\n",
        "Every workflow starts with a `PARTITION` node. This is the required first step and forms the foundation for everything else. Its job is to take in raw documentsâ€”PDFs, markdown files, emails, you name itâ€”and convert them into a standardized JSON format that the rest of the pipeline can understand.\n",
        "\n",
        "Under the hood, Unstructured offers several partitioning strategies. Hereâ€™s a quick overview:\n",
        "\n",
        "- **Auto** : A smart mode that chooses the best strategy based on the page. It balances performance and cost by dynamically selecting between VLM, High Res, or Fast.\n",
        "- **VLM**: Uses vision-language models to extract content from hard-to-read documentsâ€”like scans with handwriting or complex layouts.\n",
        "- **High Res**: A solid choice for scanned image-based documents that need strong OCR plus layout understanding.\n",
        "- **Fast**: Ideal for well-structured text files like markdown or Word docs. Lightweight and efficient.\n",
        "\n",
        "If youâ€™re curious about the structure of the output, you can explore the JSON schema [here](https://docs.unstructured.io/api-reference/partition/document-elements).\n",
        "\n",
        "\n",
        "### Breaking It Down with Chunking\n",
        "\n",
        "The next node type youâ€™ll usually add is the `CHUNK` node. This node helps divide the document into smaller, coherent pieces of text. Why do this? Because most embedding models (and downstream tools like vector databases) work best with bite-sized chunks that fit within token limits.\n",
        "\n",
        "You can read more about chunking strategies [here](https://docs.unstructured.io/ui/chunking).\n",
        "\n",
        "### Generating Embeddings\n",
        "\n",
        "Finally, thereâ€™s the `EMBED` node. This is where your clean, chunked content gets converted into numerical vector representationsâ€”aka *embeddings*. These vectors can then power similarity search, clustering, or RAG pipelines.\n",
        "\n",
        "To go deeper on how embeddings work in Unstructured, check out the guide [here](https://docs.unstructured.io/ui/embedding).\n",
        "\n",
        "---\n",
        "\n",
        "Once youâ€™ve defined your workflow steps, itâ€™s time to create the workflow and send it off to the platform.\n",
        "\n",
        "Run the next cell to spin it up.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUsSPMedfe-L"
      },
      "outputs": [],
      "source": [
        "from unstructured_client.models.shared import (\n",
        "    WorkflowNode,\n",
        "    WorkflowType,\n",
        "    Schedule\n",
        ")\n",
        "\n",
        "parition_node = WorkflowNode(\n",
        "    name=\"Partitioner\",\n",
        "    subtype=\"vlm\",\n",
        "    type=\"partition\",\n",
        "    settings={\n",
        "        \"provider\": \"anthropic\",\n",
        "        \"model\": \"claude-sonnet-4-5-20250929\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "chunk_node = WorkflowNode(\n",
        "    name='Chunker',\n",
        "    subtype='chunk_by_title',\n",
        "    type=\"chunk\",\n",
        "    settings={\n",
        "        'new_after_n_chars': 1500,\n",
        "        'max_characters': 2048,\n",
        "        'overlap': 0\n",
        "        }\n",
        "    )\n",
        "\n",
        "embedder_node = WorkflowNode(\n",
        "    name='Embedder',\n",
        "    subtype='azure_openai',\n",
        "    type=\"embed\",\n",
        "    settings={\n",
        "        'model_name': 'text-embedding-3-small'\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "response = client.workflows.create_workflow(\n",
        "    request={\n",
        "        \"create_workflow\": {\n",
        "            \"name\": \"Azure-to-snowflake-table-custom-workflow_1373\",\n",
        "            \"source_id\": source_connector_id,\n",
        "            \"destination_id\": destination_connector_id,\n",
        "            \"workflow_type\": WorkflowType.CUSTOM,\n",
        "            \"workflow_nodes\": [\n",
        "                parition_node,\n",
        "                chunk_node,\n",
        "                embedder_node\n",
        "            ],\n",
        "            \"schedule\": Schedule(\"monthly\")\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow_id = response.workflow_information.id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5JfHDo9f5ly"
      },
      "source": [
        "## Step 4: Run the workflow\n",
        "\n",
        "Run the following cell to start running the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD-IZs0af7u4"
      },
      "outputs": [],
      "source": [
        "res = client.workflows.run_workflow(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbOwxNb0gA9t"
      },
      "source": [
        "## Step 5: Get the workflow run's job ID\n",
        "\n",
        "Run the next cell to get the workflow run's job ID, which is needed to poll for job completion later. If successful, Unstructured prints the job's ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXeZ9_aOgD7g"
      },
      "outputs": [],
      "source": [
        "response = client.jobs.list_jobs(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id\n",
        "    }\n",
        ")\n",
        "\n",
        "last_job = response.response_list_jobs[0]\n",
        "job_id = last_job.id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7UZADKXgGO3"
      },
      "source": [
        "## Step 6: Poll for job completion\n",
        "\n",
        "Run the below cell to confirm the job has finished running. If successful, Unstructured prints `\"status\": \"COMPLETED\"` within the information about the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfjWzqFbgIXA"
      },
      "outputs": [],
      "source": [
        "def poll_job_status(job_id, wait_time=30):\n",
        "    while True:\n",
        "        response = client.jobs.get_job(\n",
        "            request={\n",
        "                \"job_id\": job_id\n",
        "            }\n",
        "        )\n",
        "\n",
        "        job = response.job_information\n",
        "\n",
        "        if job.status == \"SCHEDULED\":\n",
        "            print(f\"Job is scheduled, polling again in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        elif job.status == \"IN_PROGRESS\":\n",
        "            print(f\"Job is in progress, polling again in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        else:\n",
        "            print(\"Job is completed\")\n",
        "            break\n",
        "\n",
        "    return job\n",
        "\n",
        "job = poll_job_status(job_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdz4TkllgK-P"
      },
      "source": [
        "## Step 7: View the processed data\n",
        "\n",
        "\n",
        "Once the job is completed, your data is processed, and you can find it in your table under the schema you've specified:\n",
        "\n",
        "![](https://framerusercontent.com/images/V9MIETDCnhERvPsqJmSEaTYEw.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S127fl3sIbd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
