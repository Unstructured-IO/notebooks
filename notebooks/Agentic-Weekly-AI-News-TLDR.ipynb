{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0741738f",
   "metadata": {},
   "source": [
    "# Building an AI Weekly Newsletter Pipeline\n",
    "\n",
    "The AI industry moves fast. Every week brings new research papers, blog posts, product announcements, and technical breakthroughs. Keeping up with developments from ArXiv, OpenAI, Anthropic, Hugging Face, DeepLearning.AI, and other sources can be overwhelming. How do you stay informed without spending hours reading through dozens of publications?\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "AI news comes in many formatsâ€”research papers (PDFs), blog posts (HTML), newsletters, and articles. Manually tracking and summarizing content from multiple sources is time-consuming and often incomplete. What busy professionals need is an automated system that collects relevant AI content and generates a concise weekly summary of what matters.\n",
    "\n",
    "## The Solution\n",
    "\n",
    "This notebook demonstrates an end-to-end pipeline for collecting, processing, and summarizing AI industry content into a weekly newsletter. We use:\n",
    "- **Automated scraping** to collect recent AI papers and blog posts\n",
    "- **Unstructured's hi_res processing** to extract clean text from PDFs and HTML\n",
    "- **AI-powered summarization** to create concise, actionable summaries\n",
    "- **Customizable prompts** so you can tailor the newsletter to your audience\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "A complete weekly AI newsletter system that scrapes the last 7 days of content from ArXiv and leading AI blogs, processes the documents through Unstructured's API, and generates both detailed summaries and an executive brief.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  WEEKLY DATA COLLECTION (Last 7 Days)   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  â€¢ ArXiv Papers (PDFs)                   â”‚\n",
    "â”‚  â€¢ Hugging Face Blog (HTML)              â”‚\n",
    "â”‚  â€¢ OpenAI News (HTML)                    â”‚\n",
    "â”‚  â€¢ DeepLearning.AI Batch (HTML)          â”‚\n",
    "â”‚  â€¢ Anthropic Research (HTML)             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "                 â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      S3 Storage (Collected Content)      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "                 â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    Unstructured API Processing           â”‚\n",
    "â”‚    â€¢ Hi-Res PDF Partitioning             â”‚\n",
    "â”‚    â€¢ HTML Text Extraction                â”‚\n",
    "â”‚    â€¢ Page-Based Chunking                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "                 â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    MongoDB (Structured Content)          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "                 â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    AI Summarization & Newsletter Gen     â”‚\n",
    "â”‚    â€¢ Detailed Publication Summaries      â”‚\n",
    "â”‚    â€¢ Executive Brief (~700 words)        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Note**: In production, you would run the scraping daily via cron job. For this demo, we simulate a week's worth of data collection by scraping 7 days of content in one batch.\n",
    "\n",
    "By the end, you'll have a working system that can automatically generate weekly AI newsletters tailored to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0f4ea2",
   "metadata": {},
   "source": [
    "## Getting Started: Your Unstructured API Key\n",
    "\n",
    "You'll need an Unstructured API key to access the auto document processing platform.\n",
    "\n",
    "### Sign Up and Get Your API Key\n",
    "\n",
    "Visit https://platform.unstructured.io to sign up for a free account, navigate to API Keys in the sidebar, and generate your API key. For Team or Enterprise accounts, select the correct organizational workspace before creating your key.\n",
    "\n",
    "**Need help?** Contact Unstructured Support at support@unstructured.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3007af",
   "metadata": {},
   "source": [
    "## Configuration: Setting Up Your Environment\n",
    "\n",
    "We'll configure your environment with the necessary API keys and credentials to connect to data sources and AI services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101ac08",
   "metadata": {},
   "source": [
    "### Creating a .env File in Google Colab\n",
    "\n",
    "For better security and organization, we'll create a `.env` file directly in your Colab environment. Run the code cell below to create the file with placeholder values, then edit it with your actual credentials.\n",
    "\n",
    "After running the code cell, you'll need to replace each placeholder value (like `your-unstructured-api-key`) with your actual API keys and credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_dotenv_file():\n",
    "    \"\"\"Create a .env file with placeholder values for the user to fill in, only if it doesn't already exist.\"\"\"\n",
    "    \n",
    "    # Check if .env file already exists\n",
    "    if os.path.exists('.env'):\n",
    "        print(\"ğŸ“ .env file already exists - skipping creation\")\n",
    "        print(\"ğŸ’¡ Using existing .env file with current configuration\")\n",
    "        return\n",
    "    \n",
    "    env_content = \"\"\"# Image Processing Pipeline Environment Configuration\n",
    "# Fill in your actual values below\n",
    "# Configuration - Set these explicitly\n",
    "\n",
    "# ===================================================================\n",
    "# AWS CONFIGURATION\n",
    "# ===================================================================\n",
    "AWS_ACCESS_KEY_ID=\"your-aws-access-key-id\"\n",
    "AWS_SECRET_ACCESS_KEY=\"your-aws-secret-access-key\"\n",
    "AWS_REGION=\"us-east-1\"\n",
    "\n",
    "# ===================================================================\n",
    "# UNSTRUCTURED API CONFIGURATION  \n",
    "# ===================================================================\n",
    "UNSTRUCTURED_API_KEY=\"your-unstructured-api-key\"\n",
    "UNSTRUCTURED_API_URL=\"https://platform.unstructuredapp.io/api/v1\"\n",
    "\n",
    "# ===================================================================\n",
    "# MONGODB CONFIGURATION\n",
    "# ===================================================================\n",
    "MONGODB_URI=\"mongodb+srv://<username>:<password>@<host>/?retryWrites=true&w=majority\"\n",
    "MONGODB_DATABASE=\"scraped_publications\"\n",
    "MONGODB_COLLECTION=\"documents\"\n",
    "\n",
    "# ===================================================================\n",
    "# PIPELINE DATA SOURCES\n",
    "# ===================================================================\n",
    "S3_SOURCE_BUCKET=\"example-data-bose-headphones\"\n",
    "\n",
    "# ===================================================================\n",
    "# OPENAI API CONFIGURATION \n",
    "# ===================================================================\n",
    "OPENAI_API_KEY=\"your-openai-api-key\"\n",
    "\"\"\"\n",
    "    \n",
    "    with open('.env', 'w') as f:\n",
    "        f.write(env_content)\n",
    "    \n",
    "    print(\"âœ… Created .env file with placeholder values\")\n",
    "    print(\"ğŸ“ Please edit the .env file and replace the placeholder values with your actual credentials\")\n",
    "    print(\"ğŸ”‘ Required: UNSTRUCTURED_API_KEY, AWS credentials, MongoDB credentials\")\n",
    "    print(\"ğŸ“ S3_SOURCE_BUCKET should point to your PDF documents\")\n",
    "    print(\"ğŸ¤– OPENAI_API_KEY needed for AI-powered image descriptions\")\n",
    "\n",
    "create_dotenv_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190d691",
   "metadata": {},
   "source": [
    "### Installing Required Dependencies\n",
    "\n",
    "Installing the Python packages needed: Unstructured client, MongoDB connector, AWS SDK, OpenAI integration, and document processing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def ensure_notebook_deps() -> None:\n",
    "    packages = [\n",
    "        \"jupytext\",\n",
    "        \"python-dotenv\", \n",
    "        \"unstructured-client\",\n",
    "        \"boto3\",\n",
    "        \"PyYAML\",\n",
    "        \"langchain\",\n",
    "        \"langchain-openai\",\n",
    "        \"pymongo\",\n",
    "        \"firecrawl-py\",\n",
    "        \"arxiv\",\n",
    "        \"python-dateutil\"\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *packages])\n",
    "    except Exception:\n",
    "        # If install fails, continue; imports below will surface actionable errors\n",
    "        pass\n",
    "\n",
    "# Install notebook dependencies (safe no-op if present)\n",
    "ensure_notebook_deps()\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import zipfile\n",
    "import tempfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError, NoCredentialsError\n",
    "\n",
    "from unstructured_client import UnstructuredClient\n",
    "from unstructured_client.models.operations import (\n",
    "    CreateSourceRequest,\n",
    "    CreateDestinationRequest,\n",
    "    CreateWorkflowRequest\n",
    ")\n",
    "from unstructured_client.models.shared import (\n",
    "    CreateSourceConnector,\n",
    "    CreateDestinationConnector,\n",
    "    WorkflowNode,\n",
    "    WorkflowType,\n",
    "    CreateWorkflow\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# ENVIRONMENT CONFIGURATION\n",
    "# =============================================================================\n",
    "# Load from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration constants\n",
    "SKIPPED = \"SKIPPED\"\n",
    "UNSTRUCTURED_API_URL = os.getenv(\"UNSTRUCTURED_API_URL\", \"https://platform.unstructuredapp.io/api/v1\")\n",
    "\n",
    "# Get environment variables\n",
    "UNSTRUCTURED_API_KEY = os.getenv(\"UNSTRUCTURED_API_KEY\")\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")  # No default value as requested\n",
    "S3_SOURCE_BUCKET = os.getenv(\"S3_SOURCE_BUCKET\")\n",
    "S3_DESTINATION_BUCKET = os.getenv(\"S3_DESTINATION_BUCKET\")\n",
    "S3_OUTPUT_PREFIX = os.getenv(\"S3_OUTPUT_PREFIX\", \"\")\n",
    "MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
    "MONGODB_DATABASE = os.getenv(\"MONGODB_DATABASE\")\n",
    "MONGODB_COLLECTION = os.getenv(\"MONGODB_COLLECTION\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "FIRECRAWL_API_KEY = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "\n",
    "# Validation\n",
    "REQUIRED_VARS = {\n",
    "    \"UNSTRUCTURED_API_KEY\": UNSTRUCTURED_API_KEY,\n",
    "    \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n",
    "    \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n",
    "    \"AWS_REGION\": AWS_REGION,\n",
    "    \"MONGODB_URI\": MONGODB_URI,\n",
    "    \"MONGODB_DATABASE\": MONGODB_DATABASE,\n",
    "    \"MONGODB_COLLECTION\": MONGODB_COLLECTION,\n",
    "    \"S3_SOURCE_BUCKET\": S3_SOURCE_BUCKET,\n",
    "}\n",
    "\n",
    "missing_vars = [key for key, value in REQUIRED_VARS.items() if not value]\n",
    "if missing_vars:\n",
    "    print(f\"âŒ Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "    print(\"Please set these environment variables or create a .env file with your credentials.\")\n",
    "    raise ValueError(f\"Missing required environment variables: {missing_vars}\")\n",
    "\n",
    "print(\"âœ… Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda35e68",
   "metadata": {},
   "source": [
    "## AWS S3: Your Content Collection Repository\n",
    "\n",
    "Now that we have our environment configured, let's set up S3 as the central repository for collected AI content. The scraping pipeline will deposit PDFs (ArXiv papers) and HTML files (blog posts) into your S3 bucket, where they'll be ready for processing by the Unstructured API.\n",
    "\n",
    "### What You Need\n",
    "\n",
    "**An existing S3 bucket** to store scraped AI content. The following sections will automatically populate this bucket with:\n",
    "- Recent AI/ML research papers from ArXiv (PDF format)\n",
    "- Blog posts from Hugging Face, OpenAI, DeepLearning.AI, and Anthropic (HTML format)\n",
    "\n",
    "> **Note**: You'll need an AWS account with S3 access, an IAM user with read/write permissions, and your access keys (Access Key ID and Secret Access Key). For detailed S3 setup instructions, see the [Unstructured S3 source connector documentation](https://docs.unstructured.io/api-reference/api-services/source-connectors/s3).\n",
    "\n",
    "### Weekly Collection Strategy\n",
    "\n",
    "In production, you would run the scraping scripts daily (via cron job or scheduled Lambda function) to continuously collect fresh AI content. For this demo notebook, we scrape the **last 7 days** of content in one batch to simulate a week's worth of data collection. You can adjust the `DAYS_BACK` parameter in each scraping cell to collect more or less content.\n",
    "\n",
    "**Adaptable to Other Use Cases**: This same approach can be adapted for competitor tracking, industry news monitoring, internal document aggregation, or any scenario where you need to collect and summarize content from multiple sources regularly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7173adf",
   "metadata": {},
   "source": [
    "### Example Document Content\n",
    "\n",
    "The following sections will scrape AI research papers and blog posts, automatically populating your S3 bucket with fresh content for processing.\n",
    "\n",
    "[[IMG:EXAMPLE_DOCUMENT_IMAGE]]  # Image disabled - use --include-images to enable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7bed5",
   "metadata": {},
   "source": [
    "## Automated Content Scraping: Gathering AI Industry Intelligence\n",
    "\n",
    "The first step in building a weekly AI newsletter is collecting content from multiple sources. This section demonstrates automated scraping that gathers the **last 7 days** of AI research papers and blog posts, simulating what would typically run daily in production.\n",
    "\n",
    "**Data Sources:**\n",
    "1. **ArXiv** - Recent AI/ML research papers (PDFs)\n",
    "   - Papers from cs.AI, cs.LG, cs.CL, cs.CV, cs.NE categories\n",
    "   - Filtered by keywords: \"artificial intelligence\" OR \"machine learning\"\n",
    "\n",
    "2. **AI Company Blogs** - Blog posts (HTML)\n",
    "   - Hugging Face: Model releases, tutorials, and community posts\n",
    "   - OpenAI: Product announcements and research updates\n",
    "   - DeepLearning.AI: The Batch weekly newsletter issues\n",
    "   - Anthropic: Claude updates and research papers\n",
    "\n",
    "**Process Flow:**\n",
    "```\n",
    "ArXiv API â†’ PDFs â†’ S3\n",
    "Firecrawl API â†’ Blog HTML â†’ S3\n",
    "                     â†“\n",
    "            Unstructured Processing â†’ MongoDB â†’ AI Summarization\n",
    "```\n",
    "\n",
    "**Production Deployment**: In a real implementation, you would schedule these scraping scripts to run daily (e.g., via cron job, AWS Lambda, or GitHub Actions). Each day's content would accumulate in S3, and at the end of the week, you'd run the processing and summarization pipeline to generate your newsletter.\n",
    "\n",
    "**For This Demo**: We're scraping 7 days of content in one batch to simulate a week's worth of daily collection. This gives us enough diverse content to demonstrate the full pipeline without waiting a week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c0cbc",
   "metadata": {},
   "source": [
    "### Scraping ArXiv Research Papers\n",
    "\n",
    "This cell scrapes recent AI/ML papers from ArXiv, filters them by category, and uploads PDFs directly to your S3 bucket. The default configuration collects papers from the **last 7 days** to simulate a week's worth of content.\n",
    "\n",
    "**Configuration (Customize These):**\n",
    "- `SEARCH_QUERY`: Keywords to find relevant papers (default: \"artificial intelligence OR machine learning\")\n",
    "- `MAX_RESULTS`: Number of papers to retrieve (default: 10)\n",
    "- `ARXIV_CATEGORIES`: Categories to filter (default: cs.AI, cs.LG, cs.CL, cs.CV, cs.NE)\n",
    "- `DAYS_BACK`: How far back to search (default: 7 days)\n",
    "\n",
    "**What It Does:**\n",
    "1. Searches ArXiv API for papers matching criteria within the date range\n",
    "2. Filters by AI/ML categories\n",
    "3. Downloads PDFs for matching papers\n",
    "4. Uploads PDFs to S3 under `arxiv/papers/` with metadata\n",
    "5. Provides summary statistics\n",
    "\n",
    "**Customization**: Modify the search query to focus on specific topics (e.g., \"large language models\", \"computer vision\", \"reinforcement learning\"), adjust the date range, or change categories to match your newsletter's focus area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Customize these parameters\n",
    "# ============================================================\n",
    "\n",
    "# Search configuration\n",
    "SEARCH_QUERY = \"artificial intelligence OR machine learning\"\n",
    "MAX_RESULTS = 10  # Number of papers to retrieve\n",
    "DAYS_BACK = 7  # How many days back to search\n",
    "ARXIV_CATEGORIES = [\"cs.AI\", \"cs.LG\", \"cs.CL\", \"cs.CV\", \"cs.NE\"]  # AI/ML categories\n",
    "\n",
    "# ============================================================\n",
    "# ArXiv Scraping Logic\n",
    "# ============================================================\n",
    "\n",
    "import arxiv\n",
    "from datetime import datetime, timedelta\n",
    "from io import BytesIO\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“š ARXIV PAPER SCRAPING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate date threshold (timezone-aware to match arxiv library)\n",
    "from datetime import timezone\n",
    "date_threshold = datetime.now(timezone.utc) - timedelta(days=DAYS_BACK)\n",
    "print(f\"\\nğŸ” Searching for papers from the last {DAYS_BACK} days\")\n",
    "print(f\"   Query: {SEARCH_QUERY}\")\n",
    "print(f\"   Max results: {MAX_RESULTS}\")\n",
    "print(f\"   Categories: {', '.join(ARXIV_CATEGORIES)}\")\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "\n",
    "# Search ArXiv\n",
    "print(f\"\\nğŸ“¥ Searching ArXiv...\")\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=SEARCH_QUERY,\n",
    "    max_results=MAX_RESULTS,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = list(client.results(search))\n",
    "print(f\"âœ… Found {len(results)} papers\")\n",
    "\n",
    "# Filter and upload papers\n",
    "scraped_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for paper in results:\n",
    "    # Check if paper is in desired categories\n",
    "    categories = [cat.split('.')[-1] for cat in paper.categories]\n",
    "    if not any(cat in ARXIV_CATEGORIES for cat in paper.categories):\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Check if paper is recent enough (both datetimes are now timezone-aware)\n",
    "    if paper.published < date_threshold:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Processing: {paper.title[:60]}...\")\n",
    "    print(f\"   ArXiv ID: {paper.entry_id.split('/')[-1]}\")\n",
    "    print(f\"   Published: {paper.published.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Categories: {', '.join(paper.categories[:3])}\")\n",
    "    \n",
    "    try:\n",
    "        # Download PDF\n",
    "        pdf_url = paper.pdf_url\n",
    "        pdf_response = requests.get(pdf_url, timeout=30)\n",
    "        pdf_content = pdf_response.content\n",
    "        \n",
    "        # Generate S3 key\n",
    "        arxiv_id = paper.entry_id.split('/')[-1].replace('.', 'v')\n",
    "        s3_key = f\"arxiv/papers/{arxiv_id}.pdf\"\n",
    "        \n",
    "        # Upload to S3\n",
    "        s3.put_object(\n",
    "            Bucket=S3_SOURCE_BUCKET,\n",
    "            Key=s3_key,\n",
    "            Body=pdf_content,\n",
    "            ContentType='application/pdf',\n",
    "            Metadata={\n",
    "                'title': paper.title[:1000],  # S3 metadata has size limits\n",
    "                'published': paper.published.isoformat(),\n",
    "                'arxiv_id': arxiv_id,\n",
    "                'source': 'arxiv'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Uploaded to s3://{S3_SOURCE_BUCKET}/{s3_key}\")\n",
    "        scraped_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {str(e)[:100]}\")\n",
    "        skipped_count += 1\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… ARXIV SCRAPING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   ğŸ“¥ Papers scraped: {scraped_count}\")\n",
    "print(f\"   â­ï¸  Papers skipped: {skipped_count}\")\n",
    "print(f\"   ğŸ“¦ S3 Bucket: {S3_SOURCE_BUCKET}\")\n",
    "print(f\"   ğŸ“ S3 Prefix: arxiv/papers/\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833932d8",
   "metadata": {},
   "source": [
    "### Scraping AI Company Blogs with Firecrawl\n",
    "\n",
    "This cell uses Firecrawl to scrape recent blog posts from leading AI companies, extracting clean HTML content. The default configuration collects posts from the **last 7 days** across multiple sources.\n",
    "\n",
    "**Blog Sources (Pre-configured):**\n",
    "- **Hugging Face** (`https://huggingface.co/blog`) - Model releases, tutorials, community posts\n",
    "- **OpenAI** (`https://openai.com/news/`) - Product announcements and research updates\n",
    "- **DeepLearning.AI** (`https://www.deeplearning.ai/the-batch/`) - Weekly Batch newsletter issues\n",
    "- **Anthropic** (`https://www.anthropic.com/research`) - Claude updates and research papers\n",
    "\n",
    "**Configuration (Customize This):**\n",
    "- `DAYS_BACK`: How many days of recent posts to retrieve (default: 7 days)\n",
    "- Modify `BLOG_SOURCES` dictionary to add/remove sources\n",
    "\n",
    "**What It Does:**\n",
    "1. Scrapes blog directory pages using Firecrawl with link extraction\n",
    "2. Filters blog post URLs using source-specific rules (excludes images, navigation pages, etc.)\n",
    "3. Scrapes individual post content with 1-second delay between requests\n",
    "4. Uploads clean HTML to S3 under `blog-posts/{source}/` with metadata\n",
    "5. Provides summary statistics by source\n",
    "\n",
    "**Why Firecrawl?** Firecrawl handles JavaScript-rendered content, provides clean HTML output, and respects website structures, making it ideal for scraping modern AI company blogs.\n",
    "\n",
    "**Extensibility**: Add more sources by extending the `BLOG_SOURCES` dictionary with additional blog URLs and configuring appropriate filtering rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9fa076",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Customize these parameters\n",
    "# ============================================================\n",
    "\n",
    "# Scraping configuration\n",
    "DAYS_BACK = 7  # How many days of recent posts to retrieve\n",
    "\n",
    "# Blog source URLs (pre-configured)\n",
    "BLOG_SOURCES = {\n",
    "    \"huggingface\": {\n",
    "        \"name\": \"Hugging Face\",\n",
    "        \"directory_url\": \"https://huggingface.co/blog\",\n",
    "        \"icon\": \"ğŸ¤—\"\n",
    "    },\n",
    "    \"openai\": {\n",
    "        \"name\": \"OpenAI\",\n",
    "        \"directory_url\": \"https://openai.com/news/\",\n",
    "        \"icon\": \"ğŸš€\"\n",
    "    },\n",
    "    \"deeplearning\": {\n",
    "        \"name\": \"DeepLearning.AI\",\n",
    "        \"directory_url\": \"https://www.deeplearning.ai/the-batch/\",\n",
    "        \"icon\": \"ğŸ“š\"\n",
    "    },\n",
    "    \"anthropic\": {\n",
    "        \"name\": \"Anthropic\",\n",
    "        \"directory_url\": \"https://www.anthropic.com/research\",\n",
    "        \"icon\": \"ğŸ”¬\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# Blog Scraping Logic with Firecrawl\n",
    "# ============================================================\n",
    "\n",
    "from firecrawl import Firecrawl\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸŒ BLOG SCRAPING WITH FIRECRAWL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Helper function to convert Firecrawl Document objects to dictionaries\n",
    "def convert_document_to_dict(doc):\n",
    "    \"\"\"Convert Firecrawl Document object to dictionary format.\"\"\"\n",
    "    if isinstance(doc, dict):\n",
    "        return doc\n",
    "    \n",
    "    # Handle Document object from newer firecrawl-py versions\n",
    "    result_dict = {}\n",
    "    \n",
    "    # Get attributes from the Document object\n",
    "    if hasattr(doc, 'markdown'):\n",
    "        result_dict['markdown'] = doc.markdown\n",
    "    if hasattr(doc, 'html'):\n",
    "        result_dict['html'] = doc.html\n",
    "    if hasattr(doc, 'links'):\n",
    "        result_dict['links'] = doc.links if doc.links else []\n",
    "    if hasattr(doc, 'metadata'):\n",
    "        # metadata is also an object, convert to dict\n",
    "        metadata_obj = doc.metadata\n",
    "        if metadata_obj:\n",
    "            if isinstance(metadata_obj, dict):\n",
    "                result_dict['metadata'] = metadata_obj\n",
    "            else:\n",
    "                # Convert metadata object to dict using __dict__ or vars()\n",
    "                result_dict['metadata'] = vars(metadata_obj) if hasattr(metadata_obj, '__dict__') else {}\n",
    "        else:\n",
    "            result_dict['metadata'] = {}\n",
    "    if hasattr(doc, 'extract'):\n",
    "        result_dict['json'] = doc.extract\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "# Filter blog links to exclude non-blog content\n",
    "def filter_blog_links(links, source_key, directory_url):\n",
    "    \"\"\"Filter links to find actual blog posts, excluding images, profiles, etc.\"\"\"\n",
    "    # Blacklist of specific URLs to exclude\n",
    "    EXCLUDED_URLS = [\n",
    "        'https://huggingface.co/blog/community',\n",
    "        'https://anthropic.com/press-kit',\n",
    "    ]\n",
    "    \n",
    "    # Extract domain from directory URL\n",
    "    directory_domain = urlparse(directory_url).netloc\n",
    "    \n",
    "    blog_links = []\n",
    "    \n",
    "    for link in links:\n",
    "        if not isinstance(link, str):\n",
    "            continue\n",
    "        \n",
    "        # Skip non-HTTP protocols\n",
    "        if not link.startswith('http'):\n",
    "            continue\n",
    "        \n",
    "        # Skip image files\n",
    "        if any(link.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.svg', '.webp']):\n",
    "            continue\n",
    "        \n",
    "        # Skip CDN and avatar URLs\n",
    "        if 'cdn-avatars' in link or '/assets/' in link:\n",
    "            continue\n",
    "        \n",
    "        # Only include links from the same domain\n",
    "        link_domain = urlparse(link).netloc\n",
    "        if link_domain != directory_domain:\n",
    "            continue\n",
    "        \n",
    "        # Source-specific filtering\n",
    "        if source_key == 'huggingface':\n",
    "            # Must have /blog/ and content after it (not just directory or community)\n",
    "            if '/blog/' in link:\n",
    "                blog_parts = link.split('/blog/')\n",
    "                if len(blog_parts) > 1 and blog_parts[1].strip('/'):\n",
    "                    # Exclude community page\n",
    "                    if link not in EXCLUDED_URLS:\n",
    "                        blog_links.append(link)\n",
    "                        \n",
    "        elif source_key == 'deeplearning':\n",
    "            # Must have /the-batch/ but NOT /tag/ (tag pages are navigation)\n",
    "            if '/the-batch/' in link and '/tag/' not in link:\n",
    "                blog_links.append(link)\n",
    "                \n",
    "        elif source_key == 'anthropic':\n",
    "            # Include both /news/ and /research/ posts\n",
    "            if '/news/' in link or '/research/' in link:\n",
    "                if link not in EXCLUDED_URLS:\n",
    "                    blog_links.append(link)\n",
    "                    \n",
    "        elif source_key == 'openai':\n",
    "            # OpenAI uses /index/ for actual articles\n",
    "            if '/index/' in link:\n",
    "                # Exclude category pages that end with these paths\n",
    "                category_pages = ['/product-releases/', '/research/', '/safety-alignment/', '/news/']\n",
    "                is_category = any(link.endswith(cat) for cat in category_pages)\n",
    "                if not is_category:\n",
    "                    blog_links.append(link)\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    return sorted(list(set(blog_links)))\n",
    "\n",
    "# Initialize Firecrawl and S3\n",
    "firecrawl_client = Firecrawl(api_key=FIRECRAWL_API_KEY)\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "\n",
    "date_threshold = datetime.now() - timedelta(days=DAYS_BACK)\n",
    "print(f\"\\nğŸ” Scraping posts from the last {DAYS_BACK} days\")\n",
    "print(f\"   Sources: {len(BLOG_SOURCES)}\")\n",
    "\n",
    "total_scraped = 0\n",
    "\n",
    "for source_key, source_info in BLOG_SOURCES.items():\n",
    "    icon = source_info[\"icon\"]\n",
    "    name = source_info[\"name\"]\n",
    "    directory_url = source_info[\"directory_url\"]\n",
    "    \n",
    "    print(f\"\\n{icon} {name}\")\n",
    "    print(f\"   {'â”€'*50}\")\n",
    "    print(f\"   ğŸ“ {directory_url}\")\n",
    "    \n",
    "    try:\n",
    "        # Scrape directory page with link extraction\n",
    "        print(f\"   ğŸ”„ Scraping directory...\")\n",
    "        directory_result_raw = firecrawl_client.scrape(\n",
    "            url=directory_url,\n",
    "            formats=[\"markdown\", \"html\", \"links\"],\n",
    "            only_main_content=True\n",
    "        )\n",
    "        \n",
    "        # Convert Document to dict\n",
    "        directory_result = convert_document_to_dict(directory_result_raw)\n",
    "        \n",
    "        if not directory_result:\n",
    "            print(f\"   âŒ Failed to scrape directory\")\n",
    "            continue\n",
    "        \n",
    "        # Extract and filter blog links\n",
    "        all_links = directory_result.get('links', [])\n",
    "        blog_links = filter_blog_links(all_links, source_key, directory_url)\n",
    "        \n",
    "        print(f\"   âœ… Found {len(blog_links)} blog post links\")\n",
    "        \n",
    "        # Limit to 10 posts per source for demo\n",
    "        post_urls = blog_links[:10]\n",
    "        \n",
    "        # Scrape individual posts\n",
    "        scraped_count = 0\n",
    "        for post_url in post_urls:\n",
    "            try:\n",
    "                # Add delay to be respectful\n",
    "                import time\n",
    "                time.sleep(1)\n",
    "                \n",
    "                print(f\"   ğŸ“¥ Scraping: {post_url[:60]}...\")\n",
    "                \n",
    "                # Scrape post with HTML format\n",
    "                post_result_raw = firecrawl_client.scrape(\n",
    "                    url=post_url,\n",
    "                    formats=[\"html\"],\n",
    "                    only_main_content=True\n",
    "                )\n",
    "                \n",
    "                # Convert Document to dict\n",
    "                post_result = convert_document_to_dict(post_result_raw)\n",
    "                \n",
    "                if not post_result or not post_result.get('html'):\n",
    "                    print(f\"      âš ï¸  No HTML returned\")\n",
    "                    continue\n",
    "                \n",
    "                html_content = post_result['html']\n",
    "                \n",
    "                # Generate S3 key\n",
    "                url_path = urlparse(post_url).path.strip('/').replace('/', '_')\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                s3_key = f\"blog-posts/{source_key}/{url_path}_{timestamp}.html\"\n",
    "                \n",
    "                # Upload to S3\n",
    "                s3.put_object(\n",
    "                    Bucket=S3_SOURCE_BUCKET,\n",
    "                    Key=s3_key,\n",
    "                    Body=html_content.encode('utf-8'),\n",
    "                    ContentType='text/html',\n",
    "                    Metadata={\n",
    "                        'url': post_url[:1000],\n",
    "                        'source': source_key,\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                print(f\"      âœ… Uploaded to S3\")\n",
    "                scraped_count += 1\n",
    "                total_scraped += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      âŒ Error: {str(e)[:100]}\")\n",
    "        \n",
    "        print(f\"   ğŸ“Š Scraped {scraped_count} posts from {name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error scraping {name}: {str(e)[:100]}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… BLOG SCRAPING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   ğŸ“¥ Total posts scraped: {total_scraped}\")\n",
    "print(f\"   ğŸ“¦ S3 Bucket: {S3_SOURCE_BUCKET}\")\n",
    "print(f\"   ğŸ“ S3 Prefix: blog-posts/\")\n",
    "print(f\"\\nğŸ’¡ Note: Posts are now ready for Unstructured processing!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4aa756",
   "metadata": {},
   "source": [
    "## S3 Source Connector\n",
    "\n",
    "Creating the connection to your S3 document repository. This connector will authenticate with your bucket, discover PDF files, and stream them to the processing pipeline.\n",
    "\n",
    "**Recursive Processing**: The connector is configured with `recursive: true` to access files within nested folder structures, ensuring comprehensive document discovery across your entire S3 bucket hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d2945",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_s3_source_connector():\n",
    "    \"\"\"Create an S3 source connector for PDF documents.\"\"\"\n",
    "    try:\n",
    "        if not S3_SOURCE_BUCKET:\n",
    "            raise ValueError(\"S3_SOURCE_BUCKET is required (bucket name, s3:// URL, or https:// URL)\")\n",
    "        value = S3_SOURCE_BUCKET.strip()\n",
    "\n",
    "        if value.startswith(\"s3://\"):\n",
    "            s3_style = value if value.endswith(\"/\") else value + \"/\"\n",
    "        elif value.startswith(\"http://\") or value.startswith(\"https://\"):\n",
    "            parsed = urlparse(value)\n",
    "            host = parsed.netloc\n",
    "            path = parsed.path or \"/\"\n",
    "            bucket = host.split(\".s3.\")[0]\n",
    "            s3_style = f\"s3://{bucket}{path if path.endswith('/') else path + '/'}\"\n",
    "        else:\n",
    "            s3_style = f\"s3://{value if value.endswith('/') else value + '/'}\"\n",
    "        \n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            response = client.sources.create_source(\n",
    "                request=CreateSourceRequest(\n",
    "                    create_source_connector=CreateSourceConnector(\n",
    "                        name=\"<name>\",\n",
    "                        type=\"s3\",\n",
    "                        config={\n",
    "                            \"remote_url\": s3_style,\n",
    "                            \"recursive\": True, \n",
    "                            \"key\": AWS_ACCESS_KEY_ID,\n",
    "                            \"secret\": AWS_SECRET_ACCESS_KEY,\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        source_id = response.source_connector_information.id\n",
    "        print(f\"âœ… Created S3 PDF source connector: {source_id} -> {s3_style}\")\n",
    "        return source_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating S3 source connector: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create S3 source connector\n",
    "source_id = create_s3_source_connector()\n",
    "\n",
    "if source_id:\n",
    "    print(f\"ğŸ“ S3 source connector ready to read PDF documents from: {S3_SOURCE_BUCKET}\")\n",
    "else:\n",
    "    print(\"âŒ Failed to create S3 source connector - check your credentials and bucket configuration\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59420d0a",
   "metadata": {},
   "source": [
    "## MongoDB: Your Document Database\n",
    "\n",
    "MongoDB serves as the destination where our processed content will be stored. This NoSQL database will store the extracted text content, metadata, and document structure from PDFs and HTML files processed through the pipeline.\n",
    "\n",
    "### What You Need\n",
    "\n",
    "**MongoDB Atlas cluster** with connection string authentication. MongoDB Atlas is a fully managed cloud database service that provides reliability, scalability, and flexible document storage for AI-powered applications.\n",
    "\n",
    "### MongoDB Requirements\n",
    "\n",
    "Your MongoDB setup needs:\n",
    "\n",
    "- A MongoDB Atlas cluster (M10+ tier recommended for production, M0 free tier for testing)\n",
    "- Network access configured to allow connections from your application\n",
    "- Database user with read/write permissions\n",
    "- Connection string with proper authentication credentials\n",
    "\n",
    "### Why MongoDB for Newsletter Pipeline\n",
    "\n",
    "MongoDB's flexible document structure is ideal for storing diverse content types from multiple sources (ArXiv papers, blog posts, etc.). Each document in the collection contains the full text content and metadata (source, date, URL) ready for summarization.\n",
    "\n",
    "The destination collection structure is optimized for newsletter generation:\n",
    "```json\n",
    "{\n",
    "  \"_id\": \"unique_identifier\",\n",
    "  \"element_id\": \"element_uuid\",\n",
    "  \"type\": \"NarrativeText\",\n",
    "  \"text\": \"Full text content from document\",\n",
    "  \"metadata\": {\n",
    "    \"filename\": \"arxiv_paper.pdf\",\n",
    "    \"source\": \"arxiv\",\n",
    "    \"url\": \"https://arxiv.org/abs/...\",\n",
    "    \"downloaded_at\": \"2025-09-30T...\",\n",
    "    \"processed_at\": \"2025-09-30T...\",\n",
    "    \"filetype\": \"pdf\",\n",
    "    \"page_number\": 1,\n",
    "    \"languages\": [\"en\"]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Example document transformation:\n",
    "```\n",
    "Before: [PDF file in S3: arxiv_2501.12345.pdf]\n",
    "\n",
    "After: {\n",
    "  \"_id\": \"uuid_001\",\n",
    "  \"type\": \"Title\",\n",
    "  \"text\": \"Advanced Techniques in Large Language Model Training\",\n",
    "  \"metadata\": {\n",
    "    \"filename\": \"arxiv_2501.12345.pdf\",\n",
    "    \"source\": \"arxiv\",\n",
    "    \"arxiv_id\": \"2501.12345\",\n",
    "    \"downloaded_at\": \"2025-09-25T10:30:00Z\",\n",
    "    \"filetype\": \"pdf\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Clean collection on every run**: The pipeline clears the collection before processing to ensure fresh data for each newsletter generation cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e0e6f7",
   "metadata": {},
   "source": [
    "### Example Output Data Structure\n",
    "\n",
    "After processing, the pipeline creates a MongoDB collection containing extracted text content and metadata from documents. The processed data includes element types (Title, NarrativeText, ListItem, etc.), full text content, source metadata, and processing timestamps for downstream summarization and newsletter generation.\n",
    "\n",
    "[[IMG:EXAMPLE_OUTPUT_IMAGE]]  # Image disabled - use --include-images to enable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf2bba",
   "metadata": {},
   "source": [
    "## MongoDB Configuration and Collection Setup\n",
    "\n",
    "Before processing documents, we validate the MongoDB connection and prepare the collection for fresh data processing.\n",
    "\n",
    "**Configuration Validation:**\n",
    "- Verifies MongoDB connection string format and connectivity\n",
    "- Confirms database and collection name settings\n",
    "- Validates environment variable completeness\n",
    "\n",
    "**Collection Management:**\n",
    "- Connects to the specified database (creates automatically if needed)\n",
    "- Creates the collection if it doesn't exist\n",
    "- Clears existing documents for fresh processing\n",
    "- Ensures proper document storage capabilities\n",
    "\n",
    "**Environment Variables Required:**\n",
    "- `MONGODB_URI`: Your MongoDB connection string (mongodb:// or mongodb+srv://)\n",
    "- `MONGODB_DATABASE`: Target database name\n",
    "- `MONGODB_COLLECTION`: Target collection name\n",
    "\n",
    "This preprocessing step ensures your MongoDB collection is properly configured and ready to receive processed documents from the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c3b82",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def verify_collection_exists():\n",
    "    \"\"\"Verify that the MongoDB collection exists and is properly configured.\"\"\"\n",
    "    print(f\"ğŸ” Verifying collection '{MONGODB_COLLECTION}' exists...\")\n",
    "    \n",
    "    try:\n",
    "        from pymongo import MongoClient\n",
    "        \n",
    "        # Initialize MongoDB client\n",
    "        client = MongoClient(MONGODB_URI)\n",
    "        db = client[MONGODB_DATABASE]\n",
    "        \n",
    "        # Check if collection exists\n",
    "        existing_collections = db.list_collection_names()\n",
    "        \n",
    "        if MONGODB_COLLECTION not in existing_collections:\n",
    "            print(f\"âŒ Collection '{MONGODB_COLLECTION}' does not exist!\")\n",
    "            return False\n",
    "        \n",
    "        # Get collection info to verify configuration\n",
    "        try:\n",
    "            collection = db[MONGODB_COLLECTION]\n",
    "            \n",
    "            # Count documents (optional check)\n",
    "            doc_count = collection.count_documents({})\n",
    "            print(f\"âœ… Collection '{MONGODB_COLLECTION}' exists and is accessible\")\n",
    "            print(f\"ğŸ“„ Current document count: {doc_count}\")\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as collection_error:\n",
    "            print(f\"âš ï¸ Collection exists but may have access issues: {collection_error}\")\n",
    "            return True  # Don't fail if we can't get detailed info\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âš ï¸ MongoDB client not available - collection verification skipped\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Warning: Could not verify collection: {e}\")\n",
    "        return True  # Don't fail the pipeline for verification issues\n",
    "\n",
    "def initialize_mongodb_collection():\n",
    "    \"\"\"Initialize MongoDB collection - create database and collection if needed, then clear existing data for fresh start.\"\"\"\n",
    "    print(\"ğŸ—ï¸ Initializing MongoDB collection...\")\n",
    "    \n",
    "    try:\n",
    "        from pymongo import MongoClient\n",
    "        \n",
    "        # Initialize client\n",
    "        client = MongoClient(MONGODB_URI)\n",
    "        \n",
    "        # Access database (will be created automatically if it doesn't exist)\n",
    "        db = client[MONGODB_DATABASE]\n",
    "        print(f\"âœ… Connected to database '{MONGODB_DATABASE}'\")\n",
    "        \n",
    "        # List existing collections\n",
    "        existing_collections = db.list_collection_names()\n",
    "        \n",
    "        # Step 1: Ensure collection exists (create if needed)\n",
    "        if MONGODB_COLLECTION not in existing_collections:\n",
    "            print(f\"ğŸ“ Creating collection '{MONGODB_COLLECTION}'...\")\n",
    "            \n",
    "            # Create the collection (MongoDB creates it automatically on first write)\n",
    "            db.create_collection(MONGODB_COLLECTION)\n",
    "            print(f\"âœ… Created collection '{MONGODB_COLLECTION}'\")\n",
    "        else:\n",
    "            print(f\"âœ… Collection '{MONGODB_COLLECTION}' already exists\")\n",
    "        \n",
    "        # Step 2: Clear existing data\n",
    "        collection = db[MONGODB_COLLECTION]\n",
    "        delete_result = collection.delete_many({})\n",
    "        \n",
    "        deleted_count = delete_result.deleted_count\n",
    "        print(f\"ğŸ—‘ï¸ Cleared {deleted_count} existing documents\")\n",
    "            \n",
    "        print(f\"âœ… Collection '{MONGODB_COLLECTION}' is ready for document processing\")\n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âš ï¸ MongoDB client not available - install with: pip install pymongo\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error initializing MongoDB collection: {e}\")\n",
    "        print(\"ğŸ’¡ Troubleshooting:\")\n",
    "        print(\"   1. Verify your MONGODB_URI connection string is correct\")\n",
    "        print(\"   2. Ensure your MongoDB cluster allows connections from your IP\")\n",
    "        print(\"   3. Check that your database user has appropriate permissions\")\n",
    "        print(f\"   4. Verify database name '{MONGODB_DATABASE}' and collection '{MONGODB_COLLECTION}'\")\n",
    "        return False\n",
    "\n",
    "def run_mongodb_preprocessing():\n",
    "    \"\"\"Validate MongoDB configuration and initialize collection for fresh processing.\"\"\"\n",
    "    print(\"ğŸ”§ Running MongoDB preprocessing...\")\n",
    "    \n",
    "    try:\n",
    "        # Validate required environment variables\n",
    "        required_vars = [\n",
    "            (\"MONGODB_URI\", MONGODB_URI),\n",
    "            (\"MONGODB_DATABASE\", MONGODB_DATABASE),\n",
    "            (\"MONGODB_COLLECTION\", MONGODB_COLLECTION)\n",
    "        ]\n",
    "        \n",
    "        for var_name, var_value in required_vars:\n",
    "            if not var_value:\n",
    "                raise ValueError(f\"{var_name} is required\")\n",
    "        \n",
    "        # Basic URI validation\n",
    "        if not MONGODB_URI.startswith(\"mongodb\"):\n",
    "            raise ValueError(\"MONGODB_URI must be a valid MongoDB connection string (mongodb:// or mongodb+srv://)\")\n",
    "        \n",
    "        print(f\"ğŸ” MongoDB Configuration:\")\n",
    "        print(f\"  â€¢ Database: {MONGODB_DATABASE}\")\n",
    "        print(f\"  â€¢ Collection: {MONGODB_COLLECTION}\")\n",
    "        print(\"âœ… MongoDB configuration validation completed successfully\")\n",
    "        \n",
    "        # Initialize collection (create if needed + clear existing data)\n",
    "        if not initialize_mongodb_collection():\n",
    "            raise Exception(\"Failed to initialize MongoDB collection\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during MongoDB preprocessing: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddadf62",
   "metadata": {},
   "source": [
    "## MongoDB Destination Connector\n",
    "\n",
    "Creating the destination where processed documents will be stored. Your configured MongoDB collection will receive the extracted text content, metadata, and document structure ready for newsletter generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70f0036",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_mongodb_destination_connector():\n",
    "    \"\"\"Create a MongoDB destination connector for processed results.\"\"\"\n",
    "    try:\n",
    "        # Debug: Print all input variables\n",
    "        print(f\"ğŸ“Š Input variables to create_mongodb_destination_connector:\")\n",
    "        print(f\"  â€¢ Database: {MONGODB_DATABASE}\")\n",
    "        print(f\"  â€¢ Collection: {MONGODB_COLLECTION}\")\n",
    "        print(f\"  â€¢ Batch Size: 20\")\n",
    "        print(f\"  â€¢ Flatten Metadata: False\")\n",
    "        print()\n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            response = client.destinations.create_destination(\n",
    "                request=CreateDestinationRequest(\n",
    "                    create_destination_connector=CreateDestinationConnector(\n",
    "                        name=f\"mongodb_newsletter_pipeline_destination_{int(time.time())}\",\n",
    "                        type=\"mongodb\",\n",
    "                        config={\n",
    "                            \"uri\": MONGODB_URI,\n",
    "                            \"database\": MONGODB_DATABASE,\n",
    "                            \"collection\": MONGODB_COLLECTION,\n",
    "                            \"batch_size\": 20,\n",
    "                            \"flatten_metadata\": False\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        destination_id = response.destination_connector_information.id\n",
    "        print(f\"âœ… Created MongoDB destination connector: {destination_id}\")\n",
    "        print(f\"ğŸ—„ï¸ Database: {MONGODB_DATABASE}\")\n",
    "        print(f\"ğŸ“ Collection: {MONGODB_COLLECTION}\")\n",
    "        return destination_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating MongoDB destination connector: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_mongodb_destination_connector(destination_id):\n",
    "    \"\"\"Test the MongoDB destination connector.\"\"\"\n",
    "    if destination_id and destination_id != SKIPPED:\n",
    "        print(f\"ğŸ” MongoDB destination connector ready to store processed documents\")\n",
    "        print(f\"ğŸ—„ï¸ Database: {MONGODB_DATABASE}\")\n",
    "        print(f\"ğŸ“ Collection: {MONGODB_COLLECTION}\")\n",
    "    else:\n",
    "        print(\"âŒ Failed to create MongoDB destination connector - check your credentials and configuration\")\n",
    "\n",
    "# Create MongoDB destination connector\n",
    "destination_id = create_mongodb_destination_connector()\n",
    "\n",
    "test_mongodb_destination_connector(destination_id) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2d08e",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline\n",
    "\n",
    "Configuring the two-stage pipeline: Hi-Res Partitioning â†’ Page Chunking.\n",
    "\n",
    "The pipeline uses Unstructured's hi_res strategy for detailed document analysis with advanced table detection, then chunks content by page to preserve document structure for downstream summarization and newsletter generation.\n",
    "\n",
    "**Stage 1 - High-Resolution Partitioning:**\n",
    "- **Strategy**: `hi_res` for detailed document processing\n",
    "- **Table Detection**: `pdf_infer_table_structure=True` for accurate table extraction\n",
    "- **Page Breaks**: `include_page_breaks=True` to maintain document structure\n",
    "- **Text-Focused**: Excludes images, page numbers, and formatting elements\n",
    "- **Output**: Individual elements (Title, NarrativeText, Table, etc.) with metadata\n",
    "\n",
    "**Stage 2 - Page-Based Chunking:**\n",
    "- **Strategy**: `chunk_by_page` to maintain natural page boundaries\n",
    "- **Original Elements**: `include_orig_elements=False` for cleaner output\n",
    "- **Max Characters**: `max_characters=6000` for manageable chunk sizes\n",
    "- **Output**: Page-level chunks (up to 6k characters) ideal for summarization and newsletter generation\n",
    "- **MongoDB Storage**: Structured chunks stored in MongoDB for downstream processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b4fff",
   "metadata": {},
   "source": [
    "## Creating Your Document Processing Workflow\n",
    "\n",
    "Assembling the high-resolution processing pipeline to connect S3 documents to the processing workflow. This two-stage workflow uses hi_res partitioning for detailed analysis and page-based chunking to preserve document structure for effective summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615be4a8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_image_workflow_nodes():\n",
    "    \"\"\"Create workflow nodes for document processing pipeline.\"\"\"\n",
    "    # High-res partitioner for detailed document processing\n",
    "    partitioner_workflow_node = WorkflowNode(\n",
    "        name=\"Partitioner\",\n",
    "        subtype=\"unstructured_api\",\n",
    "        type=\"partition\",\n",
    "        settings={\n",
    "            \"strategy\": \"hi_res\",\n",
    "            \"include_page_breaks\": True,\n",
    "            \"pdf_infer_table_structure\": True,\n",
    "            \"exclude_elements\": [\n",
    "                \"Address\",\n",
    "                \"PageBreak\",\n",
    "                \"Formula\",\n",
    "                \"EmailAddress\",\n",
    "                \"PageNumber\",\n",
    "                \"Image\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Chunk by page - keeps page boundaries intact\n",
    "    chunker_node = WorkflowNode(\n",
    "        name=\"Chunker\",\n",
    "        subtype=\"chunk_by_page\",\n",
    "        type=\"chunk\",\n",
    "        settings={\n",
    "            \"include_orig_elements\": False,\n",
    "            \"max_characters\": 6000  # Maximum 6k characters per chunk\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return (partitioner_workflow_node, chunker_node)\n",
    "\n",
    "def create_single_workflow(s3_source_id, destination_id):\n",
    "    \"\"\"Create a single workflow for S3 document processing.\"\"\"\n",
    "    try:\n",
    "        partitioner_node, chunker_node = create_image_workflow_nodes()\n",
    "\n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            s3_workflow = CreateWorkflow(\n",
    "                name=f\"S3-Document-Processing-Workflow_{int(time.time())}\",\n",
    "                source_id=s3_source_id,\n",
    "                destination_id=destination_id,\n",
    "                workflow_type=WorkflowType.CUSTOM,\n",
    "                workflow_nodes=[\n",
    "                    partitioner_node,\n",
    "                    chunker_node\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            s3_response = client.workflows.create_workflow(\n",
    "                request=CreateWorkflowRequest(\n",
    "                    create_workflow=s3_workflow\n",
    "                )\n",
    "            )\n",
    "\n",
    "        s3_workflow_id = s3_response.workflow_information.id\n",
    "        print(f\"âœ… Created S3 document processing workflow: {s3_workflow_id}\")\n",
    "\n",
    "        return s3_workflow_id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating document processing workflow: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8a4df",
   "metadata": {},
   "source": [
    "## Starting Your Document Processing Job\n",
    "\n",
    "With our workflow configured, it's time to put it into action. This step submits the auto partitioning workflow to the Unstructured API and returns a job ID for monitoring the document processing and text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf38b2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_workflow(workflow_id, workflow_name):\n",
    "    \"\"\"Run a workflow and return job information.\"\"\"\n",
    "    try:\n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            response = client.workflows.run_workflow(\n",
    "                request={\"workflow_id\": workflow_id}\n",
    "            )\n",
    "        \n",
    "        job_id = response.job_information.id\n",
    "        print(f\"âœ… Started {workflow_name} job: {job_id}\")\n",
    "        return job_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error running {workflow_name} workflow: {e}\")\n",
    "        return None\n",
    "\n",
    "def poll_job_status(job_id, job_name, wait_time=30):\n",
    "    \"\"\"Poll job status until completion.\"\"\"\n",
    "    print(f\"â³ Monitoring {job_name} job status...\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "                response = client.jobs.get_job(\n",
    "                    request={\"job_id\": job_id}\n",
    "                )\n",
    "            \n",
    "            job = response.job_information\n",
    "            status = job.status\n",
    "            \n",
    "            if status in [\"SCHEDULED\", \"IN_PROGRESS\"]:\n",
    "                print(f\"â³ {job_name} job status: {status}\")\n",
    "                time.sleep(wait_time)\n",
    "            elif status == \"COMPLETED\":\n",
    "                print(f\"âœ… {job_name} job completed successfully!\")\n",
    "                return job\n",
    "            elif status == \"FAILED\":\n",
    "                print(f\"âŒ {job_name} job failed!\")\n",
    "                return job\n",
    "            else:\n",
    "                print(f\"â“ Unknown {job_name} job status: {status}\")\n",
    "                return job\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error polling {job_name} job status: {e}\")\n",
    "            time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e787a7",
   "metadata": {},
   "source": [
    "## Monitoring Your Document Processing Progress\n",
    "\n",
    "Jobs progress through scheduled, in-progress, completed, or failed states. The `poll_job_status` function checks status every 30 seconds and blocks execution until processing completes, so you can see exactly what's happening with your auto partitioning and text extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee970b9",
   "metadata": {},
   "source": [
    "## Pipeline Execution Summary\n",
    "\n",
    "The following summary displays all resources created during document processing pipeline setup: S3 data source path, connector IDs, workflow ID, job ID, and processing status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b120a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def print_pipeline_summary(workflow_id, job_id):\n",
    "    \"\"\"Print pipeline summary for document processing workflow.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“Š DOCUMENT PROCESSING PIPELINE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ğŸ“ S3 Source: {S3_SOURCE_BUCKET}\")\n",
    "    print(f\"ğŸ“¤ MongoDB Destination: {MONGODB_DATABASE}/{MONGODB_COLLECTION}\")\n",
    "    print(f\"\")\n",
    "    print(f\"âš™ï¸ Document Processing Workflow ID: {workflow_id}\")\n",
    "    print(f\"ğŸš€ Document Processing Job ID: {job_id}\")\n",
    "    print()\n",
    "    print(\"ğŸ’¡ Monitor job progress at: https://platform.unstructured.io\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "def verify_customer_support_results(job_id=None):\n",
    "    \"\"\"\n",
    "    Verify the document processing pipeline results by checking job status.\n",
    "    \n",
    "    Note: MongoDB verification requires additional setup for direct database queries.\n",
    "    This function focuses on job status verification.\n",
    "\n",
    "    Args:\n",
    "        job_id (str, optional): If provided, will poll job status until completion before verification.\n",
    "                               If None, assumes job has completed.\n",
    "    \"\"\"\n",
    "\n",
    "    if job_id is not None and job_id != \"\" and isinstance(job_id, str):\n",
    "        print(\"ğŸ” Starting verification process...\")\n",
    "        print(\"â³ Polling job status until completion...\")\n",
    "\n",
    "        job_info = poll_job_status(job_id, \"Document Processing\")\n",
    "\n",
    "        if not job_info or job_info.status != \"COMPLETED\":\n",
    "            print(f\"\\nâŒ Job did not complete successfully. Status: {job_info.status if job_info else 'Unknown'}\")\n",
    "            print(\"ğŸ’¡ Check the Unstructured dashboard for more details.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\nğŸ” Job completed successfully!\")\n",
    "        print(\"-\" * 50)\n",
    "    else:\n",
    "        if job_id is not None:\n",
    "            print(f\"âš ï¸  Invalid job_id provided: {job_id} (type: {type(job_id)})\")\n",
    "        print(\"ğŸ” Verifying processed results (skipping job polling)...\")\n",
    "\n",
    "    try:\n",
    "        print(f\"ğŸ“Š MongoDB Configuration:\")\n",
    "        print(f\"   ğŸ—„ï¸ Database: {MONGODB_DATABASE}\")\n",
    "        print(f\"   ğŸ“ Collection: {MONGODB_COLLECTION}\")\n",
    "        print(f\"   ğŸ”— Connection: {'*' * 20}...{MONGODB_URI[-10:] if len(MONGODB_URI) > 10 else '***'}\")\n",
    "        \n",
    "        print(f\"\\nâœ… Pipeline completed successfully!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"ğŸ‰ SCRAPED-PUBLICATIONS PIPELINE VERIFICATION COMPLETE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"âœ… Job completed successfully\")\n",
    "        print(\"âœ… Data has been written to MongoDB collection\")\n",
    "        print(\"ğŸ“š Documents are now stored in MongoDB database\")\n",
    "        print(\"ğŸ¤– Ready for data retrieval and summarization!\")\n",
    "        print(\"\\nğŸ’¡ To query your data, use the MongoDB client or aggregation pipelines\")\n",
    "        print(f\"ğŸ—„ï¸ Database: {MONGODB_DATABASE}\")\n",
    "        print(f\"ğŸ“ Collection: {MONGODB_COLLECTION}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error verifying results: {e}\")\n",
    "        print(\"ğŸ’¡ This is normal if workflow is still processing or if there is a connection issue.\")\n",
    "\n",
    "def run_verification_with_images(job_id):\n",
    "    \"\"\"\n",
    "    Legacy wrapper function - now just calls verify_customer_support_results with job_id.\n",
    "    Use verify_customer_support_results(job_id) directly instead.\n",
    "    \"\"\"\n",
    "    verify_customer_support_results(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25579cc",
   "metadata": {},
   "source": [
    "## Orchestrating Your Complete Document Processing Pipeline\n",
    "\n",
    "We'll now execute the pipeline in distinct steps, allowing you to monitor progress at each stage: preprocessing, connector setup, workflow creation, execution, and results validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ade37",
   "metadata": {},
   "source": [
    "### Step 1: MongoDB Preprocessing\n",
    "\n",
    "First, we validate the MongoDB connection and prepare the collection for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d976a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: MongoDB preprocessing\n",
    "print(\"ğŸš€ Starting Newsletter Document Processing Pipeline\")\n",
    "print(\"\\nğŸ”§ Step 1: MongoDB preprocessing\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "preprocessing_success = run_mongodb_preprocessing()\n",
    "\n",
    "if preprocessing_success:\n",
    "    print(\"âœ… MongoDB preprocessing completed successfully\")\n",
    "else:\n",
    "    print(\"âŒ Failed to complete MongoDB preprocessing\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e342d5",
   "metadata": {},
   "source": [
    "### Step 2-3: Create Data Connectors\n",
    "\n",
    "Next, we create the connectors that link your S3 content bucket to MongoDB storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e21a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create S3 source connector\n",
    "print(\"\\nğŸ”— Step 2: Creating S3 source connector\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "s3_source_id = create_s3_source_connector()\n",
    "\n",
    "if s3_source_id:\n",
    "    # Step 3: Create MongoDB destination connector\n",
    "    print(\"\\nğŸ¯ Step 3: Creating MongoDB destination connector\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    destination_id = create_mongodb_destination_connector()\n",
    "    \n",
    "    if destination_id:\n",
    "        print(\"âœ… Connectors created successfully\")\n",
    "    else:\n",
    "        print(\"âŒ Failed to create MongoDB destination connector\")\n",
    "else:\n",
    "    print(\"âŒ Failed to create S3 source connector\")\n",
    "    destination_id = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f2d6c",
   "metadata": {},
   "source": [
    "### Step 4: Create Processing Workflow\n",
    "\n",
    "Now we'll create the document processing workflow with high-resolution partitioning and page-based chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42614ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create document processing workflow\n",
    "print(\"\\nâš™ï¸ Step 4: Creating document processing workflow\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if s3_source_id and destination_id:\n",
    "    # Create workflow nodes inline\n",
    "    try:\n",
    "        # High-res partitioner for detailed document processing\n",
    "        partitioner_workflow_node = WorkflowNode(\n",
    "            name=\"Partitioner\",\n",
    "            subtype=\"unstructured_api\",\n",
    "            type=\"partition\",\n",
    "            settings={\n",
    "                \"strategy\": \"hi_res\",\n",
    "                \"include_page_breaks\": True,\n",
    "                \"pdf_infer_table_structure\": True,\n",
    "                \"exclude_elements\": [\n",
    "                    \"Address\",\n",
    "                    \"PageBreak\",\n",
    "                    \"Formula\",\n",
    "                    \"EmailAddress\",\n",
    "                    \"PageNumber\",\n",
    "                    \"Image\"\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Chunk by page - keeps page boundaries intact\n",
    "        chunker_node = WorkflowNode(\n",
    "            name=\"Chunker\",\n",
    "            subtype=\"chunk_by_page\",\n",
    "            type=\"chunk\",\n",
    "            settings={\n",
    "                \"include_orig_elements\": False,\n",
    "                \"max_characters\": 6000  # Maximum 6k characters per chunk\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Create the workflow\n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            s3_workflow = CreateWorkflow(\n",
    "                name=f\"S3-Document-Processing-Workflow_{int(time.time())}\",\n",
    "                source_id=s3_source_id,\n",
    "                destination_id=destination_id,\n",
    "                workflow_type=WorkflowType.CUSTOM,\n",
    "                workflow_nodes=[\n",
    "                    partitioner_workflow_node,\n",
    "                    chunker_node\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            s3_response = client.workflows.create_workflow(\n",
    "                request=CreateWorkflowRequest(\n",
    "                    create_workflow=s3_workflow\n",
    "                )\n",
    "            )\n",
    "\n",
    "        workflow_id = s3_response.workflow_information.id\n",
    "        print(f\"âœ… Created S3 document processing workflow: {workflow_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating document processing workflow: {e}\")\n",
    "        workflow_id = None\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping workflow creation - connectors not available\")\n",
    "    workflow_id = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e64e3c",
   "metadata": {},
   "source": [
    "### Step 5: Execute Workflow\n",
    "\n",
    "Run the workflow to start processing your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Run the workflow\n",
    "print(\"\\nğŸš€ Step 5: Running workflow\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if workflow_id:\n",
    "    # Run the workflow inline\n",
    "    try:\n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            response = client.workflows.run_workflow(\n",
    "                request={\"workflow_id\": workflow_id}\n",
    "            )\n",
    "        \n",
    "        job_id = response.job_information.id\n",
    "        print(f\"âœ… Started S3 Document Processing job: {job_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error running S3 Document Processing workflow: {e}\")\n",
    "        job_id = None\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping workflow execution - workflow not created\")\n",
    "    job_id = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4fe73",
   "metadata": {},
   "source": [
    "### Step 6: Pipeline Summary\n",
    "\n",
    "Display the pipeline configuration and job information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619885bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Display pipeline summary\n",
    "if workflow_id and job_id:\n",
    "    print_pipeline_summary(workflow_id, job_id)\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Pipeline incomplete - check previous steps for errors\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba344c40",
   "metadata": {},
   "source": [
    "## Monitoring Job Progress and Viewing Processed Documents\n",
    "\n",
    "The code above starts your document processing pipeline and returns a job ID. Now run the verification block below to monitor the job progress and confirm the processed content has been stored in your MongoDB collection.\n",
    "\n",
    "This verification process will:\n",
    "- Poll the job status until completion\n",
    "- Confirm successful data storage in your MongoDB collection\n",
    "- Display pipeline completion status and collection information\n",
    "- Validate that documents and metadata are ready for retrieval and summarization\n",
    "\n",
    "**Note**: The verification block will wait for job completion before displaying results, so you can run it immediately after the pipeline starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification Block - Run this after the main pipeline to monitor progress and view results\n",
    "# This block will wait for job completion and then display 5 random records with images\n",
    "\n",
    "print(\"ğŸ” Starting verification process...\")\n",
    "print(\"â³ This will monitor job progress and display results when complete\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check if job_id is defined from the main pipeline execution above\n",
    "try:\n",
    "    # Try to access job_id variable\n",
    "    if 'job_id' in locals() or 'job_id' in globals():\n",
    "        print(f\"ğŸ“‹ Using job_id from main pipeline: {job_id}\")\n",
    "        verify_customer_support_results(job_id)\n",
    "    else:\n",
    "        print(\"âš ï¸  job_id not found - running verification without job polling\")\n",
    "        verify_customer_support_results()\n",
    "except NameError:\n",
    "    print(\"âš ï¸  job_id variable not defined - running verification without job polling\")\n",
    "    verify_customer_support_results()\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Error accessing job_id: {e} - running verification without job polling\")\n",
    "    verify_customer_support_results() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377a0bb",
   "metadata": {},
   "source": [
    "## Generating AI Newsletters from Processed Documents\n",
    "\n",
    "Now that your documents are processed and stored in MongoDB, you can generate AI-powered newsletters! This section demonstrates how to:\n",
    "- Retrieve documents from MongoDB\n",
    "- Generate detailed summaries for each document\n",
    "- Create an executive brief highlighting the most important developments\n",
    "\n",
    "You can customize the prompts below to control the style, length, and focus of the generated content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbf73e",
   "metadata": {},
   "source": [
    "### Part 1: Generate Detailed Document Summaries\n",
    "\n",
    "This cell retrieves all processed documents from MongoDB, groups them by filename, and generates a detailed summary for each document. \n",
    "\n",
    "**Customize Your Summary Prompt**: Edit the `SUMMARY_INSTRUCTIONS` variable below to control:\n",
    "- Length (e.g., \"Maximum 10 sentences\")\n",
    "- Focus (e.g., \"Focus on business applications\" or \"Emphasize technical innovations\")\n",
    "- Tone (e.g., \"Write for executives\" or \"Write for researchers\")\n",
    "- Style (e.g., \"Be concise\" or \"Provide comprehensive details\")\n",
    "\n",
    "The summaries will be printed below so you can iterate on your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01880bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CUSTOMIZE YOUR SUMMARY PROMPT HERE\n",
    "# ============================================================\n",
    "\n",
    "SUMMARY_INSTRUCTIONS = \"\"\"\n",
    "You are an expert at summarizing AI research papers and industry developments.\n",
    "\n",
    "Please write a concise, informative summary of the following content, focusing specifically on:\n",
    "- Novel advancements or breakthroughs in AI/ML\n",
    "- State-of-the-art techniques or methodologies\n",
    "- Performance improvements or benchmark results\n",
    "- Practical applications and industry impact\n",
    "- Significance to the AI research community\n",
    "\n",
    "Keep the summary focused and relevant to AI industry professionals. Maximum 12 sentences.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Generate Summaries (code below retrieves and summarizes)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“ GENERATING DETAILED SUMMARIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from collections import defaultdict\n",
    "\n",
    "# Connect to MongoDB\n",
    "print(\"\\nğŸ”— Connecting to MongoDB...\")\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db = client[MONGODB_DATABASE]\n",
    "collection = db[MONGODB_COLLECTION]\n",
    "\n",
    "# Retrieve CompositeElement documents\n",
    "print(\"ğŸ“¥ Retrieving documents...\")\n",
    "query = {\"type\": \"CompositeElement\"}\n",
    "documents = list(collection.find(query))\n",
    "print(f\"âœ… Retrieved {len(documents)} documents\")\n",
    "\n",
    "# Group by filename\n",
    "print(\"ğŸ“Š Grouping by filename...\")\n",
    "grouped = defaultdict(list)\n",
    "for doc in documents:\n",
    "    metadata = doc.get(\"metadata\", {})\n",
    "    filename = metadata.get(\"filename\", \"unknown\")\n",
    "    grouped[filename].append(doc)\n",
    "\n",
    "print(f\"âœ… Grouped into {len(grouped)} unique files\\n\")\n",
    "\n",
    "# Generate summaries\n",
    "summaries = []\n",
    "\n",
    "for filename, docs in list(grouped.items())[:5]:  # Limit to 5 for demo\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“„ Processing: {filename}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Pages: {len(docs)}\")\n",
    "    \n",
    "    # Sort by page number and concatenate\n",
    "    sorted_docs = sorted(docs, key=lambda d: d.get(\"metadata\", {}).get(\"page_number\", 0))\n",
    "    full_text = \"\\n\\n\".join([d.get(\"text\", \"\") for d in sorted_docs if d.get(\"text\")])\n",
    "    \n",
    "    # Truncate if too long\n",
    "    max_chars = 100000\n",
    "    if len(full_text) > max_chars:\n",
    "        print(f\"âš ï¸  Text too long ({len(full_text):,} chars), truncating to {max_chars:,}\")\n",
    "        full_text = full_text[:max_chars]\n",
    "    \n",
    "    print(f\"ğŸ“ Text length: {len(full_text):,} characters\")\n",
    "    \n",
    "    # Generate summary using OpenAI\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3, openai_api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    prompt = f\"\"\"{SUMMARY_INSTRUCTIONS}\n",
    "\n",
    "Content:\n",
    "{full_text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    print(\"ğŸ¤– Generating summary...\")\n",
    "    response = llm.invoke(prompt)\n",
    "    summary = response.content.strip()\n",
    "    \n",
    "    print(f\"âœ… Summary generated ({len(summary)} characters)\\n\")\n",
    "    print(\"â”€\" * 60)\n",
    "    print(\"SUMMARY:\")\n",
    "    print(\"â”€\" * 60)\n",
    "    print(summary)\n",
    "    print(\"â”€\" * 60)\n",
    "    \n",
    "    # Store summary\n",
    "    summaries.append({\n",
    "        \"filename\": filename,\n",
    "        \"source\": sorted_docs[0].get(\"metadata\", {}).get(\"source\", \"unknown\"),\n",
    "        \"summary\": summary\n",
    "    })\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"âœ… COMPLETED: Generated {len(summaries)} summaries\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nğŸ’¡ Tip: Modify SUMMARY_INSTRUCTIONS above to change the style, length, or focus!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c10b9",
   "metadata": {},
   "source": [
    "### Part 2: Generate Executive Brief Newsletter\n",
    "\n",
    "This cell takes all the detailed summaries and synthesizes them into a concise executive brief (~700 words) highlighting the most significant developments.\n",
    "\n",
    "**Customize Your Executive Brief Prompt**: Edit the `EXECUTIVE_BRIEF_INSTRUCTIONS` variable below to control:\n",
    "- Target length (e.g., \"approximately 500 words\" or \"approximately 1000 words\")\n",
    "- Focus areas (e.g., \"competitive landscape\" or \"emerging technologies\")\n",
    "- Target audience (e.g., \"C-suite executives\" or \"technical founders\")\n",
    "- Structure (e.g., \"3 main sections\" or \"bullet point format\")\n",
    "\n",
    "The executive brief will be printed below so you can refine your prompt to get the perfect newsletter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CUSTOMIZE YOUR EXECUTIVE BRIEF PROMPT HERE\n",
    "# ============================================================\n",
    "\n",
    "EXECUTIVE_BRIEF_INSTRUCTIONS = \"\"\"\n",
    "You are an expert AI industry analyst creating executive summaries for C-suite executives and industry leaders.\n",
    "\n",
    "You are given detailed summaries of recent AI research papers and industry developments. Your task is to create a concise executive summary of approximately 700 words that:\n",
    "\n",
    "1. **Identifies the most significant industry developments** - Focus on breakthroughs that will impact businesses, products, or the competitive landscape\n",
    "2. **Highlights practical applications** - Emphasize real-world uses and business implications\n",
    "3. **Notes key performance milestones** - Include impressive benchmark results or technical achievements\n",
    "4. **Synthesizes trends** - Look for patterns or themes across multiple developments\n",
    "5. **Maintains accessibility** - Write for business leaders who may not have deep technical expertise\n",
    "\n",
    "Structure your summary with:\n",
    "- A brief opening paragraph highlighting the week's most significant theme or development\n",
    "- 3-4 paragraphs covering the most important individual developments, organized by impact or theme\n",
    "- A concluding paragraph on what these developments mean for the AI industry going forward\n",
    "\n",
    "Target length: approximately 700 words. Be selective - only include the most industry-relevant developments.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Generate Executive Brief (code below synthesizes summaries)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š GENERATING EXECUTIVE BRIEF\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Build a detailed newsletter from all summaries\n",
    "print(\"\\nğŸ“° Creating detailed content from summaries...\")\n",
    "\n",
    "detailed_content = f\"\"\"# AI Industry Weekly Digest\n",
    "*{datetime.now().strftime(\"%B %d, %Y\")}*\n",
    "\n",
    "## Summaries of Recent Publications\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i, summary_data in enumerate(summaries, 1):\n",
    "    filename = summary_data[\"filename\"]\n",
    "    summary_text = summary_data[\"summary\"]\n",
    "    \n",
    "    # Clean up title\n",
    "    title = filename.replace(\".pdf\", \"\").replace(\".html\", \"\").replace(\"_\", \" \").replace(\"-\", \" \").title()\n",
    "    if len(title) > 80:\n",
    "        title = title[:77] + \"...\"\n",
    "    \n",
    "    detailed_content += f\"\\n### {i}. {title}\\n\\n{summary_text}\\n\\n\"\n",
    "\n",
    "print(f\"âœ… Detailed content created ({len(detailed_content):,} characters)\")\n",
    "\n",
    "# Generate executive brief using OpenAI\n",
    "print(\"\\nğŸ¤– Synthesizing executive brief...\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.3, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "prompt = f\"\"\"{EXECUTIVE_BRIEF_INSTRUCTIONS}\n",
    "\n",
    "Detailed Newsletter:\n",
    "{detailed_content}\n",
    "\n",
    "Executive Summary:\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "executive_brief = response.content.strip()\n",
    "\n",
    "word_count = len(executive_brief.split())\n",
    "print(f\"âœ… Executive brief generated ({word_count} words, {len(executive_brief)} characters)\\n\")\n",
    "\n",
    "# Display the executive brief\n",
    "print(\"=\"*60)\n",
    "print(\"AI INDUSTRY EXECUTIVE BRIEF\")\n",
    "print(\"=\"*60)\n",
    "print(f\"*{datetime.now().strftime('%B %d, %Y')}*\\n\")\n",
    "print(\"â”€\" * 60)\n",
    "print(executive_brief)\n",
    "print(\"â”€\" * 60)\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"âœ… NEWSLETTER GENERATION COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nğŸ“Š Statistics:\")\n",
    "print(f\"   â€¢ Summaries analyzed: {len(summaries)}\")\n",
    "print(f\"   â€¢ Executive brief length: {word_count} words\")\n",
    "print(f\"\\nğŸ’¡ Tip: Modify EXECUTIVE_BRIEF_INSTRUCTIONS above to change the focus, length, or target audience!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ceafb",
   "metadata": {},
   "source": [
    "## What You've Learned\n",
    "\n",
    "**Document Processing Pipeline**: You've learned how to process PDF documents and HTML files with high-resolution partitioning, maintain page boundaries with page-based chunking, and store structured content in MongoDB for downstream applications.\n",
    "\n",
    "**Unstructured API Capabilities**: You've experienced intelligent document processing with hi_res strategy, advanced table detection and structure preservation, flexible chunking strategies for optimal text organization, and seamless integration with MongoDB for document storage.\n",
    "\n",
    "**AI-Powered Newsletter Generation**: You've built a complete system for retrieving processed documents from MongoDB, generating detailed summaries with customizable prompts, creating executive briefs that highlight key developments, and iterating on prompts to perfect your newsletter content.\n",
    "\n",
    "### Ready to Scale?\n",
    "\n",
    "Deploy automated newsletter systems for industry intelligence, build document summarization tools for research teams, or create AI-powered content aggregation systems. Add more document sources using additional S3 buckets, implement scheduled pipeline runs for fresh content, or scale up for production document volumes with automated processing.\n",
    "\n",
    "### Try Unstructured Today\n",
    "\n",
    "Ready to build your own AI-powered document processing system? [Sign up for a free trial](https://unstructured.io/?modal=try-for-free) and start transforming your documents into intelligent, searchable knowledge.\n",
    "\n",
    "**Need help getting started?** Contact our team to schedule a demo and see how Unstructured can solve your specific document processing challenges."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
