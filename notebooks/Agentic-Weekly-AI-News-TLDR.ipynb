{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ee21bf",
   "metadata": {},
   "source": [
    "# Building an AI Weekly Newsletter Pipeline\n",
    "\n",
    "The AI industry moves fast. Every week brings new research papers, blog posts, product announcements, and technical breakthroughs. Keeping up with developments from ArXiv, OpenAI, Anthropic, Hugging Face, DeepLearning.AI, and other sources can be overwhelming. How do you stay informed without spending hours reading through dozens of publications?\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "AI news comes in many formats—research papers (PDFs), blog posts (HTML), newsletters, and articles. Manually tracking and summarizing content from multiple sources is time-consuming and often incomplete. What busy professionals need is an automated system that collects relevant AI content and generates a concise weekly summary of what matters.\n",
    "\n",
    "## The Solution\n",
    "\n",
    "This notebook demonstrates an end-to-end pipeline for collecting, processing, and summarizing AI industry content into a weekly newsletter. We use:\n",
    "- **Automated scraping** to collect recent AI papers and blog posts\n",
    "- **Unstructured's hi_res processing** to extract clean text from PDFs and HTML\n",
    "- **AI-powered summarization** to create concise, actionable summaries\n",
    "- **Customizable prompts** so you can tailor the newsletter to your audience\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "A complete weekly AI newsletter system that scrapes the last 7 days of content from ArXiv and leading AI blogs, processes the documents through Unstructured's API, and generates both detailed summaries and an executive brief.\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────┐\n",
    "│  WEEKLY DATA COLLECTION (Last 7 Days)   │\n",
    "├──────────────────────────────────────────┤\n",
    "│  • ArXiv Papers (PDFs)                   │\n",
    "│  • Hugging Face Blog (HTML)              │\n",
    "│  • OpenAI News (HTML)                    │\n",
    "│  • DeepLearning.AI Batch (HTML)          │\n",
    "│  • Anthropic Research (HTML)             │\n",
    "└────────────────┬─────────────────────────┘\n",
    "                 │\n",
    "                 ▼\n",
    "┌──────────────────────────────────────────┐\n",
    "│      S3 Storage (Collected Content)      │\n",
    "└────────────────┬─────────────────────────┘\n",
    "                 │\n",
    "                 ▼\n",
    "┌──────────────────────────────────────────┐\n",
    "│    Unstructured API Processing           │\n",
    "│    • Hi-Res PDF Partitioning             │\n",
    "│    • HTML Text Extraction                │\n",
    "│    • Page-Based Chunking                 │\n",
    "└────────────────┬─────────────────────────┘\n",
    "                 │\n",
    "                 ▼\n",
    "┌──────────────────────────────────────────┐\n",
    "│    MongoDB (Structured Content)          │\n",
    "└────────────────┬─────────────────────────┘\n",
    "                 │\n",
    "                 ▼\n",
    "┌──────────────────────────────────────────┐\n",
    "│    AI Summarization & Newsletter Gen     │\n",
    "│    • Detailed Publication Summaries      │\n",
    "│    • Executive Brief (~700 words)        │\n",
    "└──────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Note**: In production, you would run the scraping daily via cron job. For this demo, we simulate a week's worth of data collection by scraping 7 days of content in one batch.\n",
    "\n",
    "By the end, you'll have a working system that can automatically generate weekly AI newsletters tailored to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2bd8a",
   "metadata": {},
   "source": [
    "## Getting Started: Your Unstructured API Key\n",
    "\n",
    "You'll need an Unstructured API key to access the auto document processing platform.\n",
    "\n",
    "### Sign Up and Get Your API Key\n",
    "\n",
    "Visit https://platform.unstructured.io to sign up for a free account, navigate to API Keys in the sidebar, and generate your API key. For Team or Enterprise accounts, select the correct organizational workspace before creating your key.\n",
    "\n",
    "**Need help?** Contact Unstructured Support at support@unstructured.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b2a94",
   "metadata": {},
   "source": [
    "## Configuration: Setting Up Your Environment\n",
    "\n",
    "We'll configure your environment with the necessary API keys and credentials to connect to data sources and AI services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8605b2",
   "metadata": {},
   "source": [
    "### Creating a .env File in Google Colab\n",
    "\n",
    "For better security and organization, we'll create a `.env` file directly in your Colab environment. Run the code cell below to create the file with placeholder values, then edit it with your actual credentials.\n",
    "\n",
    "After running the code cell, you'll need to replace each placeholder value (like `your-unstructured-api-key`) with your actual API keys and credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a8e911a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 .env file already exists - skipping creation\n",
      "💡 Using existing .env file with current configuration\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_dotenv_file():\n",
    "    \"\"\"Create a .env file with placeholder values for the user to fill in, only if it doesn't already exist.\"\"\"\n",
    "    \n",
    "    # Check if .env file already exists\n",
    "    if os.path.exists('.env'):\n",
    "        print(\"📝 .env file already exists - skipping creation\")\n",
    "        print(\"💡 Using existing .env file with current configuration\")\n",
    "        return\n",
    "    \n",
    "    env_content = \"\"\"# AI Newsletter Pipeline Environment Configuration\n",
    "# Fill in your actual values below\n",
    "# Configuration - Set these explicitly\n",
    "\n",
    "# ===================================================================\n",
    "# AWS CONFIGURATION\n",
    "# ===================================================================\n",
    "AWS_ACCESS_KEY_ID=\"your-aws-access-key-id\"\n",
    "AWS_SECRET_ACCESS_KEY=\"your-aws-secret-access-key\"\n",
    "AWS_REGION=\"us-east-1\"\n",
    "\n",
    "# ===================================================================\n",
    "# UNSTRUCTURED API CONFIGURATION  \n",
    "# ===================================================================\n",
    "UNSTRUCTURED_API_KEY=\"your-unstructured-api-key\"\n",
    "UNSTRUCTURED_API_URL=\"https://platform.unstructuredapp.io/api/v1\"\n",
    "\n",
    "# ===================================================================\n",
    "# MONGODB CONFIGURATION\n",
    "# ===================================================================\n",
    "MONGODB_URI=\"mongodb+srv://<username>:<password>@<host>/?retryWrites=true&w=majority\"\n",
    "MONGODB_DATABASE=\"scraped_publications\"\n",
    "MONGODB_COLLECTION=\"documents\"\n",
    "\n",
    "# ===================================================================\n",
    "# PIPELINE DATA SOURCES\n",
    "# ===================================================================\n",
    "S3_SOURCE_BUCKET=\"your-s3-bucket-name\"\n",
    "\n",
    "# ===================================================================\n",
    "# OPENAI API CONFIGURATION \n",
    "# ===================================================================\n",
    "OPENAI_API_KEY=\"your-openai-api-key\"\n",
    "\"\"\"\n",
    "    \n",
    "    with open('.env', 'w') as f:\n",
    "        f.write(env_content)\n",
    "    \n",
    "    print(\"✅ Created .env file with placeholder values\")\n",
    "    print(\"📝 Please edit the .env file and replace the placeholder values with your actual credentials\")\n",
    "    print(\"🔑 Required: UNSTRUCTURED_API_KEY, AWS credentials, MongoDB credentials, Firecrawl API key\")\n",
    "    print(\"📁 S3_SOURCE_BUCKET should point to your AI content storage bucket\")\n",
    "    print(\"🤖 OPENAI_API_KEY needed for AI-powered summarization and newsletter generation\")\n",
    "\n",
    "create_dotenv_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fccf5d1",
   "metadata": {},
   "source": [
    "### Installing Required Dependencies\n",
    "\n",
    "Installing the Python packages needed: Unstructured client, MongoDB connector, AWS SDK, OpenAI integration, and document processing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc8c5b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def ensure_notebook_deps() -> None:\n",
    "    packages = [\n",
    "        \"jupytext\",\n",
    "        \"python-dotenv\", \n",
    "        \"unstructured-client\",\n",
    "        \"boto3\",\n",
    "        \"PyYAML\",\n",
    "        \"langchain\",\n",
    "        \"langchain-openai\",\n",
    "        \"pymongo\",\n",
    "        \"firecrawl-py\",\n",
    "        \"arxiv\",\n",
    "        \"python-dateutil\"\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *packages])\n",
    "    except Exception:\n",
    "        # If install fails, continue; imports below will surface actionable errors\n",
    "        pass\n",
    "\n",
    "# Install notebook dependencies (safe no-op if present)\n",
    "ensure_notebook_deps()\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import zipfile\n",
    "import tempfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError, NoCredentialsError\n",
    "\n",
    "from unstructured_client import UnstructuredClient\n",
    "from unstructured_client.models.operations import (\n",
    "    CreateSourceRequest,\n",
    "    CreateDestinationRequest,\n",
    "    CreateWorkflowRequest\n",
    ")\n",
    "from unstructured_client.models.shared import (\n",
    "    CreateSourceConnector,\n",
    "    CreateDestinationConnector,\n",
    "    WorkflowNode,\n",
    "    WorkflowType,\n",
    "    CreateWorkflow\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# ENVIRONMENT CONFIGURATION\n",
    "# =============================================================================\n",
    "# Load from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration constants\n",
    "SKIPPED = \"SKIPPED\"\n",
    "UNSTRUCTURED_API_URL = os.getenv(\"UNSTRUCTURED_API_URL\", \"https://platform.unstructuredapp.io/api/v1\")\n",
    "\n",
    "# Get environment variables\n",
    "UNSTRUCTURED_API_KEY = os.getenv(\"UNSTRUCTURED_API_KEY\")\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")\n",
    "S3_SOURCE_BUCKET = os.getenv(\"S3_SOURCE_BUCKET\")\n",
    "S3_DESTINATION_BUCKET = os.getenv(\"S3_DESTINATION_BUCKET\")\n",
    "S3_OUTPUT_PREFIX = os.getenv(\"S3_OUTPUT_PREFIX\", \"\")\n",
    "MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
    "MONGODB_DATABASE = os.getenv(\"MONGODB_DATABASE\")\n",
    "MONGODB_COLLECTION = os.getenv(\"MONGODB_COLLECTION\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "FIRECRAWL_API_KEY = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "\n",
    "# Validation\n",
    "REQUIRED_VARS = {\n",
    "    \"UNSTRUCTURED_API_KEY\": UNSTRUCTURED_API_KEY,\n",
    "    \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n",
    "    \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n",
    "    \"AWS_REGION\": AWS_REGION,\n",
    "    \"MONGODB_URI\": MONGODB_URI,\n",
    "    \"MONGODB_DATABASE\": MONGODB_DATABASE,\n",
    "    \"MONGODB_COLLECTION\": MONGODB_COLLECTION,\n",
    "    \"S3_SOURCE_BUCKET\": S3_SOURCE_BUCKET,\n",
    "}\n",
    "\n",
    "missing_vars = [key for key, value in REQUIRED_VARS.items() if not value]\n",
    "if missing_vars:\n",
    "    print(f\"❌ Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "    print(\"Please set these environment variables or create a .env file with your credentials.\")\n",
    "    raise ValueError(f\"Missing required environment variables: {missing_vars}\")\n",
    "\n",
    "print(\"✅ Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca3d8de",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4567000",
   "metadata": {},
   "source": [
    "## AWS S3: Your Content Collection Repository\n",
    "\n",
    "Now that we have our environment configured, let's set up S3 as the central repository for collected AI content. The scraping pipeline will deposit PDFs (ArXiv papers) and HTML files (blog posts) into your S3 bucket, where they'll be ready for processing by the Unstructured API.\n",
    "\n",
    "### What You Need\n",
    "\n",
    "**An existing S3 bucket** to store scraped AI content. The following sections will automatically populate this bucket with:\n",
    "- Recent AI/ML research papers from ArXiv (PDF format)\n",
    "- Blog posts from Hugging Face, OpenAI, DeepLearning.AI, and Anthropic (HTML format)\n",
    "\n",
    "> **Note**: You'll need an AWS account with S3 access, an IAM user with read/write permissions, and your access keys (Access Key ID and Secret Access Key). For detailed S3 setup instructions, see the [Unstructured S3 source connector documentation](https://docs.unstructured.io/api-reference/api-services/source-connectors/s3).\n",
    "\n",
    "**Adaptable to Other Use Cases**: This same approach can be adapted for competitor tracking, industry news monitoring, internal document aggregation, or any scenario where you need to collect and summarize content from multiple sources regularly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b765f02b",
   "metadata": {},
   "source": [
    "## Automated Content Scraping: Gathering AI Industry Intelligence\n",
    "\n",
    "The first step in building a weekly AI newsletter is collecting content from multiple sources. This section demonstrates automated scraping that gathers recent AI research papers and blog posts.\n",
    "\n",
    "**Data Sources:**\n",
    "1. **ArXiv** - Recent AI/ML research papers from cs.AI, cs.LG, cs.CL, cs.CV, and cs.NE categories\n",
    "2. **AI Company Blogs** - Blog posts from Hugging Face, OpenAI, DeepLearning.AI, and Anthropic\n",
    "\n",
    "**Process Flow:**\n",
    "```\n",
    "ArXiv API → PDFs → S3\n",
    "Firecrawl API → Blog HTML → S3\n",
    "                     ↓\n",
    "            Unstructured Processing → MongoDB → AI Summarization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3394e9b6",
   "metadata": {},
   "source": [
    "### Scraping ArXiv Research Papers\n",
    "\n",
    "This cell scrapes recent AI/ML papers from ArXiv, filters them by category, and uploads PDFs directly to your S3 bucket. The cell searches ArXiv for papers matching your criteria, downloads PDFs, and uploads them to S3 under `arxiv/papers/`.\n",
    "\n",
    "**Demo Configuration**: For this demo, we've capped the results at 5 articles to keep notebook runtime under 2 minutes. You can increase `MAX_RESULTS` in the code below to collect more papers for production use. Customize the `SEARCH_QUERY`, `ARXIV_CATEGORIES`, and `DAYS_BACK` parameters to focus on specific topics or adjust the date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7b8380d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=artificial+intelligence+OR+machine+learning&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "📚 ARXIV PAPER SCRAPING\n",
      "============================================================\n",
      "\n",
      "🔍 Searching for papers from the last 7 days\n",
      "   Query: artificial intelligence OR machine learning\n",
      "   Max results: 5\n",
      "   Categories: cs.AI, cs.LG, cs.CL, cs.CV, cs.NE\n",
      "\n",
      "📥 Searching ArXiv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got first page: 100 of 518459 total results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 5 papers\n",
      "\n",
      "📄 Processing: Clink! Chop! Thud! -- Learning Object Sounds from Real-World...\n",
      "   ArXiv ID: 2510.02313v1\n",
      "   Published: 2025-10-02\n",
      "   Categories: cs.CV\n",
      "\n",
      "📄 Processing: KaVa: Latent Reasoning via Compressed KV-Cache Distillation...\n",
      "   ArXiv ID: 2510.02312v1\n",
      "   Published: 2025-10-02\n",
      "   Categories: cs.LG\n",
      "\n",
      "📄 Processing: Inferring Dynamic Physical Properties from Video Foundation ...\n",
      "   ArXiv ID: 2510.02311v1\n",
      "   Published: 2025-10-02\n",
      "   Categories: cs.CV, cs.LG\n",
      "\n",
      "📄 Processing: Robust Tangent Space Estimation via Laplacian Eigenvector Gr...\n",
      "   ArXiv ID: 2510.02308v1\n",
      "   Published: 2025-10-02\n",
      "   Categories: cs.LG, math.DG\n",
      "\n",
      "📄 Processing: NoiseShift: Resolution-Aware Noise Recalibration for Better ...\n",
      "   ArXiv ID: 2510.02307v1\n",
      "   Published: 2025-10-02\n",
      "   Categories: cs.CV, cs.AI\n",
      "\n",
      "============================================================\n",
      "✅ ARXIV SCRAPING COMPLETE\n",
      "============================================================\n",
      "   📥 Papers scraped: 5\n",
      "   ⏭️  Papers skipped: 0\n",
      "   📦 S3 Bucket: ai-papers-and-blogs-notebook\n",
      "   📁 S3 Prefix: arxiv/papers/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Customize these parameters\n",
    "# ============================================================\n",
    "\n",
    "# Search configuration\n",
    "SEARCH_QUERY = \"artificial intelligence OR machine learning\"\n",
    "MAX_RESULTS = 5  # Number of papers to retrieve (capped for demo - increase for production)\n",
    "DAYS_BACK = 7  # How many days back to search\n",
    "ARXIV_CATEGORIES = [\"cs.AI\", \"cs.LG\", \"cs.CL\", \"cs.CV\", \"cs.NE\"]  # AI/ML categories\n",
    "\n",
    "# ============================================================\n",
    "# ArXiv Scraping Logic\n",
    "# ============================================================\n",
    "\n",
    "import arxiv\n",
    "from datetime import datetime, timedelta\n",
    "from io import BytesIO\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"📚 ARXIV PAPER SCRAPING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate date threshold (timezone-aware to match arxiv library)\n",
    "from datetime import timezone\n",
    "date_threshold = datetime.now(timezone.utc) - timedelta(days=DAYS_BACK)\n",
    "print(f\"\\n🔍 Searching for papers from the last {DAYS_BACK} days\")\n",
    "print(f\"   Query: {SEARCH_QUERY}\")\n",
    "print(f\"   Max results: {MAX_RESULTS}\")\n",
    "print(f\"   Categories: {', '.join(ARXIV_CATEGORIES)}\")\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "\n",
    "# Search ArXiv\n",
    "print(f\"\\n📥 Searching ArXiv...\")\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=SEARCH_QUERY,\n",
    "    max_results=MAX_RESULTS,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = list(client.results(search))\n",
    "print(f\"✅ Found {len(results)} papers\")\n",
    "\n",
    "# Filter and upload papers\n",
    "scraped_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for paper in results:\n",
    "    # Check if paper is in desired categories\n",
    "    categories = [cat.split('.')[-1] for cat in paper.categories]\n",
    "    if not any(cat in ARXIV_CATEGORIES for cat in paper.categories):\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Check if paper is recent enough (both datetimes are now timezone-aware)\n",
    "    if paper.published < date_threshold:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n📄 Processing: {paper.title[:60]}...\")\n",
    "    print(f\"   ArXiv ID: {paper.entry_id.split('/')[-1]}\")\n",
    "    print(f\"   Published: {paper.published.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Categories: {', '.join(paper.categories[:3])}\")\n",
    "    \n",
    "    try:\n",
    "        # Download PDF\n",
    "        pdf_url = paper.pdf_url\n",
    "        pdf_response = requests.get(pdf_url, timeout=30)\n",
    "        pdf_content = pdf_response.content\n",
    "        \n",
    "        # Generate S3 key\n",
    "        arxiv_id = paper.entry_id.split('/')[-1].replace('.', 'v')\n",
    "        s3_key = f\"arxiv/papers/{arxiv_id}.pdf\"\n",
    "        \n",
    "        # Upload to S3\n",
    "        s3.put_object(\n",
    "            Bucket=S3_SOURCE_BUCKET,\n",
    "            Key=s3_key,\n",
    "            Body=pdf_content,\n",
    "            ContentType='application/pdf',\n",
    "            Metadata={\n",
    "                'title': paper.title[:1000],  # S3 metadata has size limits\n",
    "                'published': paper.published.isoformat(),\n",
    "                'arxiv_id': arxiv_id,\n",
    "                'source': 'arxiv'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        scraped_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {str(e)[:100]}\")\n",
    "        skipped_count += 1\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ ARXIV SCRAPING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   📥 Papers scraped: {scraped_count}\")\n",
    "print(f\"   ⏭️  Papers skipped: {skipped_count}\")\n",
    "print(f\"   📦 S3 Bucket: {S3_SOURCE_BUCKET}\")\n",
    "print(f\"   📁 S3 Prefix: arxiv/papers/\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c03cc5",
   "metadata": {},
   "source": [
    "### Scraping AI Company Blogs with Firecrawl\n",
    "\n",
    "This cell uses Firecrawl to scrape recent blog posts from AI companies, extracting clean HTML content. Firecrawl handles JavaScript-rendered content and provides clean HTML output, making it ideal for scraping modern AI company blogs.\n",
    "\n",
    "**Demo Configuration**: For this demo, we've commented out all blog sources except Hugging Face to keep notebook runtime under 2 minutes. You can uncomment the other sources in the code below (OpenAI, DeepLearning.AI, and Anthropic) to experiment with collecting data from those sources. Customize the `DAYS_BACK` parameter or modify the `BLOG_SOURCES` dictionary to add your own sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "792a063e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🌐 BLOG SCRAPING WITH FIRECRAWL\n",
      "============================================================\n",
      "\n",
      "🔍 Scraping posts from the last 7 days\n",
      "   Sources: 1\n",
      "\n",
      "🤗 Hugging Face\n",
      "   ──────────────────────────────────────────────────\n",
      "   📍 https://huggingface.co/blog\n",
      "   🔄 Scraping directory...\n",
      "   ✅ Found 35 blog post links\n",
      "   📥 Scraping: https://huggingface.co/blog/JessyTsu1/arxiv-trick...\n",
      "   📥 Scraping: https://huggingface.co/blog/Nicolas-BZRD/when-does-reasoning...\n",
      "   📥 Scraping: https://huggingface.co/blog/NormalUhr/grpo-to-dapo-and-gspo...\n",
      "   📥 Scraping: https://huggingface.co/blog/catherinearnett/in-defense-of-to...\n",
      "   📥 Scraping: https://huggingface.co/blog/dots-ocr-ne...\n",
      "   📥 Scraping: https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-...\n",
      "   📥 Scraping: https://huggingface.co/blog/faster-transformers...\n",
      "   📥 Scraping: https://huggingface.co/blog/finegrain/model-quality-hugging-...\n",
      "   📥 Scraping: https://huggingface.co/blog/gaia2...\n",
      "   📥 Scraping: https://huggingface.co/blog/giadap/preserving-agency...\n",
      "   📊 Scraped 10 posts from Hugging Face\n",
      "\n",
      "============================================================\n",
      "✅ BLOG SCRAPING COMPLETE\n",
      "============================================================\n",
      "   📥 Total posts scraped: 10\n",
      "   📦 S3 Bucket: ai-papers-and-blogs-notebook\n",
      "   📁 S3 Prefix: blog-posts/\n",
      "\n",
      "💡 Note: Posts are now ready for Unstructured processing!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Customize these parameters\n",
    "# ============================================================\n",
    "\n",
    "# Scraping configuration\n",
    "DAYS_BACK = 7  # How many days of recent posts to retrieve\n",
    "\n",
    "# Blog source URLs (pre-configured)\n",
    "BLOG_SOURCES = {\n",
    "    \"huggingface\": {\n",
    "        \"name\": \"Hugging Face\",\n",
    "        \"directory_url\": \"https://huggingface.co/blog\",\n",
    "        \"icon\": \"🤗\"\n",
    "    },\n",
    "    # \"openai\": {\n",
    "    #     \"name\": \"OpenAI\",\n",
    "    #     \"directory_url\": \"https://openai.com/news/\",\n",
    "    #     \"icon\": \"🚀\"\n",
    "    # },\n",
    "    # \"deeplearning\": {\n",
    "    #     \"name\": \"DeepLearning.AI\",\n",
    "    #     \"directory_url\": \"https://www.deeplearning.ai/the-batch/\",\n",
    "    #     \"icon\": \"📚\"\n",
    "    # },\n",
    "    # \"anthropic\": {\n",
    "    #     \"name\": \"Anthropic\",\n",
    "    #     \"directory_url\": \"https://www.anthropic.com/research\",\n",
    "    #     \"icon\": \"🔬\"\n",
    "    # }\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# Blog Scraping Logic with Firecrawl\n",
    "# ============================================================\n",
    "\n",
    "from firecrawl import Firecrawl\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"🌐 BLOG SCRAPING WITH FIRECRAWL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Helper function to convert Firecrawl Document objects to dictionaries\n",
    "def convert_document_to_dict(doc):\n",
    "    \"\"\"Convert Firecrawl Document object to dictionary format.\"\"\"\n",
    "    if isinstance(doc, dict):\n",
    "        return doc\n",
    "        \n",
    "    # Handle Document object from newer firecrawl-py versions\n",
    "    result_dict = {}\n",
    "        \n",
    "    # Get attributes from the Document object\n",
    "    if hasattr(doc, 'markdown'):\n",
    "        result_dict['markdown'] = doc.markdown\n",
    "    if hasattr(doc, 'html'):\n",
    "        result_dict['html'] = doc.html\n",
    "    if hasattr(doc, 'links'):\n",
    "        result_dict['links'] = doc.links if doc.links else []\n",
    "    if hasattr(doc, 'metadata'):\n",
    "        # metadata is also an object, convert to dict\n",
    "        metadata_obj = doc.metadata\n",
    "        if metadata_obj:\n",
    "            if isinstance(metadata_obj, dict):\n",
    "                result_dict['metadata'] = metadata_obj\n",
    "            else:\n",
    "                # Convert metadata object to dict using __dict__ or vars()\n",
    "                result_dict['metadata'] = vars(metadata_obj) if hasattr(metadata_obj, '__dict__') else {}\n",
    "        else:\n",
    "            result_dict['metadata'] = {}\n",
    "    if hasattr(doc, 'extract'):\n",
    "        result_dict['json'] = doc.extract\n",
    "            \n",
    "    return result_dict\n",
    "\n",
    "# Filter blog links to exclude non-blog content\n",
    "def filter_blog_links(links, source_key, directory_url):\n",
    "    \"\"\"Filter links to find actual blog posts, excluding images, profiles, etc.\"\"\"\n",
    "    # Blacklist of specific URLs to exclude\n",
    "    EXCLUDED_URLS = [\n",
    "        'https://huggingface.co/blog/community',\n",
    "        'https://anthropic.com/press-kit',\n",
    "    ]\n",
    "        \n",
    "    # Extract domain from directory URL\n",
    "    directory_domain = urlparse(directory_url).netloc\n",
    "        \n",
    "    blog_links = []\n",
    "        \n",
    "    for link in links:\n",
    "        if not isinstance(link, str):\n",
    "            continue\n",
    "            \n",
    "        # Skip non-HTTP protocols\n",
    "        if not link.startswith('http'):\n",
    "            continue\n",
    "            \n",
    "        # Skip image files\n",
    "        if any(link.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.svg', '.webp']):\n",
    "            continue\n",
    "            \n",
    "        # Skip CDN and avatar URLs\n",
    "        if 'cdn-avatars' in link or '/assets/' in link:\n",
    "            continue\n",
    "            \n",
    "        # Only include links from the same domain\n",
    "        link_domain = urlparse(link).netloc\n",
    "        if link_domain != directory_domain:\n",
    "            continue\n",
    "            \n",
    "        # Source-specific filtering\n",
    "        if source_key == 'huggingface':\n",
    "            # Must have /blog/ and content after it (not just directory or community)\n",
    "            if '/blog/' in link:\n",
    "                blog_parts = link.split('/blog/')\n",
    "                if len(blog_parts) > 1 and blog_parts[1].strip('/'):\n",
    "                    # Exclude community page\n",
    "                    if link not in EXCLUDED_URLS:\n",
    "                        blog_links.append(link)\n",
    "                            \n",
    "        elif source_key == 'deeplearning':\n",
    "            # Must have /the-batch/ but NOT /tag/ (tag pages are navigation)\n",
    "            if '/the-batch/' in link and '/tag/' not in link:\n",
    "                blog_links.append(link)\n",
    "                    \n",
    "        elif source_key == 'anthropic':\n",
    "            # Include both /news/ and /research/ posts\n",
    "            if '/news/' in link or '/research/' in link:\n",
    "                if link not in EXCLUDED_URLS:\n",
    "                    blog_links.append(link)\n",
    "                        \n",
    "        elif source_key == 'openai':\n",
    "            # OpenAI uses /index/ for actual articles\n",
    "            if '/index/' in link:\n",
    "                # Exclude category pages that end with these paths\n",
    "                category_pages = ['/product-releases/', '/research/', '/safety-alignment/', '/news/']\n",
    "                is_category = any(link.endswith(cat) for cat in category_pages)\n",
    "                if not is_category:\n",
    "                    blog_links.append(link)\n",
    "        \n",
    "    # Remove duplicates and sort\n",
    "    return sorted(list(set(blog_links)))\n",
    "\n",
    "# Initialize Firecrawl and S3\n",
    "firecrawl_client = Firecrawl(api_key=FIRECRAWL_API_KEY)\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "\n",
    "date_threshold = datetime.now() - timedelta(days=DAYS_BACK)\n",
    "print(f\"\\n🔍 Scraping posts from the last {DAYS_BACK} days\")\n",
    "print(f\"   Sources: {len(BLOG_SOURCES)}\")\n",
    "\n",
    "total_scraped = 0\n",
    "\n",
    "for source_key, source_info in BLOG_SOURCES.items():\n",
    "    icon = source_info[\"icon\"]\n",
    "    name = source_info[\"name\"]\n",
    "    directory_url = source_info[\"directory_url\"]\n",
    "        \n",
    "    print(f\"\\n{icon} {name}\")\n",
    "    print(f\"   {'─'*50}\")\n",
    "    print(f\"   📍 {directory_url}\")\n",
    "        \n",
    "    try:\n",
    "        # Scrape directory page with link extraction\n",
    "        print(f\"   🔄 Scraping directory...\")\n",
    "        directory_result_raw = firecrawl_client.scrape(\n",
    "            url=directory_url,\n",
    "            formats=[\"markdown\", \"html\", \"links\"],\n",
    "            only_main_content=True\n",
    "        )\n",
    "            \n",
    "        # Convert Document to dict\n",
    "        directory_result = convert_document_to_dict(directory_result_raw)\n",
    "            \n",
    "        if not directory_result:\n",
    "            print(f\"   ❌ Failed to scrape directory\")\n",
    "            continue\n",
    "            \n",
    "        # Extract and filter blog links\n",
    "        all_links = directory_result.get('links', [])\n",
    "        blog_links = filter_blog_links(all_links, source_key, directory_url)\n",
    "            \n",
    "        print(f\"   ✅ Found {len(blog_links)} blog post links\")\n",
    "            \n",
    "        # Limit to 10 posts per source for demo\n",
    "        post_urls = blog_links[:10]\n",
    "            \n",
    "        # Scrape individual posts\n",
    "        scraped_count = 0\n",
    "        for post_url in post_urls:\n",
    "            try:\n",
    "                # Add delay to be respectful\n",
    "                import time\n",
    "                time.sleep(1)\n",
    "                    \n",
    "                print(f\"   📥 Scraping: {post_url[:60]}...\")\n",
    "                    \n",
    "                # Scrape post with HTML format\n",
    "                post_result_raw = firecrawl_client.scrape(\n",
    "                    url=post_url,\n",
    "                    formats=[\"html\"],\n",
    "                    only_main_content=True\n",
    "                )\n",
    "                    \n",
    "                # Convert Document to dict\n",
    "                post_result = convert_document_to_dict(post_result_raw)\n",
    "                    \n",
    "                if not post_result or not post_result.get('html'):\n",
    "                    print(f\"      ⚠️  No HTML returned\")\n",
    "                    continue\n",
    "                    \n",
    "                html_content = post_result['html']\n",
    "                    \n",
    "                # Generate S3 key\n",
    "                url_path = urlparse(post_url).path.strip('/').replace('/', '_')\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                s3_key = f\"blog-posts/{source_key}/{url_path}_{timestamp}.html\"\n",
    "                    \n",
    "                # Upload to S3\n",
    "                s3.put_object(\n",
    "                    Bucket=S3_SOURCE_BUCKET,\n",
    "                    Key=s3_key,\n",
    "                    Body=html_content.encode('utf-8'),\n",
    "                    ContentType='text/html',\n",
    "                    Metadata={\n",
    "                        'url': post_url[:1000],\n",
    "                        'source': source_key,\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                scraped_count += 1\n",
    "                total_scraped += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"      ❌ Error: {str(e)[:100]}\")\n",
    "            \n",
    "        print(f\"   📊 Scraped {scraped_count} posts from {name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error scraping {name}: {str(e)[:100]}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ BLOG SCRAPING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   📥 Total posts scraped: {total_scraped}\")\n",
    "print(f\"   📦 S3 Bucket: {S3_SOURCE_BUCKET}\")\n",
    "print(f\"   📁 S3 Prefix: blog-posts/\")\n",
    "print(f\"\\n💡 Note: Posts are now ready for Unstructured processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a160d1",
   "metadata": {},
   "source": [
    "## S3 Source Connector\n",
    "\n",
    "Creating the connection to your S3 document repository. This connector will authenticate with your bucket, discover PDF files, and stream them to the processing pipeline.\n",
    "\n",
    "**Recursive Processing**: The connector is configured with `recursive: true` to access files within nested folder structures, ensuring comprehensive document discovery across your entire S3 bucket hierarchy.\n",
    "\n",
    "> **Note**: For detailed S3 source connector setup instructions, see the [Unstructured S3 source connector documentation](https://docs.unstructured.io/api-reference/workflow/sources/s3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cb0f3a1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nvannest/Documents/GitHub/newsletter-scraping-and-summarization/venv/lib/python3.13/site-packages/pydantic/functional_validators.py:218: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [input_value='s3', input_type=str])\n",
      "  function=lambda v, h: h(v),\n",
      "/Users/nvannest/Documents/GitHub/newsletter-scraping-and-summarization/venv/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [input_value='s3', input_type=str])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "INFO: HTTP Request: POST https://platform.unstructuredapp.io/api/v1/sources/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created S3 PDF source connector: 643599ad-2e56-4f00-b94b-e2f6bdbeaa3a -> s3://ai-papers-and-blogs-notebook/\n",
      "📁 S3 source connector ready to read PDF documents from: ai-papers-and-blogs-notebook\n"
     ]
    }
   ],
   "source": [
    "def create_s3_source_connector():\n",
    "    \"\"\"Create an S3 source connector for PDF documents.\"\"\"\n",
    "    try:\n",
    "        if not S3_SOURCE_BUCKET:\n",
    "            raise ValueError(\"S3_SOURCE_BUCKET is required (bucket name, s3:// URL, or https:// URL)\")\n",
    "        value = S3_SOURCE_BUCKET.strip()\n",
    "\n",
    "        if value.startswith(\"s3://\"):\n",
    "            s3_style = value if value.endswith(\"/\") else value + \"/\"\n",
    "        elif value.startswith(\"http://\") or value.startswith(\"https://\"):\n",
    "            parsed = urlparse(value)\n",
    "            host = parsed.netloc\n",
    "            path = parsed.path or \"/\"\n",
    "            bucket = host.split(\".s3.\")[0]\n",
    "            s3_style = f\"s3://{bucket}{path if path.endswith('/') else path + '/'}\"\n",
    "        else:\n",
    "            s3_style = f\"s3://{value if value.endswith('/') else value + '/'}\"\n",
    "        \n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            response = client.sources.create_source(\n",
    "                request=CreateSourceRequest(\n",
    "                    create_source_connector=CreateSourceConnector(\n",
    "                        name=\"<name>\",\n",
    "                        type=\"s3\",\n",
    "                        config={\n",
    "                            \"remote_url\": s3_style,\n",
    "                            \"recursive\": True, \n",
    "                            \"key\": AWS_ACCESS_KEY_ID,\n",
    "                            \"secret\": AWS_SECRET_ACCESS_KEY,\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        source_id = response.source_connector_information.id\n",
    "        print(f\"✅ Created S3 PDF source connector: {source_id} -> {s3_style}\")\n",
    "        return source_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating S3 source connector: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create S3 source connector\n",
    "source_id = create_s3_source_connector()\n",
    "\n",
    "if source_id:\n",
    "    print(f\"📁 S3 source connector ready to read PDF documents from: {S3_SOURCE_BUCKET}\")\n",
    "else:\n",
    "    print(\"❌ Failed to create S3 source connector - check your credentials and bucket configuration\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f2faac",
   "metadata": {},
   "source": [
    "## MongoDB: Your Document Database\n",
    "\n",
    "MongoDB Atlas stores processed content from your AI papers and blog posts. The pipeline uses page-based chunking (up to 6k characters per chunk) to create structured, manageable documents for downstream summarization.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- **MongoDB Atlas cluster** (M10+ for production, M0 free tier for testing)\n",
    "- **Network access** configured for your application IP\n",
    "- **Database user** with read/write permissions\n",
    "- **Connection string** in format: `mongodb+srv://<user>:<password>@<host>/...`\n",
    "\n",
    "### Document Structure\n",
    "\n",
    "Each document represents one page-level chunk:\n",
    "```json\n",
    "{\n",
    "  \"type\": \"CompositeElement\",\n",
    "  \"text\": \"Full text content from this page/chunk...\",\n",
    "  \"metadata\": {\n",
    "    \"filename\": \"arxiv_2501.12345.pdf\",\n",
    "    \"page_number\": 1,\n",
    "    \"languages\": [\"eng\"]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "The collection is cleared before each processing run to ensure fresh data for newsletter generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7b99a",
   "metadata": {},
   "source": [
    "## MongoDB Configuration and Collection Setup\n",
    "\n",
    "This cell validates your MongoDB connection and prepares the collection for processing. It confirms environment variables (`MONGODB_URI`, `MONGODB_DATABASE`, `MONGODB_COLLECTION`), creates the database and collection if needed, and clears any existing documents for a fresh run.\n",
    "\n",
    "> **Note**: If you're running this in Google Colab, you'll need to whitelist your notebook's IP address in MongoDB Network Access. Run `!curl ifconfig.me` in a cell to get your IP address, then add it to the \"Network Access\" section of your MongoDB Atlas cluster settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec0d19cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def verify_collection_exists():\n",
    "    \"\"\"Verify that the MongoDB collection exists and is properly configured.\"\"\"\n",
    "    print(f\"🔍 Verifying collection '{MONGODB_COLLECTION}' exists...\")\n",
    "    \n",
    "    try:\n",
    "        from pymongo import MongoClient\n",
    "        \n",
    "        # Initialize MongoDB client\n",
    "        client = MongoClient(MONGODB_URI)\n",
    "        db = client[MONGODB_DATABASE]\n",
    "        \n",
    "        # Check if collection exists\n",
    "        existing_collections = db.list_collection_names()\n",
    "        \n",
    "        if MONGODB_COLLECTION not in existing_collections:\n",
    "            print(f\"❌ Collection '{MONGODB_COLLECTION}' does not exist!\")\n",
    "            return False\n",
    "        \n",
    "        # Get collection info to verify configuration\n",
    "        try:\n",
    "            collection = db[MONGODB_COLLECTION]\n",
    "            \n",
    "            # Count documents (optional check)\n",
    "            doc_count = collection.count_documents({})\n",
    "            print(f\"✅ Collection '{MONGODB_COLLECTION}' exists and is accessible\")\n",
    "            print(f\"📄 Current document count: {doc_count}\")\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as collection_error:\n",
    "            print(f\"⚠️ Collection exists but may have access issues: {collection_error}\")\n",
    "            return True  # Don't fail if we can't get detailed info\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"⚠️ MongoDB client not available - collection verification skipped\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Warning: Could not verify collection: {e}\")\n",
    "        return True  # Don't fail the pipeline for verification issues\n",
    "\n",
    "def initialize_mongodb_collection():\n",
    "    \"\"\"Initialize MongoDB collection - create database and collection if needed, then clear existing data for fresh start.\"\"\"\n",
    "    print(\"🏗️ Initializing MongoDB collection...\")\n",
    "    \n",
    "    try:\n",
    "        from pymongo import MongoClient\n",
    "        \n",
    "        # Initialize client\n",
    "        client = MongoClient(MONGODB_URI)\n",
    "        \n",
    "        # Access database (will be created automatically if it doesn't exist)\n",
    "        db = client[MONGODB_DATABASE]\n",
    "        print(f\"✅ Connected to database '{MONGODB_DATABASE}'\")\n",
    "        \n",
    "        # List existing collections\n",
    "        existing_collections = db.list_collection_names()\n",
    "        \n",
    "        # Step 1: Ensure collection exists (create if needed)\n",
    "        if MONGODB_COLLECTION not in existing_collections:\n",
    "            print(f\"📝 Creating collection '{MONGODB_COLLECTION}'...\")\n",
    "            \n",
    "            # Create the collection (MongoDB creates it automatically on first write)\n",
    "            db.create_collection(MONGODB_COLLECTION)\n",
    "            print(f\"✅ Created collection '{MONGODB_COLLECTION}'\")\n",
    "        else:\n",
    "            print(f\"✅ Collection '{MONGODB_COLLECTION}' already exists\")\n",
    "        \n",
    "        # Step 2: Clear existing data\n",
    "        collection = db[MONGODB_COLLECTION]\n",
    "        delete_result = collection.delete_many({})\n",
    "        \n",
    "        deleted_count = delete_result.deleted_count\n",
    "        print(f\"🗑️ Cleared {deleted_count} existing documents\")\n",
    "            \n",
    "        print(f\"✅ Collection '{MONGODB_COLLECTION}' is ready for document processing\")\n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"⚠️ MongoDB client not available - install with: pip install pymongo\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing MongoDB collection: {e}\")\n",
    "        print(\"💡 Troubleshooting:\")\n",
    "        print(\"   1. Verify your MONGODB_URI connection string is correct\")\n",
    "        print(\"   2. Ensure your MongoDB cluster allows connections from your IP\")\n",
    "        print(\"   3. Check that your database user has appropriate permissions\")\n",
    "        print(f\"   4. Verify database name '{MONGODB_DATABASE}' and collection '{MONGODB_COLLECTION}'\")\n",
    "        return False\n",
    "\n",
    "def run_mongodb_preprocessing():\n",
    "    \"\"\"Validate MongoDB configuration and initialize collection for fresh processing.\"\"\"\n",
    "    print(\"🔧 Running MongoDB preprocessing...\")\n",
    "    \n",
    "    try:\n",
    "        # Validate required environment variables\n",
    "        required_vars = [\n",
    "            (\"MONGODB_URI\", MONGODB_URI),\n",
    "            (\"MONGODB_DATABASE\", MONGODB_DATABASE),\n",
    "            (\"MONGODB_COLLECTION\", MONGODB_COLLECTION)\n",
    "        ]\n",
    "        \n",
    "        for var_name, var_value in required_vars:\n",
    "            if not var_value:\n",
    "                raise ValueError(f\"{var_name} is required\")\n",
    "        \n",
    "        # Basic URI validation\n",
    "        if not MONGODB_URI.startswith(\"mongodb\"):\n",
    "            raise ValueError(\"MONGODB_URI must be a valid MongoDB connection string (mongodb:// or mongodb+srv://)\")\n",
    "        \n",
    "        print(f\"🔍 MongoDB Configuration:\")\n",
    "        print(f\"  • Database: {MONGODB_DATABASE}\")\n",
    "        print(f\"  • Collection: {MONGODB_COLLECTION}\")\n",
    "        print(\"✅ MongoDB configuration validation completed successfully\")\n",
    "        \n",
    "        # Initialize collection (create if needed + clear existing data)\n",
    "        if not initialize_mongodb_collection():\n",
    "            raise Exception(\"Failed to initialize MongoDB collection\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during MongoDB preprocessing: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c14201",
   "metadata": {},
   "source": [
    "## MongoDB Destination Connector\n",
    "\n",
    "Creating the destination where processed documents will be stored. Your configured MongoDB collection will receive the extracted text content, metadata, and document structure ready for newsletter generation.\n",
    "\n",
    "> **Note**: For detailed MongoDB destination connector setup instructions, including cluster configuration and authentication requirements, see the [Unstructured MongoDB destination connector documentation](https://docs.unstructured.io/api-reference/workflow/destinations/mongodb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bee94b5c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nvannest/Documents/GitHub/newsletter-scraping-and-summarization/venv/lib/python3.13/site-packages/pydantic/functional_validators.py:218: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [input_value='mongodb', input_type=str])\n",
      "  function=lambda v, h: h(v),\n",
      "/Users/nvannest/Documents/GitHub/newsletter-scraping-and-summarization/venv/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [input_value='mongodb', input_type=str])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Input variables to create_mongodb_destination_connector:\n",
      "  • Database: scraped_publications\n",
      "  • Collection: documents\n",
      "  • Batch Size: 20\n",
      "  • Flatten Metadata: False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://platform.unstructuredapp.io/api/v1/destinations/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created MongoDB destination connector: a70289ba-e38e-4406-8ec2-87f501d36c45\n",
      "🗄️ Database: scraped_publications\n",
      "📁 Collection: documents\n",
      "🔍 MongoDB destination connector ready to store processed documents\n",
      "🗄️ Database: scraped_publications\n",
      "📁 Collection: documents\n"
     ]
    }
   ],
   "source": [
    "def create_mongodb_destination_connector():\n",
    "    \"\"\"Create a MongoDB destination connector for processed results.\"\"\"\n",
    "    try:\n",
    "        # Debug: Print all input variables\n",
    "        print(f\"📊 Input variables to create_mongodb_destination_connector:\")\n",
    "        print(f\"  • Database: {MONGODB_DATABASE}\")\n",
    "        print(f\"  • Collection: {MONGODB_COLLECTION}\")\n",
    "        print(f\"  • Batch Size: 20\")\n",
    "        print(f\"  • Flatten Metadata: False\")\n",
    "        print()\n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            response = client.destinations.create_destination(\n",
    "                request=CreateDestinationRequest(\n",
    "                    create_destination_connector=CreateDestinationConnector(\n",
    "                        name=f\"mongodb_newsletter_pipeline_destination_{int(time.time())}\",\n",
    "                        type=\"mongodb\",\n",
    "                        config={\n",
    "                            \"uri\": MONGODB_URI,\n",
    "                            \"database\": MONGODB_DATABASE,\n",
    "                            \"collection\": MONGODB_COLLECTION,\n",
    "                            \"batch_size\": 20,\n",
    "                            \"flatten_metadata\": False\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        destination_id = response.destination_connector_information.id\n",
    "        print(f\"✅ Created MongoDB destination connector: {destination_id}\")\n",
    "        print(f\"🗄️ Database: {MONGODB_DATABASE}\")\n",
    "        print(f\"📁 Collection: {MONGODB_COLLECTION}\")\n",
    "        return destination_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating MongoDB destination connector: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_mongodb_destination_connector(destination_id):\n",
    "    \"\"\"Test the MongoDB destination connector.\"\"\"\n",
    "    if destination_id and destination_id != SKIPPED:\n",
    "        print(f\"🔍 MongoDB destination connector ready to store processed documents\")\n",
    "        print(f\"🗄️ Database: {MONGODB_DATABASE}\")\n",
    "        print(f\"📁 Collection: {MONGODB_COLLECTION}\")\n",
    "    else:\n",
    "        print(\"❌ Failed to create MongoDB destination connector - check your credentials and configuration\")\n",
    "\n",
    "# Create MongoDB destination connector\n",
    "destination_id = create_mongodb_destination_connector()\n",
    "\n",
    "test_mongodb_destination_connector(destination_id) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5acd6",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline\n",
    "\n",
    "Configuring the two-stage pipeline: Hi-Res Partitioning → Page Chunking.\n",
    "\n",
    "The pipeline uses Unstructured's hi_res strategy for detailed document analysis with advanced table detection, then chunks content by page to preserve document structure for downstream summarization and newsletter generation.\n",
    "\n",
    "**Stage 1 - High-Resolution Partitioning:**\n",
    "- **Strategy**: `hi_res` for detailed document processing\n",
    "- **Table Detection**: `pdf_infer_table_structure=True` for accurate table extraction\n",
    "- **Page Breaks**: `include_page_breaks=True` to maintain document structure\n",
    "- **Text-Focused**: Excludes images, page numbers, and formatting elements\n",
    "- **Output**: Individual elements (Title, NarrativeText, Table, etc.) with metadata\n",
    "\n",
    "**Stage 2 - Page-Based Chunking:**\n",
    "- **Strategy**: `chunk_by_page` to maintain natural page boundaries\n",
    "- **Original Elements**: `include_orig_elements=False` (not used in downstream workflows)\n",
    "- **Max Characters**: `max_characters=6000` for manageable chunk sizes\n",
    "- **Output**: Page-level chunks (up to 6k characters) ideal for summarization and newsletter generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7e87c",
   "metadata": {},
   "source": [
    "## Creating Your Document Processing Workflow\n",
    "\n",
    "Assembling the high-resolution processing pipeline to connect S3 documents to the processing workflow. This two-stage workflow uses hi_res partitioning for detailed analysis and page-based chunking to preserve document structure for effective summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d06d2a71",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_image_workflow_nodes():\n",
    "    \"\"\"Create workflow nodes for document processing pipeline.\"\"\"\n",
    "    # High-res partitioner for detailed document processing\n",
    "    partitioner_workflow_node = WorkflowNode(\n",
    "        name=\"Partitioner\",\n",
    "        subtype=\"unstructured_api\",\n",
    "        type=\"partition\",\n",
    "        settings={\n",
    "            \"strategy\": \"hi_res\",\n",
    "            \"include_page_breaks\": True,\n",
    "            \"pdf_infer_table_structure\": True,\n",
    "            \"exclude_elements\": [\n",
    "                \"Address\",\n",
    "                \"PageBreak\",\n",
    "                \"Formula\",\n",
    "                \"EmailAddress\",\n",
    "                \"PageNumber\",\n",
    "                \"Image\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Chunk by page - keeps page boundaries intact\n",
    "    chunker_node = WorkflowNode(\n",
    "        name=\"Chunker\",\n",
    "        subtype=\"chunk_by_page\",\n",
    "        type=\"chunk\",\n",
    "        settings={\n",
    "            \"include_orig_elements\": False,\n",
    "            \"max_characters\": 6000  # Maximum 6k characters per chunk\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return (partitioner_workflow_node, chunker_node)\n",
    "\n",
    "def create_single_workflow(s3_source_id, destination_id):\n",
    "    \"\"\"Create a single workflow for S3 document processing.\"\"\"\n",
    "    try:\n",
    "        partitioner_node, chunker_node = create_image_workflow_nodes()\n",
    "\n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            s3_workflow = CreateWorkflow(\n",
    "                name=f\"S3-Document-Processing-Workflow_{int(time.time())}\",\n",
    "                source_id=s3_source_id,\n",
    "                destination_id=destination_id,\n",
    "                workflow_type=WorkflowType.CUSTOM,\n",
    "                workflow_nodes=[\n",
    "                    partitioner_node,\n",
    "                    chunker_node\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            s3_response = client.workflows.create_workflow(\n",
    "                request=CreateWorkflowRequest(\n",
    "                    create_workflow=s3_workflow\n",
    "                )\n",
    "            )\n",
    "\n",
    "        s3_workflow_id = s3_response.workflow_information.id\n",
    "        print(f\"✅ Created S3 document processing workflow: {s3_workflow_id}\")\n",
    "\n",
    "        return s3_workflow_id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating document processing workflow: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8df0b8",
   "metadata": {},
   "source": [
    "## Starting Your Document Processing Job\n",
    "\n",
    "With our workflow configured, it's time to put it into action. This step submits the auto partitioning workflow to the Unstructured API and returns a job ID for monitoring the document processing and text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a9591cb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_workflow(workflow_id, workflow_name):\n",
    "    \"\"\"Run a workflow and return job information.\"\"\"\n",
    "    try:\n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            response = client.workflows.run_workflow(\n",
    "                request={\"workflow_id\": workflow_id}\n",
    "            )\n",
    "        \n",
    "        job_id = response.job_information.id\n",
    "        print(f\"✅ Started {workflow_name} job: {job_id}\")\n",
    "        return job_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error running {workflow_name} workflow: {e}\")\n",
    "        return None\n",
    "\n",
    "def poll_job_status(job_id, job_name, wait_time=30):\n",
    "    \"\"\"Poll job status until completion.\"\"\"\n",
    "    print(f\"⏳ Monitoring {job_name} job status...\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "                response = client.jobs.get_job(\n",
    "                    request={\"job_id\": job_id}\n",
    "                )\n",
    "            \n",
    "            job = response.job_information\n",
    "            status = job.status\n",
    "            \n",
    "            if status in [\"SCHEDULED\", \"IN_PROGRESS\"]:\n",
    "                print(f\"⏳ {job_name} job status: {status}\")\n",
    "                time.sleep(wait_time)\n",
    "            elif status == \"COMPLETED\":\n",
    "                print(f\"✅ {job_name} job completed successfully!\")\n",
    "                return job\n",
    "            elif status == \"FAILED\":\n",
    "                print(f\"❌ {job_name} job failed!\")\n",
    "                return job\n",
    "            else:\n",
    "                print(f\"❓ Unknown {job_name} job status: {status}\")\n",
    "                return job\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error polling {job_name} job status: {e}\")\n",
    "            time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d139ac6",
   "metadata": {},
   "source": [
    "## Monitoring Your Document Processing Progress\n",
    "\n",
    "Jobs progress through scheduled, in-progress, completed, or failed states. The `poll_job_status` function checks status every 30 seconds and blocks execution until processing completes, so you can see exactly what's happening with your auto partitioning and text extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd06782",
   "metadata": {},
   "source": [
    "## Pipeline Execution Summary\n",
    "\n",
    "The following summary displays all resources created during document processing pipeline setup: S3 data source path, connector IDs, workflow ID, job ID, and processing status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e2f40e4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def print_pipeline_summary(workflow_id, job_id):\n",
    "    \"\"\"Print pipeline summary for document processing workflow.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"📊 DOCUMENT PROCESSING PIPELINE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"📁 S3 Source: {S3_SOURCE_BUCKET}\")\n",
    "    print(f\"📤 MongoDB Destination: {MONGODB_DATABASE}/{MONGODB_COLLECTION}\")\n",
    "    print(f\"\")\n",
    "    print(f\"⚙️ Document Processing Workflow ID: {workflow_id}\")\n",
    "    print(f\"🚀 Document Processing Job ID: {job_id}\")\n",
    "    print()\n",
    "    print(\"💡 Monitor job progress at: https://platform.unstructured.io\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "def verify_pipeline_results(job_id=None):\n",
    "    \"\"\"\n",
    "    Verify the document processing pipeline results by checking job status.\n",
    "    \n",
    "    Note: MongoDB verification requires additional setup for direct database queries.\n",
    "    This function focuses on job status verification.\n",
    "\n",
    "    Args:\n",
    "        job_id (str, optional): If provided, will poll job status until completion before verification.\n",
    "                               If None, assumes job has completed.\n",
    "    \"\"\"\n",
    "\n",
    "    if job_id is not None and job_id != \"\" and isinstance(job_id, str):\n",
    "        print(\"🔍 Starting verification process...\")\n",
    "        print(\"⏳ Polling job status until completion...\")\n",
    "\n",
    "        job_info = poll_job_status(job_id, \"Document Processing\")\n",
    "\n",
    "        if not job_info or job_info.status != \"COMPLETED\":\n",
    "            print(f\"\\n❌ Job did not complete successfully. Status: {job_info.status if job_info else 'Unknown'}\")\n",
    "            print(\"💡 Check the Unstructured dashboard for more details.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n🔍 Job completed successfully!\")\n",
    "        print(\"-\" * 50)\n",
    "    else:\n",
    "        if job_id is not None:\n",
    "            print(f\"⚠️  Invalid job_id provided: {job_id} (type: {type(job_id)})\")\n",
    "        print(\"🔍 Verifying processed results (skipping job polling)...\")\n",
    "\n",
    "    try:\n",
    "        print(f\"📊 MongoDB Configuration:\")\n",
    "        print(f\"   🗄️ Database: {MONGODB_DATABASE}\")\n",
    "        print(f\"   📁 Collection: {MONGODB_COLLECTION}\")\n",
    "        print(f\"   🔗 Connection: {'*' * 20}...{MONGODB_URI[-10:] if len(MONGODB_URI) > 10 else '***'}\")\n",
    "        \n",
    "        print(f\"\\n✅ Pipeline completed successfully!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"🎉 SCRAPED-PUBLICATIONS PIPELINE VERIFICATION COMPLETE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"✅ Job completed successfully\")\n",
    "        print(\"✅ Data has been written to MongoDB collection\")\n",
    "        print(\"📚 Documents are now stored in MongoDB database\")\n",
    "        print(\"🤖 Ready for data retrieval and summarization!\")\n",
    "        print(\"\\n💡 To query your data, use the MongoDB client or aggregation pipelines\")\n",
    "        print(f\"🗄️ Database: {MONGODB_DATABASE}\")\n",
    "        print(f\"📁 Collection: {MONGODB_COLLECTION}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error verifying results: {e}\")\n",
    "        print(\"💡 This is normal if workflow is still processing or if there is a connection issue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f387ac",
   "metadata": {},
   "source": [
    "## Orchestrating Your Complete Document Processing Pipeline\n",
    "\n",
    "We'll now execute the pipeline in distinct steps, allowing you to monitor progress at each stage: preprocessing, connector setup, workflow creation, execution, and results validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf1209",
   "metadata": {},
   "source": [
    "### Step 1: MongoDB Preprocessing\n",
    "\n",
    "First, we validate the MongoDB connection and prepare the collection for processing.\n",
    "\n",
    "> **Note**: If you're running this in Google Colab, you'll need to whitelist your notebook's IP address in MongoDB Network Access. Run `!curl ifconfig.me` in a cell to get your IP address, then add it to the \"Network Access\" section of your MongoDB Atlas cluster settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4abbe53c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Newsletter Document Processing Pipeline\n",
      "\n",
      "🔧 Step 1: MongoDB preprocessing\n",
      "--------------------------------------------------\n",
      "🔧 Running MongoDB preprocessing...\n",
      "🔍 MongoDB Configuration:\n",
      "  • Database: scraped_publications\n",
      "  • Collection: documents\n",
      "✅ MongoDB configuration validation completed successfully\n",
      "🏗️ Initializing MongoDB collection...\n",
      "✅ Connected to database 'scraped_publications'\n",
      "✅ Collection 'documents' already exists\n",
      "🗑️ Cleared 1445 existing documents\n",
      "✅ Collection 'documents' is ready for document processing\n",
      "✅ MongoDB preprocessing completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 1: MongoDB preprocessing\n",
    "print(\"🚀 Starting Newsletter Document Processing Pipeline\")\n",
    "print(\"\\n🔧 Step 1: MongoDB preprocessing\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "preprocessing_success = run_mongodb_preprocessing()\n",
    "\n",
    "if preprocessing_success:\n",
    "    print(\"✅ MongoDB preprocessing completed successfully\")\n",
    "else:\n",
    "    print(\"❌ Failed to complete MongoDB preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e5d73b",
   "metadata": {},
   "source": [
    "### Step 2-3: Create Data Connectors\n",
    "\n",
    "Next, we create the connectors that link your S3 content bucket to MongoDB storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "275fde99",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 Step 2: Creating S3 source connector\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://platform.unstructuredapp.io/api/v1/sources/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created S3 PDF source connector: fbd6fa63-20da-4bde-8838-db4e6fe60e68 -> s3://ai-papers-and-blogs-notebook/\n",
      "\n",
      "🎯 Step 3: Creating MongoDB destination connector\n",
      "--------------------------------------------------\n",
      "📊 Input variables to create_mongodb_destination_connector:\n",
      "  • Database: scraped_publications\n",
      "  • Collection: documents\n",
      "  • Batch Size: 20\n",
      "  • Flatten Metadata: False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://platform.unstructuredapp.io/api/v1/destinations/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created MongoDB destination connector: e1faf404-3166-4307-bbfc-6b7f4249c860\n",
      "🗄️ Database: scraped_publications\n",
      "📁 Collection: documents\n",
      "✅ Connectors created successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create S3 source connector\n",
    "print(\"\\n🔗 Step 2: Creating S3 source connector\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "s3_source_id = create_s3_source_connector()\n",
    "\n",
    "if s3_source_id:\n",
    "    # Step 3: Create MongoDB destination connector\n",
    "    print(\"\\n🎯 Step 3: Creating MongoDB destination connector\")\n",
    "    print(\"-\" * 50)\n",
    "        \n",
    "    destination_id = create_mongodb_destination_connector()\n",
    "        \n",
    "    if destination_id:\n",
    "        print(\"✅ Connectors created successfully\")\n",
    "    else:\n",
    "        print(\"❌ Failed to create MongoDB destination connector\")\n",
    "else:\n",
    "    print(\"❌ Failed to create S3 source connector\")\n",
    "    destination_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0911422",
   "metadata": {},
   "source": [
    "### Step 4: Create Processing Workflow\n",
    "\n",
    "Now we'll create the document processing workflow with high-resolution partitioning and page-based chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac3f5e19",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Step 4: Creating document processing workflow\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://platform.unstructuredapp.io/api/v1/workflows/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created S3 document processing workflow: 832c73ba-4c1e-45a7-9e94-014789bf9905\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create document processing workflow\n",
    "print(\"\\n⚙️ Step 4: Creating document processing workflow\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if s3_source_id and destination_id:\n",
    "    # Create workflow nodes inline\n",
    "    try:\n",
    "        # High-res partitioner for detailed document processing\n",
    "        partitioner_workflow_node = WorkflowNode(\n",
    "            name=\"Partitioner\",\n",
    "            subtype=\"unstructured_api\",\n",
    "            type=\"partition\",\n",
    "            settings={\n",
    "                \"strategy\": \"hi_res\",\n",
    "                \"include_page_breaks\": True,\n",
    "                \"pdf_infer_table_structure\": True,\n",
    "                \"exclude_elements\": [\n",
    "                    \"Address\",\n",
    "                    \"PageBreak\",\n",
    "                    \"Formula\",\n",
    "                    \"EmailAddress\",\n",
    "                    \"PageNumber\",\n",
    "                    \"Image\"\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Chunk by page - keeps page boundaries intact\n",
    "        chunker_node = WorkflowNode(\n",
    "            name=\"Chunker\",\n",
    "            subtype=\"chunk_by_page\",\n",
    "            type=\"chunk\",\n",
    "            settings={\n",
    "                \"include_orig_elements\": False,\n",
    "                \"max_characters\": 6000  # Maximum 6k characters per chunk\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Create the workflow\n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            s3_workflow = CreateWorkflow(\n",
    "                name=f\"S3-Document-Processing-Workflow_{int(time.time())}\",\n",
    "                source_id=s3_source_id,\n",
    "                destination_id=destination_id,\n",
    "                workflow_type=WorkflowType.CUSTOM,\n",
    "                workflow_nodes=[\n",
    "                    partitioner_workflow_node,\n",
    "                    chunker_node\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            s3_response = client.workflows.create_workflow(\n",
    "                request=CreateWorkflowRequest(\n",
    "                    create_workflow=s3_workflow\n",
    "                )\n",
    "            )\n",
    "\n",
    "        workflow_id = s3_response.workflow_information.id\n",
    "        print(f\"✅ Created S3 document processing workflow: {workflow_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating document processing workflow: {e}\")\n",
    "        workflow_id = None\n",
    "else:\n",
    "    print(\"⚠️ Skipping workflow creation - connectors not available\")\n",
    "    workflow_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6991d1e2",
   "metadata": {},
   "source": [
    "### Step 5: Execute Workflow\n",
    "\n",
    "Run the workflow to start processing your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee9d5664",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Step 5: Running workflow\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://platform.unstructuredapp.io/api/v1/workflows/832c73ba-4c1e-45a7-9e94-014789bf9905/run \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Started S3 Document Processing job: 89464a12-ea03-48b6-b9d6-8ef08bc774e6\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Run the workflow\n",
    "print(\"\\n🚀 Step 5: Running workflow\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if workflow_id:\n",
    "    # Run the workflow inline\n",
    "    try:\n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            response = client.workflows.run_workflow(\n",
    "                request={\"workflow_id\": workflow_id}\n",
    "            )\n",
    "            \n",
    "        job_id = response.job_information.id\n",
    "        print(f\"✅ Started S3 Document Processing job: {job_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error running S3 Document Processing workflow: {e}\")\n",
    "        job_id = None\n",
    "else:\n",
    "    print(\"⚠️ Skipping workflow execution - workflow not created\")\n",
    "    job_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624471e",
   "metadata": {},
   "source": [
    "### Step 6: Pipeline Summary\n",
    "\n",
    "Display the pipeline configuration and job information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0d6cce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📊 DOCUMENT PROCESSING PIPELINE SUMMARY\n",
      "================================================================================\n",
      "📁 S3 Source: ai-papers-and-blogs-notebook\n",
      "📤 MongoDB Destination: scraped_publications/documents\n",
      "\n",
      "⚙️ Document Processing Workflow ID: 832c73ba-4c1e-45a7-9e94-014789bf9905\n",
      "🚀 Document Processing Job ID: 89464a12-ea03-48b6-b9d6-8ef08bc774e6\n",
      "\n",
      "💡 Monitor job progress at: https://platform.unstructured.io\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Display pipeline summary\n",
    "if workflow_id and job_id:\n",
    "    print_pipeline_summary(workflow_id, job_id)\n",
    "else:\n",
    "    print(\"\\n⚠️ Pipeline incomplete - check previous steps for errors\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b0c22",
   "metadata": {},
   "source": [
    "## Monitoring Job Progress and Viewing Processed Documents\n",
    "\n",
    "The code above starts your document processing pipeline and returns a job ID. Now run the verification block below to monitor the job progress and confirm the processed content has been stored in your MongoDB collection.\n",
    "\n",
    "This verification process will:\n",
    "- Poll the job status until completion\n",
    "- Confirm successful data storage in your MongoDB collection\n",
    "- Display pipeline completion status and collection information\n",
    "- Validate that documents and metadata are ready for retrieval and summarization\n",
    "\n",
    "**Note**: The verification block will wait for job completion before displaying results, so you can run it immediately after the pipeline starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3703ac0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Starting verification process...\n",
      "⏳ This will monitor job progress and display results when complete\n",
      "------------------------------------------------------------\n",
      "📋 Using job_id from main pipeline: 89464a12-ea03-48b6-b9d6-8ef08bc774e6\n",
      "🔍 Starting verification process...\n",
      "⏳ Polling job status until completion...\n",
      "⏳ Monitoring Document Processing job status...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://platform.unstructuredapp.io/api/v1/jobs/89464a12-ea03-48b6-b9d6-8ef08bc774e6 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Document Processing job status: JobStatus.SCHEDULED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://platform.unstructuredapp.io/api/v1/jobs/89464a12-ea03-48b6-b9d6-8ef08bc774e6 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Document Processing job status: JobStatus.IN_PROGRESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://platform.unstructuredapp.io/api/v1/jobs/89464a12-ea03-48b6-b9d6-8ef08bc774e6 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Document Processing job status: JobStatus.IN_PROGRESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://platform.unstructuredapp.io/api/v1/jobs/89464a12-ea03-48b6-b9d6-8ef08bc774e6 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Document Processing job status: JobStatus.IN_PROGRESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://platform.unstructuredapp.io/api/v1/jobs/89464a12-ea03-48b6-b9d6-8ef08bc774e6 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Document Processing job completed successfully!\n",
      "\n",
      "🔍 Job completed successfully!\n",
      "--------------------------------------------------\n",
      "📊 MongoDB Configuration:\n",
      "   🗄️ Database: scraped_publications\n",
      "   📁 Collection: documents\n",
      "   🔗 Connection: ********************...=documents\n",
      "\n",
      "✅ Pipeline completed successfully!\n",
      "======================================================================\n",
      "🎉 SCRAPED-PUBLICATIONS PIPELINE VERIFICATION COMPLETE\n",
      "======================================================================\n",
      "✅ Job completed successfully\n",
      "✅ Data has been written to MongoDB collection\n",
      "📚 Documents are now stored in MongoDB database\n",
      "🤖 Ready for data retrieval and summarization!\n",
      "\n",
      "💡 To query your data, use the MongoDB client or aggregation pipelines\n",
      "🗄️ Database: scraped_publications\n",
      "📁 Collection: documents\n"
     ]
    }
   ],
   "source": [
    "# Verification Block - Run this after the main pipeline to monitor progress and view results\n",
    "# This block will wait for job completion and then display 5 random records with images\n",
    "\n",
    "print(\"🔍 Starting verification process...\")\n",
    "print(\"⏳ This will monitor job progress and display results when complete\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check if job_id is defined from the main pipeline execution above\n",
    "try:\n",
    "    # Try to access job_id variable\n",
    "    if 'job_id' in locals() or 'job_id' in globals():\n",
    "        print(f\"📋 Using job_id from main pipeline: {job_id}\")\n",
    "        verify_pipeline_results(job_id)\n",
    "    else:\n",
    "        print(\"⚠️  job_id not found - running verification without job polling\")\n",
    "        verify_pipeline_results()\n",
    "except NameError:\n",
    "    print(\"⚠️  job_id variable not defined - running verification without job polling\")\n",
    "    verify_pipeline_results()\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Error accessing job_id: {e} - running verification without job polling\")\n",
    "    verify_pipeline_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db7414",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🤖 Orchestrator Agent: Autonomous Pipeline Management\n",
    "\n",
    "Now that you've seen how to run this process manually, let's wrap these pipeline steps in an agentic system that can orchestrate the entire workflow autonomously.\n",
    "\n",
    "**Orchestrator Agent** - Manages the complete pipeline from S3 → MongoDB:\n",
    "- Checks S3 for documents\n",
    "- Gets initial MongoDB count\n",
    "- **Creates workflow** (connectors + processing nodes)\n",
    "- Triggers the workflow\n",
    "- Waits for completion\n",
    "- Verifies MongoDB (with before/after comparison)\n",
    "- Cleans up S3\n",
    "\n",
    "The agent uses self-contained tools that directly call the Unstructured API, demonstrating how to build fully autonomous document processing systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab863475",
   "metadata": {},
   "source": [
    "### Orchestrator Agent Setup\n",
    "\n",
    "The Orchestrator Agent uses LangChain to autonomously manage the document processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f2c950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Orchestrator Agent ready!\n",
      "📋 Available tools: check_s3_documents, get_mongodb_count, create_workflow, trigger_workflow, wait_for_completion, verify_mongodb, clear_s3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ORCHESTRATOR AGENT\n",
    "Autonomous pipeline management with self-contained tools\n",
    "\"\"\"\n",
    "\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Unstructured SDK imports (needed for workflow creation)\n",
    "from unstructured_client import UnstructuredClient\n",
    "from unstructured_client.models.operations import (\n",
    "    CreateSourceRequest,\n",
    "    CreateDestinationRequest,\n",
    "    CreateWorkflowRequest\n",
    ")\n",
    "from unstructured_client.models.shared import (\n",
    "    CreateSourceConnector,\n",
    "    CreateDestinationConnector,\n",
    "    WorkflowNode,\n",
    "    WorkflowType,\n",
    "    CreateWorkflow\n",
    ")\n",
    "import time\n",
    "\n",
    "# ============================================================\n",
    "# Self-Contained Tool Functions\n",
    "# ============================================================\n",
    "\n",
    "def check_s3_documents(bucket_name: str) -> dict:\n",
    "    \"\"\"List and count documents in S3 bucket.\"\"\"\n",
    "    try:\n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "            region_name=AWS_REGION\n",
    "        )\n",
    "        \n",
    "        response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "        \n",
    "        if 'Contents' not in response:\n",
    "            return {\n",
    "                \"status\": \"empty\",\n",
    "                \"total_files\": 0,\n",
    "                \"message\": f\"Bucket {bucket_name} is empty\"\n",
    "            }\n",
    "        \n",
    "        files = response['Contents']\n",
    "        total_files = len(files)\n",
    "        \n",
    "        # Count by type\n",
    "        pdf_count = sum(1 for f in files if f['Key'].endswith('.pdf'))\n",
    "        html_count = sum(1 for f in files if f['Key'].endswith('.html'))\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"total_files\": total_files,\n",
    "            \"pdf_files\": pdf_count,\n",
    "            \"html_files\": html_count,\n",
    "            \"message\": f\"Found {total_files} files in S3 ({pdf_count} PDFs, {html_count} HTML)\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error checking S3: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def get_mongodb_count_tool(_: str = \"\") -> dict:\n",
    "    \"\"\"Get current document count in MongoDB.\"\"\"\n",
    "    try:\n",
    "        from pymongo import MongoClient\n",
    "        \n",
    "        client = MongoClient(MONGODB_URI)\n",
    "        db = client[MONGODB_DATABASE]\n",
    "        collection = db[MONGODB_COLLECTION]\n",
    "        \n",
    "        doc_count = collection.count_documents({})\n",
    "        composite_count = collection.count_documents({\"type\": \"CompositeElement\"})\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"total_documents\": doc_count,\n",
    "            \"composite_elements\": composite_count,\n",
    "            \"message\": f\"Current MongoDB count: {doc_count} total documents ({composite_count} CompositeElements)\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error counting MongoDB documents: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def create_workflow_tool(bucket_name: str) -> dict:\n",
    "    \"\"\"Create complete workflow: connectors + workflow. Returns workflow_id.\"\"\"\n",
    "    try:\n",
    "        print(\"⚙️  Creating S3 source connector...\")\n",
    "        \n",
    "        # Create S3 source connector (EXACT COPY from manual code)\n",
    "        value = bucket_name.strip()\n",
    "        if value.startswith(\"s3://\"):\n",
    "            s3_style = value if value.endswith(\"/\") else value + \"/\"\n",
    "        elif value.startswith(\"http://\") or value.startswith(\"https://\"):\n",
    "            from urllib.parse import urlparse\n",
    "            parsed = urlparse(value)\n",
    "            host = parsed.netloc\n",
    "            path = parsed.path or \"/\"\n",
    "            bucket = host.split(\".s3.\")[0]\n",
    "            s3_style = f\"s3://{bucket}{path if path.endswith('/') else path + '/'}\"\n",
    "        else:\n",
    "            s3_style = f\"s3://{value if value.endswith('/') else value + '/'}\"\n",
    "        \n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            response = client.sources.create_source(\n",
    "                request=CreateSourceRequest(\n",
    "                    create_source_connector=CreateSourceConnector(\n",
    "                        name=\"<name>\",\n",
    "                        type=\"s3\",\n",
    "                        config={\n",
    "                            \"remote_url\": s3_style,\n",
    "                            \"recursive\": True, \n",
    "                            \"key\": AWS_ACCESS_KEY_ID,\n",
    "                            \"secret\": AWS_SECRET_ACCESS_KEY,\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        s3_source_id = response.source_connector_information.id\n",
    "        print(f\"✅ S3 connector created: {s3_source_id}\")\n",
    "        \n",
    "        print(\"⚙️  Creating MongoDB destination connector...\")\n",
    "        \n",
    "        # Create MongoDB destination connector (EXACT COPY from manual code)\n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            response = client.destinations.create_destination(\n",
    "                request=CreateDestinationRequest(\n",
    "                    create_destination_connector=CreateDestinationConnector(\n",
    "                        name=f\"mongodb_newsletter_pipeline_destination_{int(time.time())}\",\n",
    "                        type=\"mongodb\",\n",
    "                        config={\n",
    "                            \"uri\": MONGODB_URI,\n",
    "                            \"database\": MONGODB_DATABASE,\n",
    "                            \"collection\": MONGODB_COLLECTION,\n",
    "                            \"batch_size\": 20,\n",
    "                            \"flatten_metadata\": False\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        destination_id = response.destination_connector_information.id\n",
    "        print(f\"✅ MongoDB connector created: {destination_id}\")\n",
    "        \n",
    "        print(\"⚙️  Creating workflow with hi_res partitioning...\")\n",
    "        \n",
    "        # Create workflow with nodes (EXACT COPY from manual code)\n",
    "        partitioner_node = WorkflowNode(\n",
    "            name=\"Partitioner\",\n",
    "            subtype=\"unstructured_api\",\n",
    "            type=\"partition\",\n",
    "            settings={\n",
    "                \"strategy\": \"hi_res\",\n",
    "                \"include_page_breaks\": True,\n",
    "                \"pdf_infer_table_structure\": True,\n",
    "                \"exclude_elements\": [\n",
    "                    \"Address\",\n",
    "                    \"PageBreak\",\n",
    "                    \"Formula\",\n",
    "                    \"EmailAddress\",\n",
    "                    \"PageNumber\",\n",
    "                    \"Image\"\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        chunker_node = WorkflowNode(\n",
    "            name=\"Chunker\",\n",
    "            subtype=\"chunk_by_page\",\n",
    "            type=\"chunk\",\n",
    "            settings={\n",
    "                \"include_orig_elements\": False,\n",
    "                \"max_characters\": 6000\n",
    "            }\n",
    "        )\n",
    "\n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            s3_workflow = CreateWorkflow(\n",
    "                name=f\"S3-Document-Processing-Workflow_{int(time.time())}\",\n",
    "                source_id=s3_source_id,\n",
    "                destination_id=destination_id,\n",
    "                workflow_type=WorkflowType.CUSTOM,\n",
    "                workflow_nodes=[\n",
    "                    partitioner_node,\n",
    "                    chunker_node\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            s3_response = client.workflows.create_workflow(\n",
    "                request=CreateWorkflowRequest(\n",
    "                    create_workflow=s3_workflow\n",
    "                )\n",
    "            )\n",
    "\n",
    "        workflow_id = s3_response.workflow_information.id\n",
    "        print(f\"✅ Workflow created: {workflow_id}\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"workflow_id\": workflow_id,\n",
    "            \"s3_source_id\": s3_source_id,\n",
    "            \"destination_id\": destination_id,\n",
    "            \"message\": f\"Workflow created successfully. ID: {workflow_id}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error creating workflow: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def trigger_workflow_tool(workflow_id: str) -> dict:\n",
    "    \"\"\"Trigger Unstructured API workflow (self-contained).\"\"\"\n",
    "    try:\n",
    "        # Direct Unstructured API call (not using external function)\n",
    "        with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "            response = client.workflows.run_workflow(\n",
    "                request={\"workflow_id\": workflow_id}\n",
    "            )\n",
    "        \n",
    "        job_id = response.job_information.id\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"job_id\": job_id,\n",
    "            \"message\": f\"Workflow triggered successfully. Job ID: {job_id}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error triggering workflow: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def wait_for_completion_tool(job_id: str) -> dict:\n",
    "    \"\"\"Wait for workflow job to complete (self-contained polling).\"\"\"\n",
    "    try:\n",
    "        print(f\"⏳ Monitoring job status: {job_id}\")\n",
    "        \n",
    "        # Poll until completion (self-contained logic)\n",
    "        while True:\n",
    "            with UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY) as client:\n",
    "                response = client.jobs.get_job(\n",
    "                    request={\"job_id\": job_id}\n",
    "                )\n",
    "            \n",
    "            job_info = response.job_information\n",
    "            status = job_info.status\n",
    "            \n",
    "            if status in [\"SCHEDULED\", \"IN_PROGRESS\"]:\n",
    "                print(f\"⏳ Job status: {status}\")\n",
    "                time.sleep(30)  # Wait 30 seconds\n",
    "            elif status == \"COMPLETED\":\n",
    "                print(f\"✅ Job completed successfully!\")\n",
    "                return {\n",
    "                    \"status\": \"success\",\n",
    "                    \"job_status\": \"COMPLETED\",\n",
    "                    \"message\": \"Job completed successfully\"\n",
    "                }\n",
    "            elif status == \"FAILED\":\n",
    "                return {\n",
    "                    \"status\": \"failed\",\n",
    "                    \"job_status\": \"FAILED\",\n",
    "                    \"message\": \"Job failed\"\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"status\": \"unknown\",\n",
    "                    \"job_status\": str(status),\n",
    "                    \"message\": f\"Job finished with unknown status: {status}\"\n",
    "                }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error waiting for job: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def verify_mongodb_tool(_: str = \"\") -> dict:\n",
    "    \"\"\"Verify processed documents in MongoDB.\"\"\"\n",
    "    try:\n",
    "        from pymongo import MongoClient\n",
    "        \n",
    "        client = MongoClient(MONGODB_URI)\n",
    "        db = client[MONGODB_DATABASE]\n",
    "        collection = db[MONGODB_COLLECTION]\n",
    "        \n",
    "        doc_count = collection.count_documents({})\n",
    "        composite_count = collection.count_documents({\"type\": \"CompositeElement\"})\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"total_documents\": doc_count,\n",
    "            \"composite_elements\": composite_count,\n",
    "            \"message\": f\"MongoDB verified: {doc_count} total documents ({composite_count} CompositeElements)\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error verifying MongoDB: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def clear_s3_bucket(bucket_name: str) -> dict:\n",
    "    \"\"\"Delete all objects from S3 bucket.\"\"\"\n",
    "    try:\n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "            region_name=AWS_REGION\n",
    "        )\n",
    "        \n",
    "        # List all objects\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "        \n",
    "        if 'Contents' not in response:\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"files_deleted\": 0,\n",
    "                \"message\": f\"Bucket {bucket_name} was already empty\"\n",
    "            }\n",
    "        \n",
    "        # Delete all objects\n",
    "        objects_to_delete = [{'Key': obj['Key']} for obj in response['Contents']]\n",
    "        \n",
    "        if objects_to_delete:\n",
    "            s3.delete_objects(\n",
    "                Bucket=bucket_name,\n",
    "                Delete={'Objects': objects_to_delete}\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"files_deleted\": len(objects_to_delete),\n",
    "            \"message\": f\"Deleted {len(objects_to_delete)} files from S3\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error clearing S3: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# ============================================================\n",
    "# Create LangChain Tools\n",
    "# ============================================================\n",
    "\n",
    "orchestrator_tools = [\n",
    "    Tool(\n",
    "        name=\"check_s3_documents\",\n",
    "        func=check_s3_documents,\n",
    "        description=\"Check S3 bucket for documents. Input: bucket_name (string). Returns count of files by type (PDF/HTML).\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"get_mongodb_count\",\n",
    "        func=get_mongodb_count_tool,\n",
    "        description=\"Get current document count in MongoDB. No input needed. Returns document counts.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"create_workflow\",\n",
    "        func=create_workflow_tool,\n",
    "        description=\"Create workflow with connectors. Input: bucket_name (string). Returns workflow_id to use for triggering.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"trigger_workflow\",\n",
    "        func=trigger_workflow_tool,\n",
    "        description=\"Start the document processing workflow. Input: workflow_id (string). Returns job_id for monitoring.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"wait_for_completion\",\n",
    "        func=wait_for_completion_tool,\n",
    "        description=\"Wait for workflow job to complete. Input: job_id (string). Polls every 30 seconds until done.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"verify_mongodb\",\n",
    "        func=verify_mongodb_tool,\n",
    "        description=\"Verify processed documents are in MongoDB. No input needed. Returns document counts.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"clear_s3\",\n",
    "        func=clear_s3_bucket,\n",
    "        description=\"Delete all files from S3 bucket after successful processing. Input: bucket_name (string).\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# Create Orchestrator Agent\n",
    "# ============================================================\n",
    "\n",
    "orchestrator_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an autonomous pipeline orchestrator. You MUST EXECUTE the tools, not just describe them.\n",
    "\n",
    "EXECUTE these steps by CALLING the tools:\n",
    "\n",
    "1. CALL get_mongodb_count to get the initial count\n",
    "2. CALL check_s3_documents with the bucket name to see what files exist\n",
    "3. If files exist, CALL create_workflow with the bucket name to create the workflow\n",
    "4. CALL trigger_workflow with the workflow_id from step 3\n",
    "5. CALL wait_for_completion with the job_id from step 4\n",
    "6. CALL get_mongodb_count again to get the final count\n",
    "7. CALL verify_mongodb to double-check the data\n",
    "8. CALL clear_s3 with the bucket name to clean up\n",
    "\n",
    "After each tool call, examine the result and proceed to the next step.\n",
    "Report the before/after MongoDB counts at the end.\n",
    "\n",
    "DO NOT write pseudocode. DO NOT describe what you would do. ACTUALLY CALL THE TOOLS.\n",
    "\n",
    "S3 bucket: {s3_bucket}\n",
    "\"\"\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "orchestrator_agent = create_openai_functions_agent(llm, orchestrator_tools, orchestrator_prompt)\n",
    "orchestrator_executor = AgentExecutor(\n",
    "    agent=orchestrator_agent,\n",
    "    tools=orchestrator_tools,\n",
    "    verbose=True,\n",
    "    max_iterations=10,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "print(\"✅ Orchestrator Agent ready!\")\n",
    "print(f\"📋 Available tools: {', '.join([t.name for t in orchestrator_tools])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b3c31",
   "metadata": {},
   "source": [
    "### Execute Orchestrator Agent\n",
    "\n",
    "Run the agent and watch it autonomously orchestrate the entire pipeline.\n",
    "\n",
    "> **Note**: If you're running this in Google Colab, you'll need to whitelist your notebook's IP address in MongoDB Network Access. Run `!curl ifconfig.me` in a cell to get your IP address, then add it to the \"Network Access\" section of your MongoDB Atlas cluster settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8069948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Starting Orchestrator Agent\n",
      "============================================================\n",
      "📋 Task: Process documents from S3 → MongoDB\n",
      "📁 S3 Bucket: ai-papers-and-blogs-notebook\n",
      "============================================================\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_mongodb_count` with ``\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m{'status': 'success', 'total_documents': 150, 'composite_elements': 140, 'message': 'Current MongoDB count: 150 total documents (140 CompositeElements)'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `check_s3_documents` with `ai-papers-and-blogs-notebook`\n",
      "responded: The initial count of documents in MongoDB is 150 total documents (140 CompositeElements). \n",
      "\n",
      "Now, let's check the S3 bucket 'ai-papers-and-blogs-notebook' for documents.\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'status': 'success', 'total_files': 15, 'pdf_files': 5, 'html_files': 10, 'message': 'Found 15 files in S3 (5 PDFs, 10 HTML)'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `create_workflow` with `ai-papers-and-blogs-notebook`\n",
      "responded: There are 15 files in the S3 bucket 'ai-papers-and-blogs-notebook' (5 PDFs, 10 HTML). \n",
      "\n",
      "Now, let's create a workflow for these documents.\n",
      "\n",
      "\u001b[0m⚙️  Creating S3 source connector...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nvannest/Documents/GitHub/newsletter-scraping-and-summarization/venv/lib/python3.13/site-packages/pydantic/functional_validators.py:218: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [input_value='s3', input_type=str])\n",
      "  function=lambda v, h: h(v),\n",
      "/Users/nvannest/Documents/GitHub/newsletter-scraping-and-summarization/venv/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [input_value='s3', input_type=str])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "INFO: HTTP Request: POST https://platform.unstructuredapp.io/api/v1/sources/ \"HTTP/1.1 200 OK\"\n",
      "/Users/nvannest/Documents/GitHub/newsletter-scraping-and-summarization/venv/lib/python3.13/site-packages/pydantic/functional_validators.py:218: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [input_value='mongodb', input_type=str])\n",
      "  function=lambda v, h: h(v),\n",
      "/Users/nvannest/Documents/GitHub/newsletter-scraping-and-summarization/venv/lib/python3.13/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [input_value='mongodb', input_type=str])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ S3 connector created: 7c8afd52-1b7b-443e-9a5e-23685c84ecfe\n",
      "⚙️  Creating MongoDB destination connector...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://platform.unstructuredapp.io/api/v1/destinations/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MongoDB connector created: 4ad95404-e920-4d2c-8040-69a81715e9a4\n",
      "⚙️  Creating workflow with hi_res partitioning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://platform.unstructuredapp.io/api/v1/workflows/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Workflow created: 654d8323-c990-4403-9b8e-7db9447cee7c\n",
      "\u001b[38;5;200m\u001b[1;3m{'status': 'success', 'workflow_id': '654d8323-c990-4403-9b8e-7db9447cee7c', 's3_source_id': '7c8afd52-1b7b-443e-9a5e-23685c84ecfe', 'destination_id': '4ad95404-e920-4d2c-8040-69a81715e9a4', 'message': 'Workflow created successfully. ID: 654d8323-c990-4403-9b8e-7db9447cee7c'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `trigger_workflow` with `654d8323-c990-4403-9b8e-7db9447cee7c`\n",
      "responded: The workflow has been created successfully with the ID: 654d8323-c990-4403-9b8e-7db9447cee7c. \n",
      "\n",
      "Now, let's trigger this workflow.\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://platform.unstructuredapp.io/api/v1/workflows/654d8323-c990-4403-9b8e-7db9447cee7c/run \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m{'status': 'success', 'job_id': '32d5567b-4029-4c74-9136-9c7603b3eada', 'message': 'Workflow triggered successfully. Job ID: 32d5567b-4029-4c74-9136-9c7603b3eada'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: GET https://platform.unstructuredapp.io/api/v1/jobs/32d5567b-4029-4c74-9136-9c7603b3eada \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wait_for_completion` with `32d5567b-4029-4c74-9136-9c7603b3eada`\n",
      "responded: The workflow has been triggered successfully with the Job ID: 32d5567b-4029-4c74-9136-9c7603b3eada. \n",
      "\n",
      "Now, let's wait for the workflow to complete.\n",
      "\n",
      "\u001b[0m⏳ Monitoring job status: 32d5567b-4029-4c74-9136-9c7603b3eada\n",
      "⏳ Job status: JobStatus.IN_PROGRESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://platform.unstructuredapp.io/api/v1/jobs/32d5567b-4029-4c74-9136-9c7603b3eada \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Job status: JobStatus.IN_PROGRESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://platform.unstructuredapp.io/api/v1/jobs/32d5567b-4029-4c74-9136-9c7603b3eada \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Job status: JobStatus.IN_PROGRESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://platform.unstructuredapp.io/api/v1/jobs/32d5567b-4029-4c74-9136-9c7603b3eada \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Job status: JobStatus.IN_PROGRESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://platform.unstructuredapp.io/api/v1/jobs/32d5567b-4029-4c74-9136-9c7603b3eada \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Job completed successfully!\n",
      "\u001b[33;1m\u001b[1;3m{'status': 'success', 'job_status': 'COMPLETED', 'message': 'Job completed successfully'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_mongodb_count` with ``\n",
      "responded: The job with ID: 32d5567b-4029-4c74-9136-9c7603b3eada has completed successfully. \n",
      "\n",
      "Now, let's get the final count of documents in MongoDB.\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m{'status': 'success', 'total_documents': 300, 'composite_elements': 280, 'message': 'Current MongoDB count: 300 total documents (280 CompositeElements)'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `verify_mongodb` with ``\n",
      "responded: The final count of documents in MongoDB is 300 total documents (280 CompositeElements). \n",
      "\n",
      "This means 150 new documents (140 CompositeElements) have been added to MongoDB.\n",
      "\n",
      "Now, let's verify the data in MongoDB.\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m{'status': 'success', 'total_documents': 300, 'composite_elements': 280, 'message': 'MongoDB verified: 300 total documents (280 CompositeElements)'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `clear_s3` with `ai-papers-and-blogs-notebook`\n",
      "responded: The data in MongoDB has been verified. The final count is 300 total documents (280 CompositeElements). \n",
      "\n",
      "Now, let's clean up the S3 bucket 'ai-papers-and-blogs-notebook'.\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'status': 'success', 'files_deleted': 15, 'message': 'Deleted 15 files from S3'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe S3 bucket 'ai-papers-and-blogs-notebook' has been cleaned up. 15 files have been deleted.\n",
      "\n",
      "In summary, we started with 150 documents in MongoDB, processed 15 files from the S3 bucket, and ended with 300 documents in MongoDB. The process was successful and the S3 bucket has been cleaned up.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "============================================================\n",
      "✅ ORCHESTRATOR COMPLETE\n",
      "============================================================\n",
      "\n",
      "The S3 bucket 'ai-papers-and-blogs-notebook' has been cleaned up. 15 files have been deleted.\n",
      "\n",
      "In summary, we started with 150 documents in MongoDB, processed 15 files from the S3 bucket, and ended with 300 documents in MongoDB. The process was successful and the S3 bucket has been cleaned up.\n"
     ]
    }
   ],
   "source": [
    "print(\"🤖 Starting Orchestrator Agent\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📋 Task: Process documents from S3 → MongoDB\")\n",
    "print(f\"📁 S3 Bucket: {S3_SOURCE_BUCKET}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "orchestrator_response = orchestrator_executor.invoke({\n",
    "    \"input\": f\"\"\"Process documents from S3 bucket '{S3_SOURCE_BUCKET}' to MongoDB.\n",
    "\n",
    "Steps:\n",
    "1. Get the INITIAL MongoDB document count\n",
    "2. Check S3 for documents\n",
    "3. If documents exist, CREATE the workflow (connectors + nodes)\n",
    "4. Trigger the workflow you just created\n",
    "5. Wait for completion\n",
    "6. Get the FINAL MongoDB document count\n",
    "7. Compare before/after counts and report the difference\n",
    "8. Clean up S3 when verified\n",
    "\n",
    "Report status at each step with clear before/after comparison.\"\"\",\n",
    "    \"s3_bucket\": S3_SOURCE_BUCKET\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ ORCHESTRATOR COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{orchestrator_response['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db7850",
   "metadata": {},
   "source": [
    "## Generating AI Newsletters from Processed Documents\n",
    "\n",
    "Now that your documents are processed and stored in MongoDB, you can generate AI-powered newsletters using the autonomous Summarizer Agent below!\n",
    "\n",
    "The agent will:\n",
    "- Retrieve documents from MongoDB\n",
    "- Generate detailed summaries for each document\n",
    "- Create an executive brief highlighting the most important developments\n",
    "- Handle context window limitations automatically\n",
    "\n",
    "You can customize the summary and executive brief prompts in the agent execution cell to control the style, length, and focus of the generated content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f65e138",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🤖 Summarizer Agent: Autonomous Newsletter Generation\n",
    "\n",
    "Now that documents are processed and stored in MongoDB, let's use an AI agent to autonomously generate the newsletter content.\n",
    "\n",
    "**Summarizer Agent** - Generates newsletter from MongoDB:\n",
    "- Retrieves documents from MongoDB\n",
    "- Handles context window limitations\n",
    "- Generates individual summaries\n",
    "- Synthesizes executive brief\n",
    "\n",
    "Like the Orchestrator Agent, this agent uses self-contained tools that demonstrate how to build autonomous content generation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e6a73",
   "metadata": {},
   "source": [
    "## Summarizer Agent Setup\n",
    "\n",
    "The Summarizer Agent uses LangChain to autonomously generate newsletter content from processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd44d348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Summarizer Agent ready!\n",
      "📋 Available tools: retrieve_documents, get_document_text, count_tokens, batch_documents, generate_summary, collapse_summaries, generate_brief\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SUMMARIZER AGENT\n",
    "Autonomous newsletter generation from MongoDB\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Tool Functions\n",
    "# ============================================================\n",
    "\n",
    "def retrieve_documents_from_mongodb(_: str = \"\") -> dict:\n",
    "    \"\"\"Retrieve list of unique filenames from MongoDB (NOT the full content).\"\"\"\n",
    "    try:\n",
    "        from pymongo import MongoClient\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        client = MongoClient(MONGODB_URI)\n",
    "        db = client[MONGODB_DATABASE]\n",
    "        collection = db[MONGODB_COLLECTION]\n",
    "        \n",
    "        # Query for CompositeElement documents\n",
    "        query = {\"type\": \"CompositeElement\"}\n",
    "        documents = list(collection.find(query))\n",
    "        \n",
    "        # Group by filename to get unique files\n",
    "        grouped = defaultdict(list)\n",
    "        for doc in documents:\n",
    "            metadata = doc.get(\"metadata\", {})\n",
    "            filename = metadata.get(\"filename\", \"unknown\")\n",
    "            grouped[filename].append(doc)\n",
    "        \n",
    "        # Return just the filenames list (NOT the full content)\n",
    "        filenames = list(grouped.keys())\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"total_documents\": len(documents),\n",
    "            \"unique_files\": len(filenames),\n",
    "            \"filenames\": filenames,  # Just the list of files\n",
    "            \"message\": f\"Found {len(filenames)} unique files to process (use get_document_text to retrieve content)\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error retrieving documents: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def get_document_text(filename: str) -> dict:\n",
    "    \"\"\"Get full text for a specific document (grouped by page, sorted, concatenated).\"\"\"\n",
    "    try:\n",
    "        from pymongo import MongoClient\n",
    "        \n",
    "        client = MongoClient(MONGODB_URI)\n",
    "        db = client[MONGODB_DATABASE]\n",
    "        collection = db[MONGODB_COLLECTION]\n",
    "        \n",
    "        # Query for this specific filename\n",
    "        query = {\n",
    "            \"type\": \"CompositeElement\",\n",
    "            \"metadata.filename\": filename\n",
    "        }\n",
    "        documents = list(collection.find(query))\n",
    "        \n",
    "        if not documents:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"No documents found for filename: {filename}\"\n",
    "            }\n",
    "        \n",
    "        # Sort by page number (same as manual code)\n",
    "        sorted_docs = sorted(documents, key=lambda d: d.get(\"metadata\", {}).get(\"page_number\", 0))\n",
    "        \n",
    "        # Concatenate text (same as manual code)\n",
    "        full_text = \"\\n\\n\".join([d.get(\"text\", \"\") for d in sorted_docs if d.get(\"text\")])\n",
    "        \n",
    "        # Truncate if too long (same as manual code)\n",
    "        max_chars = 100000\n",
    "        if len(full_text) > max_chars:\n",
    "            full_text = full_text[:max_chars]\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"filename\": filename,\n",
    "            \"pages\": len(documents),\n",
    "            \"text\": full_text,\n",
    "            \"text_length\": len(full_text),\n",
    "            \"message\": f\"Retrieved {len(documents)} pages for {filename}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error retrieving document text: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def count_tokens(text: str) -> dict:\n",
    "    \"\"\"Estimate token count and check if it fits in context window.\"\"\"\n",
    "    # Simple estimation: ~4 characters per token\n",
    "    estimated_tokens = len(text) // 4\n",
    "    max_tokens = 120000  # GPT-4 context window\n",
    "    \n",
    "    fits = estimated_tokens < max_tokens\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"estimated_tokens\": estimated_tokens,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"fits_in_window\": fits,\n",
    "        \"message\": f\"Estimated {estimated_tokens:,} tokens. {'Fits' if fits else 'Does not fit'} in context window.\"\n",
    "    }\n",
    "\n",
    "def batch_documents(documents_json: str, max_tokens: int = 100000) -> dict:\n",
    "    \"\"\"Split documents into batches that fit in context window.\"\"\"\n",
    "    try:\n",
    "        import json\n",
    "        documents = json.loads(documents_json)\n",
    "        \n",
    "        batches = []\n",
    "        current_batch = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for filename, docs in documents.items():\n",
    "            # Estimate tokens for this file\n",
    "            text = \"\\n\\n\".join([d.get(\"text\", \"\") for d in docs if d.get(\"text\")])\n",
    "            file_tokens = len(text) // 4\n",
    "            \n",
    "            if current_tokens + file_tokens > max_tokens and current_batch:\n",
    "                # Start new batch\n",
    "                batches.append(current_batch)\n",
    "                current_batch = [filename]\n",
    "                current_tokens = file_tokens\n",
    "            else:\n",
    "                current_batch.append(filename)\n",
    "                current_tokens += file_tokens\n",
    "        \n",
    "        if current_batch:\n",
    "            batches.append(current_batch)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"num_batches\": len(batches),\n",
    "            \"batches\": batches,\n",
    "            \"message\": f\"Split into {len(batches)} batches\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error batching documents: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def generate_document_summary(text: str, instructions: str = None) -> dict:\n",
    "    \"\"\"Generate summary for document text.\"\"\"\n",
    "    try:\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        \n",
    "        if not instructions:\n",
    "            instructions = \"\"\"Summarize this AI/ML content focusing on:\n",
    "            - Novel advancements or breakthroughs\n",
    "            - Performance improvements or benchmark results\n",
    "            - Practical applications and industry impact\n",
    "            \n",
    "            Keep summary focused and concise (max 12 sentences).\"\"\"\n",
    "        \n",
    "        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.3, openai_api_key=OPENAI_API_KEY)\n",
    "        \n",
    "        prompt = f\"\"\"{instructions}\n",
    "\n",
    "Content:\n",
    "{text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        summary = response.content.strip()\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"summary\": summary,\n",
    "            \"length\": len(summary),\n",
    "            \"message\": f\"Generated summary ({len(summary)} characters)\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error generating summary: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def collapse_summaries_tool(summaries_json: str, max_tokens: int = 15000) -> dict:\n",
    "    \"\"\"Collapse multiple summaries into fewer summaries to fit context window.\n",
    "    \n",
    "    Based on LangChain map-reduce pattern. Use this when you have many summaries\n",
    "    that might exceed context limits. More aggressive threshold to prevent overflow.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import json\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        \n",
    "        summaries = json.loads(summaries_json)\n",
    "        \n",
    "        if not isinstance(summaries, list):\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": \"summaries_json must be a JSON array of summary objects\"\n",
    "            }\n",
    "        \n",
    "        # Estimate tokens (rough: ~4 chars per token)\n",
    "        total_text = \" \".join([s.get(\"summary\", \"\") for s in summaries])\n",
    "        estimated_tokens = len(total_text) // 4\n",
    "        \n",
    "        if estimated_tokens < max_tokens:\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"collapsed_summaries\": summaries,\n",
    "                \"message\": f\"Summaries already fit in context ({estimated_tokens:,} tokens). No collapse needed.\"\n",
    "            }\n",
    "        \n",
    "        # Batch summaries into groups\n",
    "        batch_size = max(2, len(summaries) // 3)  # Collapse 3:1 ratio\n",
    "        batches = [summaries[i:i+batch_size] for i in range(0, len(summaries), batch_size)]\n",
    "        \n",
    "        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.3, openai_api_key=OPENAI_API_KEY)\n",
    "        \n",
    "        collapsed = []\n",
    "        for i, batch in enumerate(batches):\n",
    "            batch_text = \"\\n\\n\".join([f\"**{s.get('filename', f'Doc {j}')}**: {s.get('summary', '')}\" \n",
    "                                       for j, s in enumerate(batch)])\n",
    "            \n",
    "            prompt = f\"\"\"Consolidate these summaries into a single summary that preserves key points:\n",
    "\n",
    "{batch_text}\n",
    "\n",
    "Consolidated summary:\"\"\"\n",
    "            \n",
    "            response = llm.invoke(prompt)\n",
    "            collapsed.append({\n",
    "                \"filename\": f\"collapsed_batch_{i+1}\",\n",
    "                \"summary\": response.content.strip()\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"collapsed_summaries\": collapsed,\n",
    "            \"original_count\": len(summaries),\n",
    "            \"collapsed_count\": len(collapsed),\n",
    "            \"message\": f\"Collapsed {len(summaries)} summaries into {len(collapsed)} batches\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error collapsing summaries: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def generate_executive_brief(summaries_json: str, instructions: str = None) -> dict:\n",
    "    \"\"\"Create executive brief from summaries.\"\"\"\n",
    "    try:\n",
    "        import json\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        from datetime import datetime\n",
    "        \n",
    "        summaries = json.loads(summaries_json)\n",
    "        \n",
    "        if not instructions:\n",
    "            instructions = \"\"\"Create an executive summary (~700 words) that:\n",
    "            1. Identifies the most significant industry developments\n",
    "            2. Highlights practical applications\n",
    "            3. Notes key performance milestones\n",
    "            4. Synthesizes trends across developments\n",
    "            \n",
    "            Write for C-suite executives. Be selective - only include most relevant developments.\"\"\"\n",
    "        \n",
    "        # Build detailed content\n",
    "        detailed_content = f\"\"\"# AI Industry Weekly Digest\n",
    "*{datetime.now().strftime(\"%B %d, %Y\")}*\n",
    "\n",
    "## Summaries of Recent Publications\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        for i, summary_data in enumerate(summaries, 1):\n",
    "            filename = summary_data.get(\"filename\", f\"Document {i}\")\n",
    "            summary_text = summary_data.get(\"summary\", \"\")\n",
    "            \n",
    "            title = filename.replace(\".pdf\", \"\").replace(\".html\", \"\").replace(\"_\", \" \").title()\n",
    "            if len(title) > 80:\n",
    "                title = title[:77] + \"...\"\n",
    "            \n",
    "            detailed_content += f\"\\n### {i}. {title}\\n\\n{summary_text}\\n\\n\"\n",
    "        \n",
    "        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.3, openai_api_key=OPENAI_API_KEY)\n",
    "        \n",
    "        prompt = f\"\"\"{instructions}\n",
    "\n",
    "Detailed Newsletter:\n",
    "{detailed_content}\n",
    "\n",
    "Executive Summary:\"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        brief = response.content.strip()\n",
    "        word_count = len(brief.split())\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"brief\": brief,\n",
    "            \"word_count\": word_count,\n",
    "            \"message\": f\"Generated executive brief ({word_count} words)\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"Error generating brief: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# ============================================================\n",
    "# Create LangChain Tools\n",
    "# ============================================================\n",
    "\n",
    "summarizer_tools = [\n",
    "    Tool(\n",
    "        name=\"retrieve_documents\",\n",
    "        func=retrieve_documents_from_mongodb,\n",
    "        description=\"Get list of unique filenames from MongoDB. Returns filenames list (NOT full content). No input needed.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"get_document_text\",\n",
    "        func=get_document_text,\n",
    "        description=\"Get full text for ONE specific document by filename. Input: filename (string). Returns grouped, sorted, concatenated text.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"count_tokens\",\n",
    "        func=count_tokens,\n",
    "        description=\"Estimate token count for text. Input: text (string). Returns whether it fits in context window.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"batch_documents\",\n",
    "        func=batch_documents,\n",
    "        description=\"Split documents into batches. Input: documents_json (JSON string), max_tokens (int). Returns batches.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"generate_summary\",\n",
    "        func=generate_document_summary,\n",
    "        description=\"Generate summary for document. Input: text (string), optional instructions (string).\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"collapse_summaries\",\n",
    "        func=collapse_summaries_tool,\n",
    "        description=\"Collapse many summaries into fewer summaries if approaching context limits. Input: summaries_json (JSON array). Use if you have 10+ summaries.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"generate_brief\",\n",
    "        func=generate_executive_brief,\n",
    "        description=\"Create executive brief from summaries. Input: summaries_json (JSON array), optional instructions (string).\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# Create Summarizer Agent\n",
    "# ============================================================\n",
    "\n",
    "summarizer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You generate AI newsletter content from MongoDB documents.\n",
    "\n",
    "IMPORTANT WORKFLOW:\n",
    "1. Call retrieve_documents to get the list of filenames\n",
    "2. For EACH filename:\n",
    "   a. Call get_document_text(filename) to get the full text\n",
    "   b. Call generate_summary(text) to create a summary\n",
    "   c. Store the summary\n",
    "3. After processing 3-4 files (or sooner if context is filling):\n",
    "   a. IMMEDIATELY call collapse_summaries to reduce accumulated context\n",
    "   b. Continue with remaining files (if any)\n",
    "4. Before generating the executive brief:\n",
    "   a. Call collapse_summaries ONE MORE TIME to ensure context is minimal\n",
    "   b. Then call generate_brief with the fully collapsed summaries\n",
    "5. Present the final newsletter\n",
    "\n",
    "CONTEXT WINDOW SAFETY (CRITICAL):\n",
    "- Your conversation history accumulates tool outputs and can exceed limits\n",
    "- Call collapse_summaries EARLY and OFTEN (every 3-4 documents)\n",
    "- ALWAYS collapse before generate_brief, even if you already collapsed earlier\n",
    "- This prevents context window overflow by keeping intermediate history small\n",
    "\n",
    "CRITICAL: Process ONE document at a time. DO NOT try to retrieve all documents at once.\n",
    "Each document's chunks are already grouped, sorted by page, and concatenated by get_document_text.\n",
    "\n",
    "Focus summaries on AI/ML advancements. Keep executive brief ~700 words.\n",
    "\n",
    "MongoDB Database: {mongodb_database}\n",
    "MongoDB Collection: {mongodb_collection}\n",
    "\"\"\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "# Create Summarizer LLM with larger context window\n",
    "summarizer_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.3, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "summarizer_agent = create_openai_functions_agent(summarizer_llm, summarizer_tools, summarizer_prompt)\n",
    "summarizer_executor = AgentExecutor(\n",
    "    agent=summarizer_agent,\n",
    "    tools=summarizer_tools,\n",
    "    verbose=True,\n",
    "    max_iterations=20,  # Increased for multiple documents\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "print(\"✅ Summarizer Agent ready!\")\n",
    "print(f\"📋 Available tools: {', '.join([t.name for t in summarizer_tools])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d44e191",
   "metadata": {},
   "source": [
    "### Execute Summarizer Agent\n",
    "\n",
    "Generate this week's AI newsletter autonomously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1058df58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Starting Summarizer Agent\n",
      "============================================================\n",
      "📋 Task: Generate AI newsletter from MongoDB\n",
      "🗄️  Database: scraped_publications\n",
      "📁 Collection: documents\n",
      "📄 Documents to process: 15 unique files (280 total chunks)\n",
      "============================================================\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retrieve_documents` with ``\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'status': 'success', 'total_documents': 280, 'unique_files': 15, 'filenames': ['2510v02308v1.pdf', '2510v02312v1.pdf', 'blog_dvgodoy_fine-tuning-llm-hugging-face_20251003_161407.html', '2510v02307v1.pdf', '2510v02311v1.pdf', 'blog_JessyTsu1_arxiv-trick_20251003_161346.html', '2510v02313v1.pdf', 'blog_giadap_preserving-agency_20251003_161422.html', 'blog_faster-transformers_20251003_161412.html', 'blog_gaia2_20251003_161420.html', 'blog_dots-ocr-ne_20251003_161405.html', 'blog_NormalUhr_grpo-to-dapo-and-gspo_20251003_161356.html', 'blog_catherinearnett_in-defense-of-tokenizers_20251003_161400.html', 'blog_finegrain_model-quality-hugging-face-all-you-need_20251003_161416.html', 'blog_Nicolas-BZRD_when-does-reasoning-matter_20251003_161354.html'], 'message': 'Found 15 unique files to process (use get_document_text to retrieve content)'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_document_text` with `2510v02308v1.pdf`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m{'status': 'success', 'filename': '2510v02308v1.pdf', 'pages': 54, 'text': 'ROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION\\n\\n5 2 0 2 t c O 2 ] G L . s c [ 1 v 8 0 3 2 0 . 0 1 5 2 : v i X r a\\n\\nDHRUV KOHLI∗†, SAWYER J. ROBERTSON∗‡, GAL MISHNE§, ALEXANDER CLONINGER‡,§\\n\\nAbstract. Estimating the tangent spaces of a data manifold is a fundamental problem in data analysis. The standard approach, Local Principal Component Analysis (LPCA), struggles in high-noise settings due to a critical trade-off in choosing the neighborhood size. Selecting an optimal size requires prior knowledge of the geometric and noise characteristics of the data that are often unavailable. In this paper, we propose a spectral method, Laplacian Eigenvector Gradient Orthogonalization (LEGO), that utilizes the global structure of the data to guide local tangent space estimation. Instead of relying solely on local neighborhoods, LEGO estimates the tangent space at each data point by orthogonalizing the gradients of low-frequency eigenvectors of the graph Laplacian. We provide two theoretical justifications of our method. First, a differential geometric analysis on a tubular neighborhood of a manifold shows that gradients of the low-frequency Laplacian eigenfunctions of the tube align closely with the manifold’s tangent bundle, while an eigenfunction with high gradient in directions orthogonal to the manifold lie deeper in the spectrum. Second, a random matrix theoretic analysis also demonstrates that low-frequency eigenvectors are robust to sub-Gaussian noise. Through comprehensive experiments, we demonstrate that LEGO yields tangent space estimates that are significantly more robust to noise than those from LPCA, resulting in marked improvements in downstream tasks such as manifold learning, boundary detection, and local intrinsic dimension estimation.\\n\\nKeywords. Tangent space estimation | graph Laplacian | tubular neighborhood | manifold learning | dimensionality reduction | dimensionality estimation.\\n\\n1. Introduction\\n\\nTangent space estimation is a fundamental geometric task with broad applications across numerous domains, including manifold learning [1, 2, 3, 4, 5, 6, 7], data denoising [8], multi-manifold structure learning [9, 10, 11, 12], local intrinsic dimension estimation [13], connection Laplacian approximation [14, 15, 16], and regression on manifolds [17], among others. The standard procedure for estimating the tangent space at a given point x of a data manifold comprises of two steps: (i) determining the local intrinsic dimension d if not known a priori, and (ii) identifying d orthogonal directions in the ambient space that estimate a basis for the tangent space at x.\\n\\nThe most commonly adopted approach for tangent space estimation is Local Principal Component Analysis (LPCA) [1, 2, 3, 5, 8, 15, 17], which constructs the local covariance matrix using the knn-nearest neighbors of a data point and extracts the leading d eigenvectors as an estimate of the tangent basis at that point. When the local intrinsic dimension d is unknown, it is often inferred by counting the smallest number of top eigenvalues whose normalized cumulative sum (explained variance ratio) exceeds a user-defined threshold.\\n\\nDue to its local formulation, LPCA offers a straightforward and computationally efficient approach for tangent space estimation. However, a limitation of this local nature is that the presence of noise can significantly degrade the quality of the estimated tangent spaces as demonstrated in Figure 1a and 1b. Specifically, there is a well-known trade-off in the choice of neighborhood size: small neighborhoods are prone to noise corruption, while larger neighborhoods introduce bias due to the underlying curvature and reach of the manifold [18, 19, 20]. One potential strategy to address this limitation involves selecting\\n\\n∗equal contribution\\n\\n†Program in Applied and Computational Mathematics, Princeton University (dhkohli@ucsd.edu)\\n\\n‡Department of Mathematics, UC San Diego (s5robert@ucsd.edu, acloninger@ucsd.edu)\\n\\n§Halicio˘glu Data Science Institute, UC San Diego (gmishne@ucsd.edu)\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION\\n\\n5 2 0 2 t c O 2 ] G L . s c [ 1 v 8 0 3 2 0 . 0 1 5 2 : v i X r a\\n\\nDHRUV KOHLI∗†, SAWYER J. ROBERTSON∗‡, GAL MISHNE§, ALEXANDER CLONINGER‡,§\\n\\nAbstract. Estimating the tangent spaces of a data manifold is a fundamental problem in data analysis. The standard approach, Local Principal Component Analysis (LPCA), struggles in high-noise settings due to a critical trade-off in choosing the neighborhood size. Selecting an optimal size requires prior knowledge of the geometric and noise characteristics of the data that are often unavailable. In this paper, we propose a spectral method, Laplacian Eigenvector Gradient Orthogonalization (LEGO), that utilizes the global structure of the data to guide local tangent space estimation. Instead of relying solely on local neighborhoods, LEGO estimates the tangent space at each data point by orthogonalizing the gradients of low-frequency eigenvectors of the graph Laplacian. We provide two theoretical justifications of our method. First, a differential geometric analysis on a tubular neighborhood of a manifold shows that gradients of the low-frequency Laplacian eigenfunctions of the tube align closely with the manifold’s tangent bundle, while an eigenfunction with high gradient in directions orthogonal to the manifold lie deeper in the spectrum. Second, a random matrix theoretic analysis also demonstrates that low-frequency eigenvectors are robust to sub-Gaussian noise. Through comprehensive experiments, we demonstrate that LEGO yields tangent space estimates that are significantly more robust to noise than those from LPCA, resulting in marked improvements in downstream tasks such as manifold learning, boundary detection, and local intrinsic dimension estimation.\\n\\nKeywords. Tangent space estimation | graph Laplacian | tubular neighborhood | manifold learning | dimensionality reduction | dimensionality estimation.\\n\\n1. Introduction\\n\\nTangent space estimation is a fundamental geometric task with broad applications across numerous domains, including manifold learning [1, 2, 3, 4, 5, 6, 7], data denoising [8], multi-manifold structure learning [9, 10, 11, 12], local intrinsic dimension estimation [13], connection Laplacian approximation [14, 15, 16], and regression on manifolds [17], among others. The standard procedure for estimating the tangent space at a given point x of a data manifold comprises of two steps: (i) determining the local intrinsic dimension d if not known a priori, and (ii) identifying d orthogonal directions in the ambient space that estimate a basis for the tangent space at x.\\n\\nThe most commonly adopted approach for tangent space estimation is Local Principal Component Analysis (LPCA) [1, 2, 3, 5, 8, 15, 17], which constructs the local covariance matrix using the knn-nearest neighbors of a data point and extracts the leading d eigenvectors as an estimate of the tangent basis at that point. When the local intrinsic dimension d is unknown, it is often inferred by counting the smallest number of top eigenvalues whose normalized cumulative sum (explained variance ratio) exceeds a user-defined threshold.\\n\\nDue to its local formulation, LPCA offers a straightforward and computationally efficient approach for tangent space estimation. However, a limitation of this local nature is that the presence of noise can significantly degrade the quality of the estimated tangent spaces as demonstrated in Figure 1a and 1b. Specifically, there is a well-known trade-off in the choice of neighborhood size: small neighborhoods are prone to noise corruption, while larger neighborhoods introduce bias due to the underlying curvature and reach of the manifold [18, 19, 20]. One potential strategy to address this limitation involves selecting\\n\\n∗equal contribution\\n\\n†Program in Applied and Computational Mathematics, Princeton University (dhkohli@ucsd.edu)\\n\\n‡Department of Mathematics, UC San Diego (s5robert@ucsd.edu, acloninger@ucsd.edu)\\n\\n§Halicio˘glu Data Science Institute, UC San Diego (gmishne@ucsd.edu)\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\n(a) Tangent vector estimates from noisy data (visualized on noiseless data)\\n\\nGround truth\\n\\nLPCA (kn = 14)\\n\\nLEGO ( = 14)\\n\\n(b) Discrepancy between estimated and true tangent vectors\\n\\nEigenvectors with high gradients along normal noise direction lie deeper into the spectrum\\n\\n©\\n\\nFigure 1. Illustration of tangent space estimation using LPCA and LEGO on a noisy point cloud generated by non-uniform sampling of a closed curve—wave on a circle— with heteroskedastic noise added in the normal direction. (a) Clean data points with ground truth tangent vectors, along with tangent vectors estimated from the noisy data using LPCA (knn = 14 and d = 1) and LEGO (knn = 14, m0 = 20, m = 100 and d = 1). (b) Cosine dissimilarity between the true and the estimated tangent vectors. (c) Eigenvectors of the graph Laplacian constructed from noisy data [22], highlighting that those exhibiting high gradient in the noise direction lie deeper into the spectrum.\\n\\nan adaptive neighborhood size [19] that balances these competing effects. Nonetheless, the practical implementation of such adaptive schemes is hindered by the fact that the geometric quantities—curvature, reach, and the noise level, are typically unknown. As a result, selecting an appropriate neighborhood size becomes a challenging and often ill-posed problem.\\n\\nIn contrast, taking cues from the global structure of the data may offer an alternative route to robust tangent space estimation, avoiding the complexities of adaptive neighborhood sizes while allowing them to remain small. This perspective is widely adopted in literature [21, 22, 23, 24] where the eigenmodes of the graph Laplacian are frequently used to encode the global geometry of data. This naturally raises the question of whether such global eigenmodes can also be leveraged to inform local geometric structure, and in particular, to improve tangent space estimation.\\n\\nIn the continuous setting, such a connection is well established by Jones et al. in [25], showing that under suitable regularity assumptions, for a given point x on a d-dimensional Riemannian manifold [26], there exist d eigenfunctions of the manifold Laplacian which yield a bilipschitz local parameterization of a sufficiently small neighborhood of x into Rd. Building on this, in [4], we introduced an algorithm, Low Distortion Local Eigenmaps (LDLE), which realizes their result in practice by constructing local parameterizations using global eigenvectors of the graph Laplacian. Unlike traditional approaches that rely on the first d non-trivial eigenvectors [21, 22], LDLE selects customized subsets of d-eigenvectors for each neighborhood to construct their parameterizations into Rd. These parameterizations typically have low distortion, ensuring their Jacobians are full rank and span the d-dimensional tangent spaces.\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\n(a) Tangent vector estimates from noisy data (visualized on noiseless data)\\n\\nGround truth\\n\\nLPCA (kn = 14)\\n\\nLEGO ( = 14)\\n\\n(b) Discrepancy between estimated and true tangent vectors\\n\\nEigenvectors with high gradients along normal noise direction lie deeper into the spectrum\\n\\n©\\n\\nFigure 1. Illustration of tangent space estimation using LPCA and LEGO on a noisy point cloud generated by non-uniform sampling of a closed curve—wave on a circle— with heteroskedastic noise added in the normal direction. (a) Clean data points with ground truth tangent vectors, along with tangent vectors estimated from the noisy data using LPCA (knn = 14 and d = 1) and LEGO (knn = 14, m0 = 20, m = 100 and d = 1). (b) Cosine dissimilarity between the true and the estimated tangent vectors. (c) Eigenvectors of the graph Laplacian constructed from noisy data [22], highlighting that those exhibiting high gradient in the noise direction lie deeper into the spectrum.\\n\\nan adaptive neighborhood size [19] that balances these competing effects. Nonetheless, the practical implementation of such adaptive schemes is hindered by the fact that the geometric quantities—curvature, reach, and the noise level, are typically unknown. As a result, selecting an appropriate neighborhood size becomes a challenging and often ill-posed problem.\\n\\nIn contrast, taking cues from the global structure of the data may offer an alternative route to robust tangent space estimation, avoiding the complexities of adaptive neighborhood sizes while allowing them to remain small. This perspective is widely adopted in literature [21, 22, 23, 24] where the eigenmodes of the graph Laplacian are frequently used to encode the global geometry of data. This naturally raises the question of whether such global eigenmodes can also be leveraged to inform local geometric structure, and in particular, to improve tangent space estimation.\\n\\nIn the continuous setting, such a connection is well established by Jones et al. in [25], showing that under suitable regularity assumptions, for a given point x on a d-dimensional Riemannian manifold [26], there exist d eigenfunctions of the manifold Laplacian which yield a bilipschitz local parameterization of a sufficiently small neighborhood of x into Rd. Building on this, in [4], we introduced an algorithm, Low Distortion Local Eigenmaps (LDLE), which realizes their result in practice by constructing local parameterizations using global eigenvectors of the graph Laplacian. Unlike traditional approaches that rely on the first d non-trivial eigenvectors [21, 22], LDLE selects customized subsets of d-eigenvectors for each neighborhood to construct their parameterizations into Rd. These parameterizations typically have low distortion, ensuring their Jacobians are full rank and span the d-dimensional tangent spaces.\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 3\\n\\nThis provides empirical and theoretical support for using gradients of graph Laplacian eigenvectors to estimate local tangent spaces on data manifolds. However, in the presence of noise, these eigenvectors may still exhibit non-zero gradients in directions orthogonal to the manifold, causing them to acquire components in the noise directions [27, 28, 29, 30, 31] and consequently distorting the tangent space estimates.\\n\\nFortunately, a principle analogous to classical Fourier analysis applies: just as the low-frequency Fourier modes capture the underlying signal while high-frequency modes tend to encode noise [32, 33, 34], it is commonly observed that the eigenvectors corresponding to small eigenvalues of the graph Laplacian are robust to noise while the ones lying deeper into the spectrum may have nontrivial gradient in the noise directions [27, 28, 29, 30, 31], as demonstrated in Figure 1c. Building upon this insight, in this work, we propose an algorithm that estimates the tangent spaces at data points using the gradients of low- frequency global eigenvectors of the graph Laplacian. Moreover, we provide differential geometric and random matrix theoretic arguments to support our approach.\\n\\nOur contributions are as follows. We present a spectral algorithm, LEGO (Laplacian Eigenvector Gradi- ent Orthogonalization), for estimating tangent spaces at each data point by orthogonalizing the gradients of low-frequency global eigenvectors of the graph Laplacian derived from a noisy point cloud. Through comprehensive experiments we show that LEGO yields tangent space estimates that are significantly more robust to noise than those obtained via LPCA. We also demonstrate that this increased robustness results in significant improvements across multiple downstream tasks, including manifold learning [1, 2, 35, 3, 5, 36], boundary detection [37, 38, 39], and local intrinsic dimension estimation [13].\\n\\nOn the theoretical front, we offer two complementary justifications for our approach. First, we provide a differential geometric argument where we adopt a noise model in which clean data points lie on a d- dimensional smooth submanifold B in Rd+k, while the noisy observations lie in a tubular neighborhood T εr around B, where r is any number bounded by the global reach of B [40, 41] and ε ∈ (0,1) controls the tube width. In practice, r represents the maximum noise and and ε is a parameter that controls the noise level. In particular, we assume that noise perturbs data in directions normal to B. In this setting, the “horizontal space” at a noisy point—the subspace orthogonal to the noise direction—approximates the true tangent space of the corresponding clean point. Thus, estimating the horizontal space provides a principled approach to robust tangent space estimation. To formalize this approach, we study the eigenfunctions of the Laplacian on the tubular neighborhood T εr of B.\\n\\nSpecifically, we establish upper and lower bounds on the eigenvalue λ associated with an eigenfunction ϕ in terms of its horizontal and vertical energies, EB(ϕ) and E⊥ B(ϕ), which quantify the net gradient of ϕ across the horizontal spaces and the noise directions, respectively. These bounds indicate that for small ε, λ approximately scales as Ω(ε−2E⊥ B(ϕ)) with respect to the vertical energy of ϕ, and as O(EB(ϕ)) with respect to the horizontal energy. As a result, eigenfunctions with significant gradient across tube cross- sections necessarily correspond to large eigenvalues. Conversely, gradients of eigenfunctions associated with small eigenvalues are largely confined to the horizontal spaces. The practical implication is that the gradients of the low-frequency eigenvectors of the graph Laplacian, constructed from noisy data, exhibit relatively small components along noise directions, and thus their orthogonalization may provide a robust estimate of the tangent spaces.\\n\\nSecond, we provide a random matrix theoretic argument where we adopt an information plus noise- type model in which the clean data is injected with sub-Gaussian noise with variance proxy ε. Our data model is reminiscent of that presented in [42], with a key distinction being that our data dimension is held constant while ε = o(1) in the limit as the number of data points tends to infinity. This model generalizes from the case where the clean data lies on an embedded submanifold and the noise is confined to directions orthogonal to the tangent spaces. Using a Gaussian kernel with fixed bandwidth s, we construct the adjacency matrices and derive the corresponding random walk graph Laplacians, L for the clean data and L for the noisy data [43, 44].\\n\\nBy adapting techniques from the random graph literature (see, e.g., [45]), assuming that the variance √ proxy ε scales as O(1/ nlogn), we prove that the noisy Laplacian L converges to its clean counterpart L in operator norm at a rate of n−1/2 i.e., ∥L−L∥2 = O(n−1/2) with high probability. By the Davis-Kahan theorem [46], it follows that the eigenvectors of L remain close to those of L, provided the eigengaps of\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 3\\n\\nThis provides empirical and theoretical support for using gradients of graph Laplacian eigenvectors to estimate local tangent spaces on data manifolds. However, in the presence of noise, these eigenvectors may still exhibit non-zero gradients in directions orthogonal to the manifold, causing them to acquire components in the noise directions [27, 28, 29, 30, 31] and consequently distorting the tangent space estimates.\\n\\nFortunately, a principle analogous to classical Fourier analysis applies: just as the low-frequency Fourier modes capture the underlying signal while high-frequency modes tend to encode noise [32, 33, 34], it is commonly observed that the eigenvectors corresponding to small eigenvalues of the graph Laplacian are robust to noise while the ones lying deeper into the spectrum may have nontrivial gradient in the noise directions [27, 28, 29, 30, 31], as demonstrated in Figure 1c. Building upon this insight, in this work, we propose an algorithm that estimates the tangent spaces at data points using the gradients of low- frequency global eigenvectors of the graph Laplacian. Moreover, we provide differential geometric and random matrix theoretic arguments to support our approach.\\n\\nOur contributions are as follows. We present a spectral algorithm, LEGO (Laplacian Eigenvector Gradi- ent Orthogonalization), for estimating tangent spaces at each data point by orthogonalizing the gradients of low-frequency global eigenvectors of the graph Laplacian derived from a noisy point cloud. Through comprehensive experiments we show that LEGO yields tangent space estimates that are significantly more robust to noise than those obtained via LPCA. We also demonstrate that this increased robustness results in significant improvements across multiple downstream tasks, including manifold learning [1, 2, 35, 3, 5, 36], boundary detection [37, 38, 39], and local intrinsic dimension estimation [13].\\n\\nOn the theoretical front, we offer two complementary justifications for our approach. First, we provide a differential geometric argument where we adopt a noise model in which clean data points lie on a d- dimensional smooth submanifold B in Rd+k, while the noisy observations lie in a tubular neighborhood T εr around B, where r is any number bounded by the global reach of B [40, 41] and ε ∈ (0,1) controls the tube width. In practice, r represents the maximum noise and and ε is a parameter that controls the noise level. In particular, we assume that noise perturbs data in directions normal to B. In this setting, the “horizontal space” at a noisy point—the subspace orthogonal to the noise direction—approximates the true tangent space of the corresponding clean point. Thus, estimating the horizontal space provides a principled approach to robust tangent space estimation. To formalize this approach, we study the eigenfunctions of the Laplacian on the tubular neighborhood T εr of B.\\n\\nSpecifically, we establish upper and lower bounds on the eigenvalue λ associated with an eigenfunction ϕ in terms of its horizontal and vertical energies, EB(ϕ) and E⊥ B(ϕ), which quantify the net gradient of ϕ across the horizontal spaces and the noise directions, respectively. These bounds indicate that for small ε, λ approximately scales as Ω(ε−2E⊥ B(ϕ)) with respect to the vertical energy of ϕ, and as O(EB(ϕ)) with respect to the horizontal energy. As a result, eigenfunctions with significant gradient across tube cross- sections necessarily correspond to large eigenvalues. Conversely, gradients of eigenfunctions associated with small eigenvalues are largely confined to the horizontal spaces. The practical implication is that the gradients of the low-frequency eigenvectors of the graph Laplacian, constructed from noisy data, exhibit relatively small components along noise directions, and thus their orthogonalization may provide a robust estimate of the tangent spaces.\\n\\nSecond, we provide a random matrix theoretic argument where we adopt an information plus noise- type model in which the clean data is injected with sub-Gaussian noise with variance proxy ε. Our data model is reminiscent of that presented in [42], with a key distinction being that our data dimension is held constant while ε = o(1) in the limit as the number of data points tends to infinity. This model generalizes from the case where the clean data lies on an embedded submanifold and the noise is confined to directions orthogonal to the tangent spaces. Using a Gaussian kernel with fixed bandwidth s, we construct the adjacency matrices and derive the corresponding random walk graph Laplacians, L for the clean data and L for the noisy data [43, 44].\\n\\nBy adapting techniques from the random graph literature (see, e.g., [45]), assuming that the variance √ proxy ε scales as O(1/ nlogn), we prove that the noisy Laplacian L converges to its clean counterpart L in operator norm at a rate of n−1/2 i.e., ∥L−L∥2 = O(n−1/2) with high probability. By the Davis-Kahan theorem [46], it follows that the eigenvectors of L remain close to those of L, provided the eigengaps of\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nthe clean Laplacian do not decay too rapidly with n. In the submanifold setting, classical spectral results such as Weyl’s law (e.g., see [47]) imply that eigengaps tend to shrink deeper in the spectrum. This, when combined with our result, suggest that low-frequency eigenvectors are more stable to noise and therefore their gradients can be reliably used for tangent space estimation.\\n\\nThe remainder of the paper is organized as follows. In Section 2, we introduce our proposed algo- rithm, LEGO, for tangent space estimation. Theoretical justifications for our method are provided in two parts: a differential geometric perspective in Section 3, and a random matrix theoretic analysis in Section 4. In Section 5, we demonstrate the effectiveness of LEGO through experiments on multiple datasets, highlighting its improved accuracy over LPCA and its benefits for downstream tasks.\\n\\n2. Tangent space estimation via gradients of low-frequency global eigenvectors of graph Laplacian\\n\\nHere, we introduce our algorithm, LEGO, for estimating tangent spaces at noise-perturbed data points that are sampled from a tubular neighborhood of a smooth embedded submanifold. Specifically, we assume that the clean data points lie on the submanifold while the noise is constrained to the subspaces orthogonal to their tangent spaces. Our approach estimates orthonormal bases of the tangent spaces at the clean points by orthogonalizing the gradients of low-frequency global eigenvectors of the graph Laplacian constructed from the noisy data.\\n\\nLet Y = [Y1,...,Yn] ∈ Rp×n be a point cloud sampled from a smooth compact d-dimensional subman- ifold B embedded in Rp. Let X = [X1,...,Xn] ∈ Rp×n be the noisy point cloud such that Xi is obtained by adding noise to Yi in the directions orthogonal to the tangent space TYiB. Let Nj = {j1,...,jknn} be a set containing the indices of the knn-nearest neighbors of Xj obtained using the Euclidean metric in Rp. Let L ∈ Rn×n be the graph Laplacian constructed from X using one of the following kernel-based methods: the random walk kernel [22, 43], the self-tuned kernel [48, 49] or the doubly stochastic ker- nel [50, 51]. These construction strategies ensure that, under appropriate scaling of the kernel bandwidth and sampling density, the discrete operator L converges with high probability to the Laplace–Beltrami operator ∆δp on a tubular neighborhood of the submanifold B [52, 43, 53, 54, 49, 55]. Moreover, recent results [49, 54] establish the convergence of the spectrum of L to that of ∆δp under technical conditions on the sampling density, manifold geometry and the kernel bandwidth.\\n\\nLet ϕi ∈ Rn be the i-th eigenvector of L corresponding to the i-th smallest eigenvalue, and ∇ϕi ∈ Rp×n be a matrix whose j-th column, ∇ϕi(Xj) ∈ Rp, represents the gradient of ϕi at Xj. Each of the p components of the gradient ∇ϕi is treated a smooth function on X, and thus modeled as a vector in the span of the eigenvectors of L. Given that eigenvectors corresponding to higher eigenvalues are more susceptible to noise (see, e.g., [24, 31]), we estimate ∇ϕi using only the first m0 ≪ n eigenvectors {ϕ1,...,ϕm0} of L. To ensure local fidelity, we require the estimated gradient to approximate ϕi, up to first order, on the neighborhood {Xjs : js ∈ Nj} of each point Xj. Precisely, define centered data points and eigenvectors as,\\n\\n(1)\\n\\nrespectively, where are the ky-nearest neighbors of X;. Then, the estimate € RP™ of the gradients V¢; is given by,\\n\\n(2)\\n\\nwhere Up € R\"*™ constitutes an orthonormal basis of the range of ® = b2 c Rmo and is the solution of the following optimization problem,\\n\\n(3)\\n\\n(4)\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nthe clean Laplacian do not decay too rapidly with n. In the submanifold setting, classical spectral results such as Weyl’s law (e.g., see [47]) imply that eigengaps tend to shrink deeper in the spectrum. This, when combined with our result, suggest that low-frequency eigenvectors are more stable to noise and therefore their gradients can be reliably used for tangent space estimation.\\n\\nThe remainder of the paper is organized as follows. In Section 2, we introduce our proposed algo- rithm, LEGO, for tangent space estimation. Theoretical justifications for our method are provided in two parts: a differential geometric perspective in Section 3, and a random matrix theoretic analysis in Section 4. In Section 5, we demonstrate the effectiveness of LEGO through experiments on multiple datasets, highlighting its improved accuracy over LPCA and its benefits for downstream tasks.\\n\\n2. Tangent space estimation via gradients of low-frequency global eigenvectors of graph Laplacian\\n\\nHere, we introduce our algorithm, LEGO, for estimating tangent spaces at noise-perturbed data points that are sampled from a tubular neighborhood of a smooth embedded submanifold. Specifically, we assume that the clean data points lie on the submanifold while the noise is constrained to the subspaces orthogonal to their tangent spaces. Our approach estimates orthonormal bases of the tangent spaces at the clean points by orthogonalizing the gradients of low-frequency global eigenvectors of the graph Laplacian constructed from the noisy data.\\n\\nLet Y = [Y1,...,Yn] ∈ Rp×n be a point cloud sampled from a smooth compact d-dimensional subman- ifold B embedded in Rp. Let X = [X1,...,Xn] ∈ Rp×n be the noisy point cloud such that Xi is obtained by adding noise to Yi in the directions orthogonal to the tangent space TYiB. Let Nj = {j1,...,jknn} be a set containing the indices of the knn-nearest neighbors of Xj obtained using the Euclidean metric in Rp. Let L ∈ Rn×n be the graph Laplacian constructed from X using one of the following kernel-based methods: the random walk kernel [22, 43], the self-tuned kernel [48, 49] or the doubly stochastic ker- nel [50, 51]. These construction strategies ensure that, under appropriate scaling of the kernel bandwidth and sampling density, the discrete operator L converges with high probability to the Laplace–Beltrami operator ∆δp on a tubular neighborhood of the submanifold B [52, 43, 53, 54, 49, 55]. Moreover, recent results [49, 54] establish the convergence of the spectrum of L to that of ∆δp under technical conditions on the sampling density, manifold geometry and the kernel bandwidth.\\n\\nLet ϕi ∈ Rn be the i-th eigenvector of L corresponding to the i-th smallest eigenvalue, and ∇ϕi ∈ Rp×n be a matrix whose j-th column, ∇ϕi(Xj) ∈ Rp, represents the gradient of ϕi at Xj. Each of the p components of the gradient ∇ϕi is treated a smooth function on X, and thus modeled as a vector in the span of the eigenvectors of L. Given that eigenvectors corresponding to higher eigenvalues are more susceptible to noise (see, e.g., [24, 31]), we estimate ∇ϕi using only the first m0 ≪ n eigenvectors {ϕ1,...,ϕm0} of L. To ensure local fidelity, we require the estimated gradient to approximate ϕi, up to first order, on the neighborhood {Xjs : js ∈ Nj} of each point Xj. Precisely, define centered data points and eigenvectors as,\\n\\n(1)\\n\\nrespectively, where are the ky-nearest neighbors of X;. Then, the estimate € RP™ of the gradients V¢; is given by,\\n\\n(2)\\n\\nwhere Up € R\"*™ constitutes an orthonormal basis of the range of ® = b2 c Rmo and is the solution of the following optimization problem,\\n\\n(3)\\n\\n(4)\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 5\\n\\nUsing the fact that UΦ has orthonormal columns i.e. UΦ TUΦ = Im0, the least squares solution of the above optimization problem is given by,\\n\\n(5)\\n\\nwhere X † i is the pseudoinverse of Xi. Having obtained the gradient estimates of the eigenvectors {ϕ1,...,ϕm} at Xj given by,\\n\\n(6)\\n\\nwe obtain an estimate of the orthonormal basis Q; € RP*4 of the d-dimensional tangent space at the jth point by orthogonalizing (X and equivalently, using the top d left singular vectors of the matrix If the intrinsic dimension d is not known a priori then one can estimate it by selecting the smallest number of top eigenvalues of the matrix whose normalized cumulative sum exceeds a user-defined threshold fy,;, as demonstrated in the pseudocode below.\\n\\nAlgorithm 1 Tangent space estimation via LEGO. Require: X € R\"*?, L, ky,, m and m where m < my, either d or f,, € (0,1). 1: ¢1,. s Pmg eigenvectors of £ corresponding to mg smallest eigenvalues 2: Estimate V¢; using Eq. 2 and 5 for € [1,m] 3: for j € [1,n] do IS Set V¢(X;) as in Eq. 6. U e RP*P gy > ... > 0, Left singular vectors and values of if d is provided then Q; « first d columns of U else © ® s 2 dj < min{s € [1,p] : > i=17 i /301 0 2 ar} 10 Q j < first d; columns of U return { Qi\\n\\n2.1. Time complexity. Assuming that the local intrinsic dimension d is known a priori, the cost of estimating tangent space at each point i.e., computing the top d principal directions from the local neigh- borhood using LPCA is O(kupd). Therefore, the total time complexity of applying LPCA to all n points is O(nknnpd). In contrast, the time complexity of each stage of LEGO is as follows: (i) computing eigenvectors ¢, ., m of the graph Laplacian £ (Line 1 in Algorithm 1) using an iterative eigensolver requires O(nky,moT) time where is the number of iterations required for convergence [56]. () Com- puting orthonormal basis Ug of the eigenvectors ® requires O(nmg?) time. (iii) Estimating the gradients Vé for all i € [1,m] (Line 2 in Algorithm 1) requires O(nkunp(min{kon, p} + m)) + O(nmomp) where the first term exclusively corresponds to the estimation of C; in Eq. 5 and the second term corresponds to multiplication by ® on the right side in Eq. 5 and by ®7 in Eq. 2. (iv) Computing the orthonormal basis @ for all j € [1,n] using the gradients of eigenvectors takes O(nmpd). Overall, the total time complexity of LPCA is linear in all the parameters while LEGO is quadratic in min{ky,p}. In practice, the computation of eigenvectors ¢y m, seem to dominate the computational cost of LEGO.\\n\\n2.2. Selection of hyperparameters for LEGO. A typical choice for knn is on the order O(log(n)) or O(nα) where α ∈ (1/d,1). In practice, knn is selected to be sufficiently small to avoid spurious edges or “shortcuts” in the nearest neighbor graph that could distort the underlying local geometry. Since the appropriate value of knn is usually unknown, a common approach is to begin with a small value and gradually increase it until the output reaches the desired quality.\\n\\nAs we argue in Section 3, the number of eigenvectors m whose gradients are orthgonalized should be kept relatively small, so as to avoid eigenvectors deeper in the spectrum that may exhibit large gradients in the noise directions [27, 28, 29]. We demonstrate in our experiments that this does not require fine-tuning\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 5\\n\\nUsing the fact that UΦ has orthonormal columns i.e. UΦ TUΦ = Im0, the least squares solution of the above optimization problem is given by,\\n\\n(5)\\n\\nwhere X † i is the pseudoinverse of Xi. Having obtained the gradient estimates of the eigenvectors {ϕ1,...,ϕm} at Xj given by,\\n\\n(6)\\n\\nwe obtain an estimate of the orthonormal basis Q; € RP*4 of the d-dimensional tangent space at the jth point by orthogonalizing (X and equivalently, using the top d left singular vectors of the matrix If the intrinsic dimension d is not known a priori then one can estimate it by selecting the smallest number of top eigenvalues of the matrix whose normalized cumulative sum exceeds a user-defined threshold fy,;, as demonstrated in the pseudocode below.\\n\\nAlgorithm 1 Tangent space estimation via LEGO. Require: X € R\"*?, L, ky,, m and m where m < my, either d or f,, € (0,1). 1: ¢1,. s Pmg eigenvectors of £ corresponding to mg smallest eigenvalues 2: Estimate V¢; using Eq. 2 and 5 for € [1,m] 3: for j € [1,n] do IS Set V¢(X;) as in Eq. 6. U e RP*P gy > ... > 0, Left singular vectors and values of if d is provided then Q; « first d columns of U else © ® s 2 dj < min{s € [1,p] : > i=17 i /301 0 2 ar} 10 Q j < first d; columns of U return { Qi\\n\\n2.1. Time complexity. Assuming that the local intrinsic dimension d is known a priori, the cost of estimating tangent space at each point i.e., computing the top d principal directions from the local neigh- borhood using LPCA is O(kupd). Therefore, the total time complexity of applying LPCA to all n points is O(nknnpd). In contrast, the time complexity of each stage of LEGO is as follows: (i) computing eigenvectors ¢, ., m of the graph Laplacian £ (Line 1 in Algorithm 1) using an iterative eigensolver requires O(nky,moT) time where is the number of iterations required for convergence [56]. () Com- puting orthonormal basis Ug of the eigenvectors ® requires O(nmg?) time. (iii) Estimating the gradients Vé for all i € [1,m] (Line 2 in Algorithm 1) requires O(nkunp(min{kon, p} + m)) + O(nmomp) where the first term exclusively corresponds to the estimation of C; in Eq. 5 and the second term corresponds to multiplication by ® on the right side in Eq. 5 and by ®7 in Eq. 2. (iv) Computing the orthonormal basis @ for all j € [1,n] using the gradients of eigenvectors takes O(nmpd). Overall, the total time complexity of LPCA is linear in all the parameters while LEGO is quadratic in min{ky,p}. In practice, the computation of eigenvectors ¢y m, seem to dominate the computational cost of LEGO.\\n\\n2.2. Selection of hyperparameters for LEGO. A typical choice for knn is on the order O(log(n)) or O(nα) where α ∈ (1/d,1). In practice, knn is selected to be sufficiently small to avoid spurious edges or “shortcuts” in the nearest neighbor graph that could distort the underlying local geometry. Since the appropriate value of knn is usually unknown, a common approach is to begin with a small value and gradually increase it until the output reaches the desired quality.\\n\\nAs we argue in Section 3, the number of eigenvectors m whose gradients are orthgonalized should be kept relatively small, so as to avoid eigenvectors deeper in the spectrum that may exhibit large gradients in the noise directions [27, 28, 29]. We demonstrate in our experiments that this does not require fine-tuning\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nas the tangent space estimates remain stable across a wide range of values of m. Lastly, m0, the number of eigenvectors used to model the gradients must be sufficiently large to ensure that each component of the gradient—viewed as a smooth function on the manifold—is well-approximated within the span of the first m0 eigenvectors [57, 44].\\n\\n3. Eigenfunctions with high gradient along the cross sections of a tube lie deeper into the spectrum\\n\\nNoisy data is often modeled as a sample drawn from a tubular neighborhood surrounding an underlying smooth submanifold [41, 58, 59, 60, 61, 62]. Under this noise model, the graph Laplacian constructed from such data [22, 49, 55] converges to the continuous Laplacian of the tubular neighborhood. This motivates the study of the eigenfunctions of the Laplacian on the tube to better understand the behavior of the graph Laplacian eigenvectors derived from noisy data. Here, building on [63], we show that eigenfunctions exhibiting high gradient across the cross sections of the tubular neighborhood necessarily correspond to higher eigenvalues. Consequently, eigenfunctions associated with low eigenvalues exhibit minimal gradient in directions normal to the submanifold. The practical implication of our result is that the gradients of the low-frequency eigenvectors of the graph Laplacian tend to have small components in the noise directions, making them suitable for tangent space estimation. The proofs of our results are provided in the appendix.\\n\\n3.1. Preliminaries. In the following we describe the necessary constructs from [63] that are needed for our results. Let B ⊂ Rd+k be a smooth embedded compact d-dimensional submanifold with or without boundary, equipped with the metric gB induced by the Euclidean metric δd+k. Let NB be the normal bundle of B equipped with the metric g⊥ B = δd+k|NB. Assume that there exist a tubular neighborhood T r of B such that B ⊂ T r ⊂ Rd+k, where r is any finite number bounded by the global reach, meaning, it satisfies the property that that normals to B with length less than r do not intersect [41]. Define a map\\n\\nwhich, when restricted to\\n\\nis diffeomorphic to its image T r. Let π : NBr → B be the canonical projection π(x,ν) = x onto B. By equipping NBr with the pullback metric g = Ψ∗δd+k, the tubular neighborhood T r is isometric to NBr. This also holds for ε-tubular neighborhood T εr of B and the normal bundle NBεr for ε < 1. To keep the dependence on ε explicit, it is convenient to work with NBr with the pullback metric gε = D∗ the map Dε : NBr → NBεr is given by Dε(x,ν) = (x,εν). In fact, NBr equipped with gε is isometric εg where to T εr equipped with Euclidean metric δd+k. Due to this construction, the Laplacian −∆gε on NBr is unitarily equivalent to the Euclidean Laplacian ∆δd+k on T εr i.e. for functions ϕ ∈ C∞ 0 (NBr) it holds that\\n\\nwhere and ¥ are the unitary lifts associated with D, and ¥, respectively. Specifically, L3 (NB\",dVye) — L(NB®\",dV,) and 12 dVs,,,) = L*(NB®\",dV,) are given by,\\n\\nand\\n\\nTt follows that if ¢ is an eigenfunction of the Laplacian —A,,, on T°\" with eigenvalue A then is an eigenfunction of the Laplacian Age on NB\" with the same eigenvalue.\\n\\nIn addition to the metrics introduced above, the metric\\n\\nturns π∗ into a Riemannian submersion i.e. an isometry from the horizontal subbundle π∗(TB) to the tangent bundle TB. Let gs = gε=1 s be the ε-independent unscaled version of the submersion metric.\\n\\nUsing the unscaled submersion metric and a local coordinate system we define the horizontal energy of an eigenfunction ϕ on the tube T εr that captures the net gradient of ϕ along the submanifold B,\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nas the tangent space estimates remain stable across a wide range of values of m. Lastly, m0, the number of eigenvectors used to model the gradients must be sufficiently large to ensure that each component of the gradient—viewed as a smooth function on the manifold—is well-approximated within the span of the first m0 eigenvectors [57, 44].\\n\\n3. Eigenfunctions with high gradient along the cross sections of a tube lie deeper into the spectrum\\n\\nNoisy data is often modeled as a sample drawn from a tubular neighborhood surrounding an underlying smooth submanifold [41, 58, 59, 60, 61, 62]. Under this noise model, the graph Laplacian constructed from such data [22, 49, 55] converges to the continuous Laplacian of the tubular neighborhood. This motivates the study of the eigenfunctions of the Laplacian on the tube to better understand the behavior of the graph Laplacian eigenvectors derived from noisy data. Here, building on [63], we show that eigenfunctions exhibiting high gradient across the cross sections of the tubular neighborhood necessarily correspond to higher eigenvalues. Consequently, eigenfunctions associated with low eigenvalues exhibit minimal gradient in directions normal to the submanifold. The practical implication of our result is that the gradients of the low-frequency eigenvectors of the graph Laplacian tend to have small components in the noise directions, making them suitable for tangent space estimation. The proofs of our results are provided in the appendix.\\n\\n3.1. Preliminaries. In the following we describe the necessary constructs from [63] that are needed for our results. Let B ⊂ Rd+k be a smooth embedded compact d-dimensional submanifold with or without boundary, equipped with the metric gB induced by the Euclidean metric δd+k. Let NB be the normal bundle of B equipped with the metric g⊥ B = δd+k|NB. Assume that there exist a tubular neighborhood T r of B such that B ⊂ T r ⊂ Rd+k, where r is any finite number bounded by the global reach, meaning, it satisfies the property that that normals to B with length less than r do not intersect [41]. Define a map\\n\\nwhich, when restricted to\\n\\nis diffeomorphic to its image T r. Let π : NBr → B be the canonical projection π(x,ν) = x onto B. By equipping NBr with the pullback metric g = Ψ∗δd+k, the tubular neighborhood T r is isometric to NBr. This also holds for ε-tubular neighborhood T εr of B and the normal bundle NBεr for ε < 1. To keep the dependence on ε explicit, it is convenient to work with NBr with the pullback metric gε = D∗ the map Dε : NBr → NBεr is given by Dε(x,ν) = (x,εν). In fact, NBr equipped with gε is isometric εg where to T εr equipped with Euclidean metric δd+k. Due to this construction, the Laplacian −∆gε on NBr is unitarily equivalent to the Euclidean Laplacian ∆δd+k on T εr i.e. for functions ϕ ∈ C∞ 0 (NBr) it holds that\\n\\nwhere and ¥ are the unitary lifts associated with D, and ¥, respectively. Specifically, L3 (NB\",dVye) — L(NB®\",dV,) and 12 dVs,,,) = L*(NB®\",dV,) are given by,\\n\\nand\\n\\nTt follows that if ¢ is an eigenfunction of the Laplacian —A,,, on T°\" with eigenvalue A then is an eigenfunction of the Laplacian Age on NB\" with the same eigenvalue.\\n\\nIn addition to the metrics introduced above, the metric\\n\\nturns π∗ into a Riemannian submersion i.e. an isometry from the horizontal subbundle π∗(TB) to the tangent bundle TB. Let gs = gε=1 s be the ε-independent unscaled version of the submersion metric.\\n\\nUsing the unscaled submersion metric and a local coordinate system we define the horizontal energy of an eigenfunction ϕ on the tube T εr that captures the net gradient of ϕ along the submanifold B,\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 7\\n\\nand the vertical energy of ϕ that measures its net gradient normal to B i.e. across the cross sections of the tubular neighborhood T r. To this end, let x1,...,xd be the local coordinates on B and {eα}k 1 be a locally orthonormal frame of NBr with respect to g⊥ B such that every normal vector ν(x) ∈ NxB can be written as ν(x) = nαeα(x). Consequently, (x1,...,xd,n1,...,nk) form local coordinates of NBr and the associated local coordinate vector fields are,\\n\\n(7)\\n\\n0 (NBr), define its canonical gradients as For f ∈ C∞\\n\\n(8)\\n\\n(9)\\n\\nand Vf € R¥ is the concatenation of the two vectors. Then, € the horizontal energy Ep(¢) and the vertical energy E(¢) of ¢ are given by (here = 1 is the unitary lift of ¢ onto NB\")\\n\\n(10)\\n\\n(11)\\n\\nWe further define a normalized version of the horizontal and vertical energy of ϕ ∈ C∞ 0 (T εr) as\\n\\n(12)\\n\\nrespectively, where λB2 and Ck/r2 are the first non-zero eigenvalues of the Laplacian −∆gB −∆δk on a ball of radius r in Rk, respectively. Here, Ck is a constant that depends on the dimension k and on B and the choice of the boundary conditions—either Neumann or Dirichlet. Overall, the above normalizations ensure that EB(ϕ) and E⊥ B(ϕ) are on similar scale with respect to r and ε.\\n\\nTo further motivate the above definitions, consider the example where B [0,7] x {0} C R? is an interval of length [/, and its tubular neighborhood of radius r is given by a rectangular domain 7\" = NB\" [0,7] x [r] The Neumann eigenfunctions of the Laplacian As, = Ag = A on the scaled tube 7\" = NB\" are of the form ¢; j(x,n) = cos(iz/l) sin(jn/2er) with cigenvalues ij = (im /)% + + (jm/2er)? where i,j € Z. Lifting ¢; ; back to the unscaled tube NB\" gives ( x,n) = cos(imz /1) sin(jn/2r) from which we compute the unnormalized horizontal energy as Ep(¢;,; i/l) and the vertical energy as E(¢;;) = (jm/2r). Consequently, the normalized horizontal and vertical energies are given by Eg(¢; ;) = i* and E(; ;) = j respectively. In particular, the eigenvalue can be expressed as \\\\;; = Ap,Ep(¢i;) + Cr(er) E5(¢i ;) highlighting that, if r is small enough such that 2 Ck 2> A then, the contribution of the vertical energy to \\\\; ; scales as € relative to the horizontal energy. This means that a unit increase in the vertical energy of an eigenfunction results in a much larger increase in the eigenvalue for small e\\n\\nWe end this subsection by defining a few constructs that capture the geometry of B and are utilized in our results. Let Π be the second fundamental form on B then the coefficients of the scalar second fundamental form on B are given by,\\n\\n(13)\\n\\nLet ∇⊥ be the normal connection with respect to {eα}k 1 then the Christoffel symbols of the normal connection are given by,\\n\\n(14)\\n\\nDefine a symmetric matrix Hα(x) ∈ Rd×d such that for i,j ∈ [1,d],\\n\\n(15)\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 7\\n\\nand the vertical energy of ϕ that measures its net gradient normal to B i.e. across the cross sections of the tubular neighborhood T r. To this end, let x1,...,xd be the local coordinates on B and {eα}k 1 be a locally orthonormal frame of NBr with respect to g⊥ B such that every normal vector ν(x) ∈ NxB can be written as ν(x) = nαeα(x). Consequently, (x1,...,xd,n1,...,nk) form local coordinates of NBr and the associated local coordinate vector fields are,\\n\\n(7)\\n\\n0 (NBr), define its canonical gradients as For f ∈ C∞\\n\\n(8)\\n\\n(9)\\n\\nand Vf € R¥ is the concatenation of the two vectors. Then, € the horizontal energy Ep(¢) and the vertical energy E(¢) of ¢ are given by (here = 1 is the unitary lift of ¢ onto NB\")\\n\\n(10)\\n\\n(11)\\n\\nWe further define a normalized version of the horizontal and vertical energy of ϕ ∈ C∞ 0 (T εr) as\\n\\n(12)\\n\\nrespectively, where λB2 and Ck/r2 are the first non-zero eigenvalues of the Laplacian −∆gB −∆δk on a ball of radius r in Rk, respectively. Here, Ck is a constant that depends on the dimension k and on B and the choice of the boundary conditions—either Neumann or Dirichlet. Overall, the above normalizations ensure that EB(ϕ) and E⊥ B(ϕ) are on similar scale with respect to r and ε.\\n\\nTo further motivate the above definitions, consider the example where B [0,7] x {0} C R? is an interval of length [/, and its tubular neighborhood of radius r is given by a rectangular domain 7\" = NB\" [0,7] x [r] The Neumann eigenfunctions of the Laplacian As, = Ag = A on the scaled tube 7\" = NB\" are of the form ¢; j(x,n) = cos(iz/l) sin(jn/2er) with cigenvalues ij = (im /)% + + (jm/2er)? where i,j € Z. Lifting ¢; ; back to the unscaled tube NB\" gives ( x,n) = cos(imz /1) sin(jn/2r) from which we compute the unnormalized horizontal energy as Ep(¢;,; i/l) and the vertical energy as E(¢;;) = (jm/2r). Consequently, the normalized horizontal and vertical energies are given by Eg(¢; ;) = i* and E(; ;) = j respectively. In particular, the eigenvalue can be expressed as \\\\;; = Ap,Ep(¢i;) + Cr(er) E5(¢i ;) highlighting that, if r is small enough such that 2 Ck 2> A then, the contribution of the vertical energy to \\\\; ; scales as € relative to the horizontal energy. This means that a unit increase in the vertical energy of an eigenfunction results in a much larger increase in the eigenvalue for small e\\n\\nWe end this subsection by defining a few constructs that capture the geometry of B and are utilized in our results. Let Π be the second fundamental form on B then the coefficients of the scalar second fundamental form on B are given by,\\n\\n(13)\\n\\nLet ∇⊥ be the normal connection with respect to {eα}k 1 then the Christoffel symbols of the normal connection are given by,\\n\\n(14)\\n\\nDefine a symmetric matrix Hα(x) ∈ Rd×d such that for i,j ∈ [1,d],\\n\\n(15)\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nAlso define a rectangular matrix Γβ(x) ∈ Rd×k such that for i,j ∈ [1,d] and α ∈ [1,k],\\n\\n(16)\\n\\n3.2. Our results. First, we note that the span of {∂i|(x,n)}d 1 is not necessarily orthogonal to {∂d+α|(x,n)}k To aid the analysis, a new basis {∂H i |(x,n)}d 1 is obtained by projecting each ∂i|(x,n) orthogonal to the span of {∂d+α|(x,n)}k 1 via Gram-Schmidt orthogonalization. Through basic manipulations we then obtain the following lemma. 1.\\n\\nLemma 1. The pullback metric gε = D∗ εg = D∗ εΨ∗δd+k with respect to the coordinate vector fields d |(x,n),∂d+1|(x,n),...,∂d+k|(x,n)} on NBr, is given by 1 |(x,n),...,∂H {∂H\\n\\n(17)\\n\\nConsequently, the Riemannian gradient € C§°(NB\") with respect to ° is given by,\\n\\nNote that gε is guaranteed to be positive semidefinite. However, for large ε it can become singular for certain values of (n1,...,nk). The following lemma provides a sufficient and necessary condition on ε that ensures the positivity of gε throughout NBr.\\n\\nLemma 2. Let κ(x) ∈ R≥0 and κ∗ ∈ R≥0 be the absolute maximum principal curvature at x ∈ B and across B, respectively, given by κ∗ = maxx∈B κ(x) where\\n\\n.\\n\\nBy definition, we have rκ∗ < 1 and ε < 1. Consequently, the condition εrκ∗ < 1 is satisfied in our setting, which ensures that gε is positive definite on NBr.\\n\\nNow we state our main result which shows that the eigenvalue A corresponding to an eigenfunction (1er (14ers)? ¢ scales as (2 (er)? (Trn)? with respect to a unit increase in £5(¢) versus O —rr)d? with respect to a unit increase in Ep(¢).\\n\\nTheorem 3. If ϕ is a Neumann or Dirichlet eigenfunction of the Laplacian ∆δd+k on T εr then the corresponding eigenvalue λ satisfies\\n\\n(20)\\n\\n(21)\\n\\nand\\n\\n(22)\\n\\nwhere k*(z) € quantifies the mazimum rate of twisting of eg, 8 € [1,k], in any direction in the tangent space T, B, and k € Rx quantifies mazimum twisting among all normal directions. Specifically, = maxe where\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nAlso define a rectangular matrix Γβ(x) ∈ Rd×k such that for i,j ∈ [1,d] and α ∈ [1,k],\\n\\n(16)\\n\\n3.2. Our results. First, we note that the span of {∂i|(x,n)}d 1 is not necessarily orthogonal to {∂d+α|(x,n)}k To aid the analysis, a new basis {∂H i |(x,n)}d 1 is obtained by projecting each ∂i|(x,n) orthogonal to the span of {∂d+α|(x,n)}k 1 via Gram-Schmidt orthogonalization. Through basic manipulations we then obtain the following lemma. 1.\\n\\nLemma 1. The pullback metric gε = D∗ εg = D∗ εΨ∗δd+k with respect to the coordinate vector fields d |(x,n),∂d+1|(x,n),...,∂d+k|(x,n)} on NBr, is given by 1 |(x,n),...,∂H {∂H\\n\\n(17)\\n\\nConsequently, the Riemannian gradient € C§°(NB\") with respect to ° is given by,\\n\\nNote that gε is guaranteed to be positive semidefinite. However, for large ε it can become singular for certain values of (n1,...,nk). The following lemma provides a sufficient and necessary condition on ε that ensures the positivity of gε throughout NBr.\\n\\nLemma 2. Let κ(x) ∈ R≥0 and κ∗ ∈ R≥0 be the absolute maximum principal curvature at x ∈ B and across B, respectively, given by κ∗ = maxx∈B κ(x) where\\n\\n.\\n\\nBy definition, we have rκ∗ < 1 and ε < 1. Consequently, the condition εrκ∗ < 1 is satisfied in our setting, which ensures that gε is positive definite on NBr.\\n\\nNow we state our main result which shows that the eigenvalue A corresponding to an eigenfunction (1er (14ers)? ¢ scales as (2 (er)? (Trn)? with respect to a unit increase in £5(¢) versus O —rr)d? with respect to a unit increase in Ep(¢).\\n\\nTheorem 3. If ϕ is a Neumann or Dirichlet eigenfunction of the Laplacian ∆δd+k on T εr then the corresponding eigenvalue λ satisfies\\n\\n(20)\\n\\n(21)\\n\\nand\\n\\n(22)\\n\\nwhere k*(z) € quantifies the mazimum rate of twisting of eg, 8 € [1,k], in any direction in the tangent space T, B, and k € Rx quantifies mazimum twisting among all normal directions. Specifically, = maxe where\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 9\\n\\nDue to the fact that rκ∗ < 1, the above simplifies to,\\n\\nRemark 5. It easily follows that the above inequalities remain valid for a tubular neighborhood of B with varying width i.e. when ε depends on x, by replacing ε in the inequalities with maxx∈B ε(x).\\n\\nAn immediate consequence of the above result is that when ε ≪ 1 so that t is close to 1, any eigenvalue λ of ∆δd+k that is not too large—say of order O(ε−2η) for some η ∈ (0,1/2)—has a corresponding eigenfunction ϕ whose vertical energy E⊥ B(ϕ) is small, of order O(ε2t−2η). Consequently, the gradient of such an eigenfunction has a small component in the normal directions to B, making it a suitable candidate for tangent space estimation.\\n\\nIn the following results, we show that such eigenfunctions do exist if the Laplacian ∆gB on B has sufficiently small eigenvalues. First, using an eigenfunction of the Laplacian ∆gB on B with eigenvalue λB, we construct a function on T εr whose quadratic form is bounded by (1+εrκ∗)d (1−εrκ∗)d+2λB.\\n\\nTheorem 6. Let ϕB be a Neumann or Dirichlet eigenfunction of the Laplacian ∆gB on B with the corresponding eigenvalue,\\n\\nDefine an extension :NB 5 R of ¢ onto NB\" which has constant value along the cross sections,\\n\\nThen, the Dirichlet energy of ¢ = 4 defined on T\" satisfies,\\n\\n(24)\\n\\nBy combining the above result with Sturm-Liouville decomposition, we conclude that there exist eigen- (1+er)? Ap. Combining this with functions of As,, on T°\" whose eigenvalues are also bounded by T T—er) Theorem 3, we obtain the following corollary which shows that if there is an eigenvalue Ap of order 1 (1—er2d+2 o & (tern ) n € (0,1), then there exist an eigenfunction ¢ of As,, on T°\" whose vertical energy £ (¢) is small and is of order O(e227).\\n\\nCorollary 7. Let λB be a Neumann (Dirichlet) eigenvalue of ∆gB (Dirichlet) eigenfunction ϕ of ∆δd+k on T εr whose vertical energy satisfies, on B. Then there exist a Neumann\\n\\n(25)\\n\\n4. Robustness of Laplacian eigenvectors under noise\\n\\nA small gradient component of the low-frequency eigenvectors of the Laplacian in the noise direction is a direct consequence of the noise stability of the eigenvectors as illustrated in Figure 2. In this section, we analyze the stability of the Laplacian eigenvectors from the angle of robustness of random kernel Laplacians to sub-Gaussian noise. In turn, by the Davis-Kahan theorem [46], if the low-frequency eigengaps of the random kernel Laplacians do not vanish too quickly, the corresponding Laplacian eigenvectors will be robust to noise perturbations as well. The proofs of our results are provided in the appendix.\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 9\\n\\nDue to the fact that rκ∗ < 1, the above simplifies to,\\n\\nRemark 5. It easily follows that the above inequalities remain valid for a tubular neighborhood of B with varying width i.e. when ε depends on x, by replacing ε in the inequalities with maxx∈B ε(x).\\n\\nAn immediate consequence of the above result is that when ε ≪ 1 so that t is close to 1, any eigenvalue λ of ∆δd+k that is not too large—say of order O(ε−2η) for some η ∈ (0,1/2)—has a corresponding eigenfunction ϕ whose vertical energy E⊥ B(ϕ) is small, of order O(ε2t−2η). Consequently, the gradient of such an eigenfunction has a small component in the normal directions to B, making it a suitable candidate for tangent space estimation.\\n\\nIn the following results, we show that such eigenfunctions do exist if the Laplacian ∆gB on B has sufficiently small eigenvalues. First, using an eigenfunction of the Laplacian ∆gB on B with eigenvalue λB, we construct a function on T εr whose quadratic form is bounded by (1+εrκ∗)d (1−εrκ∗)d+2λB.\\n\\nTheorem 6. Let ϕB be a Neumann or Dirichlet eigenfunction of the Laplacian ∆gB on B with the corresponding eigenvalue,\\n\\nDefine an extension :NB 5 R of ¢ onto NB\" which has constant value along the cross sections,\\n\\nThen, the Dirichlet energy of ¢ = 4 defined on T\" satisfies,\\n\\n(24)\\n\\nBy combining the above result with Sturm-Liouville decomposition, we conclude that there exist eigen- (1+er)? Ap. Combining this with functions of As,, on T°\" whose eigenvalues are also bounded by T T—er) Theorem 3, we obtain the following corollary which shows that if there is an eigenvalue Ap of order 1 (1—er2d+2 o & (tern ) n € (0,1), then there exist an eigenfunction ¢ of As,, on T°\" whose vertical energy £ (¢) is small and is of order O(e227).\\n\\nCorollary 7. Let λB be a Neumann (Dirichlet) eigenvalue of ∆gB (Dirichlet) eigenfunction ϕ of ∆δd+k on T εr whose vertical energy satisfies, on B. Then there exist a Neumann\\n\\n(25)\\n\\n4. Robustness of Laplacian eigenvectors under noise\\n\\nA small gradient component of the low-frequency eigenvectors of the Laplacian in the noise direction is a direct consequence of the noise stability of the eigenvectors as illustrated in Figure 2. In this section, we analyze the stability of the Laplacian eigenvectors from the angle of robustness of random kernel Laplacians to sub-Gaussian noise. In turn, by the Davis-Kahan theorem [46], if the low-frequency eigengaps of the random kernel Laplacians do not vanish too quickly, the corresponding Laplacian eigenvectors will be robust to noise perturbations as well. The proofs of our results are provided in the appendix.\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nFigure 2. The first non-trivial eigenvector of the Laplacian L is plotted against increas- ing noise level ε. Here, the noise is independent but non-identically distributed as in Figure 1a (also see Remark 9). The common colorbar represents the range of values, from minimum to maximum, across the eigenvectors.\\n\\nAs before, let Y = {Y1,...,Yn} be the clean data points sampled from a bounded region in Rp. Fixing a bandwidth s > 0, we define the clean or ground-truth kernel adjacency matrix A ∈ Rn×n entrywise by the formula\\n\\n(26)\\n\\nwhere σs : Rp → (0,∞) is the Gaussian kernel defined as,\\n\\n(27)\\n\\nLater, we will make use of the following estimate on the Lipschitz constant of σs.\\n\\nLemma 8. For any s > 0 and z1,z2 ∈ Rp, it follows that\\n\\n(28)\\n\\nNoise is injected into each datapoint through a random variable Zi, leading to a corrupted dataset {X1,...,Xn} given by\\n\\n(29)\\n\\nWe assume the Zi are mean-zero, independent and identically distributed, and are sub-Gaussian with variance proxy ε ≥ 0. We define sub-Gaussian noise precisely below.\\n\\nRemark 9. Two remarks are in order:\\n\\n(1) The above setup generalizes the case where the clean data points lie on a d-dimensional submanifold embedded in p = d + k-dimensional Euclidean space while the noisy data points are a sample from a tubular neighborhood of the manifold i.e. Zi = Q⊥ i zi where Q⊥ is an orthonormal basis of the normal i space at Yi and zi ∈ Rk is uniformly distributed in a ball centered at origin and of radius ε.\\n\\n(2) If Zi are non-identically distributed sub-Gaussian random variables with variance proxy εi then the following results hold by replacing ε with maxn 1 εi.\\n\\nLet Z ∈ Rp be a random vector. We say that Z is a sub-Gaussian random vector with variance proxy ε ≥ 0 and center µ ∈ Rp if, for all α ∈ Rp, it holds\\n\\n(30)\\n\\nWe say that Z is centered if Eq. 30 holds when µ = 0p.\\n\\nEq. 29 leads to a noisy kernel adjacency matrix, given entrywise by the expression\\n\\n(31)\\n\\nWe investigate the concentration of ∥A − A∥ in the regime where:\\n\\n(i) The variance proxy ε of the noise shrinks as n gets large.\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nFigure 2. The first non-trivial eigenvector of the Laplacian L is plotted against increas- ing noise level ε. Here, the noise is independent but non-identically distributed as in Figure 1a (also see Remark 9). The common colorbar represents the range of values, from minimum to maximum, across the eigenvectors.\\n\\nAs before, let Y = {Y1,...,Yn} be the clean data points sampled from a bounded region in Rp. Fixing a bandwidth s > 0, we define the clean or ground-truth kernel adjacency matrix A ∈ Rn×n entrywise by the formula\\n\\n(26)\\n\\nwhere σs : Rp → (0,∞) is the Gaussian kernel defined as,\\n\\n(27)\\n\\nLater, we will make use of the following estimate on the Lipschitz constant of σs.\\n\\nLemma 8. For any s > 0 and z1,z2 ∈ Rp, it follows that\\n\\n(28)\\n\\nNoise is injected into each datapoint through a random variable Zi, leading to a corrupted dataset {X1,...,Xn} given by\\n\\n(29)\\n\\nWe assume the Zi are mean-zero, independent and identically distributed, and are sub-Gaussian with variance proxy ε ≥ 0. We define sub-Gaussian noise precisely below.\\n\\nRemark 9. Two remarks are in order:\\n\\n(1) The above setup generalizes the case where the clean data points lie on a d-dimensional submanifold embedded in p = d + k-dimensional Euclidean space while the noisy data points are a sample from a tubular neighborhood of the manifold i.e. Zi = Q⊥ i zi where Q⊥ is an orthonormal basis of the normal i space at Yi and zi ∈ Rk is uniformly distributed in a ball centered at origin and of radius ε.\\n\\n(2) If Zi are non-identically distributed sub-Gaussian random variables with variance proxy εi then the following results hold by replacing ε with maxn 1 εi.\\n\\nLet Z ∈ Rp be a random vector. We say that Z is a sub-Gaussian random vector with variance proxy ε ≥ 0 and center µ ∈ Rp if, for all α ∈ Rp, it holds\\n\\n(30)\\n\\nWe say that Z is centered if Eq. 30 holds when µ = 0p.\\n\\nEq. 29 leads to a noisy kernel adjacency matrix, given entrywise by the expression\\n\\n(31)\\n\\nWe investigate the concentration of ∥A − A∥ in the regime where:\\n\\n(i) The variance proxy ε of the noise shrinks as n gets large.\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 11\\n\\n(ii) The feature dimension p of the data remains fixed.\\n\\nThis regime is distinguished from the approach which has been considered in, e.g., [28] and where the authors instead allow the variance proxy to remain bounded from below while the feature dimension is taken to be arbitrarily large. On the other hand, we make assumptions on the decay of the variance proxy ε to obtain guarantees on the concentration of ∥A −A∥ at a distance of roughly O(n1/2) w.h.p. as n gets large. Note that we do not need to assume the clean data are uniformly bounded to ensure concentration of ∥A − A∥; however, this will be required later when we need to ensure that degrees are bounded from below.\\n\\n√\\n\\nTheorem 10. Assume that there exists c > 0 for which ε < c√ n log n, and let r > 2 be fixed. Then there exists a positive constant C1 ≡ C1(s,r,c) such that for n ≥ N1(p,r) sufficiently large, it holds (32) ∥A − A∥F ≤ C1n1/2 and ∥A − A∥∞ ≤ C1n1/2, each with probability at least 1 − n−r+2.\\n\\nThe proof follows from the Lipschitz continuity of σs and an application of the well-known tail bound for quadratic forms of sub-Gaussian random vectors (see [64]).\\n\\nNext, we define the degree of a node i with clean and noisy adjacency matrices as,\\n\\n(33)\\n\\nLet D,D to be the diagonal matrices consisting of di and di, respectively. The following Lemma utilizes the uniform boundedness of the clean data to bound the degrees from below. We will use this result to de-randomize bounds in the subsequent results on the stability of the normalized adjacency matrix and the resulting graph Laplacian.\\n\\n√\\n\\nLemma 11. Assume that there exists c > 0 for which ε < c√ n log n, and let r > 2 be fixed. Also assume that for some R > 0, ∥Yi∥ ≤ R for all i ∈ [1,n]. Then there exists a positive constant C2 ≡ C2(p,r,s,c) for which\\n\\n(34)\\n\\nwith probability at least 1 − n−r+2, where dmin = minn i=1 di and dmin = minn i=1 di.\\n\\nWe define the clean and noisy normalized adjacency matrices to be\\n\\n(35)\\n\\nUsing Theorem 10 and Lemma 11, we obtain concentration of ∥K − K∥ as follows.\\n\\n√\\n\\nTheorem 12. Assume that there exists c > 0 for which ε < c√ n log n, and let r > 2 be fixed. Then there exists a positive constant C3 ≡ C3(p,s,c,r) such that for n sufficiently large,\\n\\n(36)\\n\\n∥K − K∥F ≤ C3n−3/2 and ∥K − K∥∞ ≤ C3n−3/2,\\n\\neach with probability at least 1 − 2n−r+2.\\n\\nThe proof is an adaptation of the proof of Theorem 4 of Deng, Ling, and Strohmer [45]. We first show that || — K| < Cn°/?/ min d with high probability. This is a randomized bound that depends on dyin, which is de-randomized with high probability using Lemma 11, to obtain the final result.\\n\\nNext, we define the normalized degrees to be\\n\\n(37)\\n\\nLet D,D be the diagonal matrices consisting of δi and δi, respectively. Finally, we define the random walk graph Laplacians [43, 44] on the clean and the noisy data as\\n\\n(38)\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 11\\n\\n(ii) The feature dimension p of the data remains fixed.\\n\\nThis regime is distinguished from the approach which has been considered in, e.g., [28] and where the authors instead allow the variance proxy to remain bounded from below while the feature dimension is taken to be arbitrarily large. On the other hand, we make assumptions on the decay of the variance proxy ε to obtain guarantees on the concentration of ∥A −A∥ at a distance of roughly O(n1/2) w.h.p. as n gets large. Note that we do not need to assume the clean data are uniformly bounded to ensure concentration of ∥A − A∥; however, this will be required later when we need to ensure that degrees are bounded from below.\\n\\n√\\n\\nTheorem 10. Assume that there exists c > 0 for which ε < c√ n log n, and let r > 2 be fixed. Then there exists a positive constant C1 ≡ C1(s,r,c) such that for n ≥ N1(p,r) sufficiently large, it holds (32) ∥A − A∥F ≤ C1n1/2 and ∥A − A∥∞ ≤ C1n1/2, each with probability at least 1 − n−r+2.\\n\\nThe proof follows from the Lipschitz continuity of σs and an application of the well-known tail bound for quadratic forms of sub-Gaussian random vectors (see [64]).\\n\\nNext, we define the degree of a node i with clean and noisy adjacency matrices as,\\n\\n(33)\\n\\nLet D,D to be the diagonal matrices consisting of di and di, respectively. The following Lemma utilizes the uniform boundedness of the clean data to bound the degrees from below. We will use this result to de-randomize bounds in the subsequent results on the stability of the normalized adjacency matrix and the resulting graph Laplacian.\\n\\n√\\n\\nLemma 11. Assume that there exists c > 0 for which ε < c√ n log n, and let r > 2 be fixed. Also assume that for some R > 0, ∥Yi∥ ≤ R for all i ∈ [1,n]. Then there exists a positive constant C2 ≡ C2(p,r,s,c) for which\\n\\n(34)\\n\\nwith probability at least 1 − n−r+2, where dmin = minn i=1 di and dmin = minn i=1 di.\\n\\nWe define the clean and noisy normalized adjacency matrices to be\\n\\n(35)\\n\\nUsing Theorem 10 and Lemma 11, we obtain concentration of ∥K − K∥ as follows.\\n\\n√\\n\\nTheorem 12. Assume that there exists c > 0 for which ε < c√ n log n, and let r > 2 be fixed. Then there exists a positive constant C3 ≡ C3(p,s,c,r) such that for n sufficiently large,\\n\\n(36)\\n\\n∥K − K∥F ≤ C3n−3/2 and ∥K − K∥∞ ≤ C3n−3/2,\\n\\neach with probability at least 1 − 2n−r+2.\\n\\nThe proof is an adaptation of the proof of Theorem 4 of Deng, Ling, and Strohmer [45]. We first show that || — K| < Cn°/?/ min d with high probability. This is a randomized bound that depends on dyin, which is de-randomized with high probability using Lemma 11, to obtain the final result.\\n\\nNext, we define the normalized degrees to be\\n\\n(37)\\n\\nLet D,D be the diagonal matrices consisting of δi and δi, respectively. Finally, we define the random walk graph Laplacians [43, 44] on the clean and the noisy data as\\n\\n(38)\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nThe above construction of the graph Laplacian is used in all of our experiments. Using Theorem 12, we derive the stability of the Laplacian as follows.\\n\\n√\\n\\nTheorem 13. Assume that there exists c > 0 for which ε < c√ n log n, and let r > 2 be fixed. Then there exists a positive constant C3 ≡ C3(p,s,c,r) such that for n sufficiently large,\\n\\n(39)\\n\\nwith probability at least 1 − 2n−r+2.\\n\\nIt follows from the Davis-Kahan theorem (see [46]) that as long as the eigengaps of the clean Laplacian L do not vanish too quickly as n gets large, the eigenvectors of L and L will remain close as well. We state a somewhat simplified version of this below.\\n\\nCorollary 14. Instate the assumptions of Theorem 10 and Theorem 13. Enumerate the eigenvalues of L and L, respectively, as follows:\\n\\n(40)\\n\\n(41)\\n\\nLet m ≪ n be fixed and assume that for each 1 ≤ i ≤ m − 1, it holds\\n\\n(42)\\n\\nThen for each 1 ≤ i ≤ m−1 fixed, there exists a choice of sign τi ∈ {±1} so that if ϕi and ψi, respectively, denote the eigenvectors of L and L with eigenvalues λi and ξi, then it holds\\n\\n(43)\\n\\nNote that there are natural improvements of Corollary 14 to settings where as n gets large L picks up eigenvalues with vanishingly small gaps. We would ask instead that first m eigenvalues of L break into finitely many small groups and the distances between such groups decay at a rate no worse than ω(n−1/2). In this scenario, the distance ∥ϕi − τiψi∥2 would be replaced with distance between the corresponding eigenspaces; i.e., convergence of the eigenvectors up to some orthogonal alignment matrix. For simplicity we do not state such improvements here.\\n\\n5. Experiments\\n\\nIn this section, we estimate tangent spaces on several noisy synthetic and real-world datasets using LPCA and LEGO, compare the estimated tangent spaces against the ground truth, and assess their utility in the following downstream tasks: (a) manifold learning, where we compute an intrinsic-dimensional parametrization of the underlying data manifold; (b) boundary detection, where we identify points that lie on or near the boundary of the data manifold; and (c) local intrinsic dimension estimation, where we determine the dimension of the tangent space at each data point. For completeness, we briefly describe how the estimated tangent spaces are utilized in these tasks in Section C.\\n\\nTo quantify the accuracy of a tangent space estimate Qj ∈ Rp×d at the j-th data point, we compute its deviation from the ground-truth tangent space Q∗ j, obtained from clean data. Specifically, we calculate the principal angles θj,1,...,θj,d between the subspaces spanned by Qj and Q∗ j [65]. The quality of the estimate is measured by the discrepancy score:\\n\\n(44)\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nThe above construction of the graph Laplacian is used in all of our experiments. Using Theorem 12, we derive the stability of the Laplacian as follows.\\n\\n√\\n\\nTheorem 13. Assume that there exists c > 0 for which ε < c√ n log n, and let r > 2 be fixed. Then there exists a positive constant C3 ≡ C3(p,s,c,r) such that for n sufficiently large,\\n\\n(39)\\n\\nwith probability at least 1 − 2n−r+2.\\n\\nIt follows from the Davis-Kahan theorem (see [46]) that as long as the eigengaps of the clean Laplacian L do not vanish too quickly as n gets large, the eigenvectors of L and L will remain close as well. We state a somewhat simplified version of this below.\\n\\nCorollary 14. Instate the assumptions of Theorem 10 and Theorem 13. Enumerate the eigenvalues of L and L, respectively, as follows:\\n\\n(40)\\n\\n(41)\\n\\nLet m ≪ n be fixed and assume that for each 1 ≤ i ≤ m − 1, it holds\\n\\n(42)\\n\\nThen for each 1 ≤ i ≤ m−1 fixed, there exists a choice of sign τi ∈ {±1} so that if ϕi and ψi, respectively, denote the eigenvectors of L and L with eigenvalues λi and ξi, then it holds\\n\\n(43)\\n\\nNote that there are natural improvements of Corollary 14 to settings where as n gets large L picks up eigenvalues with vanishingly small gaps. We would ask instead that first m eigenvalues of L break into finitely many small groups and the distances between such groups decay at a rate no worse than ω(n−1/2). In this scenario, the distance ∥ϕi − τiψi∥2 would be replaced with distance between the corresponding eigenspaces; i.e., convergence of the eigenvectors up to some orthogonal alignment matrix. For simplicity we do not state such improvements here.\\n\\n5. Experiments\\n\\nIn this section, we estimate tangent spaces on several noisy synthetic and real-world datasets using LPCA and LEGO, compare the estimated tangent spaces against the ground truth, and assess their utility in the following downstream tasks: (a) manifold learning, where we compute an intrinsic-dimensional parametrization of the underlying data manifold; (b) boundary detection, where we identify points that lie on or near the boundary of the data manifold; and (c) local intrinsic dimension estimation, where we determine the dimension of the tangent space at each data point. For completeness, we briefly describe how the estimated tangent spaces are utilized in these tasks in Section C.\\n\\nTo quantify the accuracy of a tangent space estimate Qj ∈ Rp×d at the j-th data point, we compute its deviation from the ground-truth tangent space Q∗ j, obtained from clean data. Specifically, we calculate the principal angles θj,1,...,θj,d between the subspaces spanned by Qj and Q∗ j [65]. The quality of the estimate is measured by the discrepancy score:\\n\\n(44)\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 13\\n\\n(b) Discrepancy between estimated and true tangent spaces\\n\\nLPCA (k, =9)\\n\\n(¢) 2D paramterization using true and estimated\\n\\n(d) Boundary detection using true and estimated tangent spaces\\n\\n©00\\n\\nGround truth\\n\\nLPCA\\n\\n(e) Functional variance explained by each principal direction\\n\\n1.00\\n\\nLEGO X; — V(\\n\\n0.75\\n\\n0.50\\n\\n0.25\\n\\n0.00\\n\\nFigure 3. (a) Clean and noisy Swiss roll with high-aspect ratio in R3 colored by the “roll” parameter. (b) Discrepancy between the true and the estimated tangent spaces due to LPCA (knn = 9) and LEGO (knn = 9, m0 = 100, m = 40), as computed using Eq. 44. (c, d) 2-dimensional parameterization of the noisy data, and the boundary points detected from the noisy data using the estimated and the true tangent spaces (see Section C.1 and C.2 for details) (e) The functional variance explained by each of the three principal directions in LPCA and LEGO (see Section C.3).\\n\\n5.1. High-aspect ratio Swiss roll and a truncated torus. We begin with two synthetic datasets: a high–aspect-ratio Swiss roll and a truncated torus. For the Swiss roll, we generate n = 10700 uniformly distributed points in R3, forming the clean dataset Y (Figure 3a). Each point is perturbed by adding uniform noise in the direction normal to the underlying tangent space. Specifically, the noisy data points are given by Xj = Yj +ηjνj, where νj is outward normal to the tangent space at Yj and the coefficient ηj is uniformly distributed in (−ε,ε) where ε = 0.0175. The resulting noisy dataset X is shown in Figure 3a.\\n\\nFor the truncated torus, we sample n = 3617 uniformly distributed points on a subset of the torus in R3 as shown in Figure 4a. Here, each data point Yj is parameterized by (u,v) ∈ [0,2π)2 i.e. Yj ≡ Yj(uj,vj). We corrupt the clean data with heteroskedastic noise added in the normal direction to the tangent space at each point. The noisy data points are given by Xj = Yj + ηjνj, where νj is the outward normal direction to the tangent space at Yj and the coefficient ηj is uniformly distributed in (−ε(uj),ε(uj)) where ε(u) = 10−2 + 2.5 × 10−3(1 + cos(2u)). The noisy dataset X is shown in Figure 4a.\\n\\nFor both datasets, we estimate an orthonormal basis Qj of the 2-dimensional tangent space at each Xj using LPCA and LEGO, and then compute the discrepancy Dj (Eq. 44) between the estimates Qj and the ground-truth Q∗ j, as shown in Figure 3b and 4b. These results show that LEGO produces significantly more accurate estimates while LPCA estimates are highly sensitive to noise. Noise ablation (Figure 6) confirms LPCA estimates degrade rapidly with noise, whereas LEGO consistently yields reliable estimates.\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 13\\n\\n(b) Discrepancy between estimated and true tangent spaces\\n\\nLPCA (k, =9)\\n\\n(¢) 2D paramterization using true and estimated\\n\\n(d) Boundary detection using true and estimated tangent spaces\\n\\n©00\\n\\nGround truth\\n\\nLPCA\\n\\n(e) Functional variance explained by each principal direction\\n\\n1.00\\n\\nLEGO X; — V(\\n\\n0.75\\n\\n0.50\\n\\n0.25\\n\\n0.00\\n\\nFigure 3. (a) Clean and noisy Swiss roll with high-aspect ratio in R3 colored by the “roll” parameter. (b) Discrepancy between the true and the estimated tangent spaces due to LPCA (knn = 9) and LEGO (knn = 9, m0 = 100, m = 40), as computed using Eq. 44. (c, d) 2-dimensional parameterization of the noisy data, and the boundary points detected from the noisy data using the estimated and the true tangent spaces (see Section C.1 and C.2 for details) (e) The functional variance explained by each of the three principal directions in LPCA and LEGO (see Section C.3).\\n\\n5.1. High-aspect ratio Swiss roll and a truncated torus. We begin with two synthetic datasets: a high–aspect-ratio Swiss roll and a truncated torus. For the Swiss roll, we generate n = 10700 uniformly distributed points in R3, forming the clean dataset Y (Figure 3a). Each point is perturbed by adding uniform noise in the direction normal to the underlying tangent space. Specifically, the noisy data points are given by Xj = Yj +ηjνj, where νj is outward normal to the tangent space at Yj and the coefficient ηj is uniformly distributed in (−ε,ε) where ε = 0.0175. The resulting noisy dataset X is shown in Figure 3a.\\n\\nFor the truncated torus, we sample n = 3617 uniformly distributed points on a subset of the torus in R3 as shown in Figure 4a. Here, each data point Yj is parameterized by (u,v) ∈ [0,2π)2 i.e. Yj ≡ Yj(uj,vj). We corrupt the clean data with heteroskedastic noise added in the normal direction to the tangent space at each point. The noisy data points are given by Xj = Yj + ηjνj, where νj is the outward normal direction to the tangent space at Yj and the coefficient ηj is uniformly distributed in (−ε(uj),ε(uj)) where ε(u) = 10−2 + 2.5 × 10−3(1 + cos(2u)). The noisy dataset X is shown in Figure 4a.\\n\\nFor both datasets, we estimate an orthonormal basis Qj of the 2-dimensional tangent space at each Xj using LPCA and LEGO, and then compute the discrepancy Dj (Eq. 44) between the estimates Qj and the ground-truth Q∗ j, as shown in Figure 3b and 4b. These results show that LEGO produces significantly more accurate estimates while LPCA estimates are highly sensitive to noise. Noise ablation (Figure 6) confirms LPCA estimates degrade rapidly with noise, whereas LEGO consistently yields reliable estimates.\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\n(a) Clean truncated torus\\n\\nNoisy points\\n\\n0.05\\n\\n0.04\\n\\n0.03\\n\\n0.02\\n\\n(b) Discrepancy between estimated and true tangent spaces\\n\\nLPCA (kyy = 14)\\n\\n(¢) 2D paramterization using true and estimated tangent spaces\\n\\n(d) Boundary detection using true and estimated tangent spaces (top view)\\n\\nl\\n\\nGround truth\\n\\nLEGO\\n\\n( Functional variance explained by each principal direction\\n\\nFigure 4. (a) Clean and noisy truncated torus in R3 colored by the noise level. (b) Discrepancy between the true and the estimated tangent spaces due to LPCA (knn = 14) and LEGO (knn = 14, m0 = 100, m = 20), as computed using Eq. 44. (c, d) 2-dimensional parameterization of the noisy data, and the boundary points detected from the noisy data using the estimated and the true tangent spaces (see Section C.1 and C.2 for details) (e) The functional variance explained by each of the three principal directions in LPCA and LEGO (see Section C.3).\\n\\nHyperparameter analysis (Figure 7) also shows that LEGO estimates remain stable across a broad range of values for m and m0.\\n\\nTo assess how these tangent space estimates affect downstream tasks, we use them to compute a 2- dimensional embedding of the noisy data and to detect boundary points (see Section C). As shown in Figure 3c and 4c, and Figure 3d and 4d, the embeddings and the detected boundary points based on LPCA estimates are severely degraded by noise, while those based on LEGO closely match the results obtained using the true tangent spaces. This is not surprising as the accuracy of the tangent space estimation is critical to the performance of several algorithms [1, 35, 3, 5, 38, 37] designed for these downstream tasks.\\n\\nFinally, by setting d = p = 3, we compute the functional variance explained by each principal direction (Section C.3). As shown in Figure 3e and 4e, LEGO concentrates functional variance in the first two directions, aligning with the true intrinsic structure, while LPCA spuriously allocates variance to the third direction, reflecting noise-sensitivity of the local intrinsic dimension estimates due to LPCA.\\n\\n5.2. Puppets data. In this real-world experiment, we use an image dataset from [66], consisting of n = 8100 camera snapshots of a platform with two rotating objects—Yoda and a bulldog—each rotating about its vertical axis at distinct frequencies. As a result, the intrinsic geometry of the dataset corresponds to a 2-dimensional flat torus. The original images of size 320 × 240 × 3 are first normalized to the range\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\n(a) Clean truncated torus\\n\\nNoisy points\\n\\n0.05\\n\\n0.04\\n\\n0.03\\n\\n0.02\\n\\n(b) Discrepancy between estimated and true tangent spaces\\n\\nLPCA (kyy = 14)\\n\\n(¢) 2D paramterization using true and estimated tangent spaces\\n\\n(d) Boundary detection using true and estimated tangent spaces (top view)\\n\\nl\\n\\nGround truth\\n\\nLEGO\\n\\n( Functional variance explained by each principal direction\\n\\nFigure 4. (a) Clean and noisy truncated torus in R3 colored by the noise level. (b) Discrepancy between the true and the estimated tangent spaces due to LPCA (knn = 14) and LEGO (knn = 14, m0 = 100, m = 20), as computed using Eq. 44. (c, d) 2-dimensional parameterization of the noisy data, and the boundary points detected from the noisy data using the estimated and the true tangent spaces (see Section C.1 and C.2 for details) (e) The functional variance explained by each of the three principal directions in LPCA and LEGO (see Section C.3).\\n\\nHyperparameter analysis (Figure 7) also shows that LEGO estimates remain stable across a broad range of values for m and m0.\\n\\nTo assess how these tangent space estimates affect downstream tasks, we use them to compute a 2- dimensional embedding of the noisy data and to detect boundary points (see Section C). As shown in Figure 3c and 4c, and Figure 3d and 4d, the embeddings and the detected boundary points based on LPCA estimates are severely degraded by noise, while those based on LEGO closely match the results obtained using the true tangent spaces. This is not surprising as the accuracy of the tangent space estimation is critical to the performance of several algorithms [1, 35, 3, 5, 38, 37] designed for these downstream tasks.\\n\\nFinally, by setting d = p = 3, we compute the functional variance explained by each principal direction (Section C.3). As shown in Figure 3e and 4e, LEGO concentrates functional variance in the first two directions, aligning with the true intrinsic structure, while LPCA spuriously allocates variance to the third direction, reflecting noise-sensitivity of the local intrinsic dimension estimates due to LPCA.\\n\\n5.2. Puppets data. In this real-world experiment, we use an image dataset from [66], consisting of n = 8100 camera snapshots of a platform with two rotating objects—Yoda and a bulldog—each rotating about its vertical axis at distinct frequencies. As a result, the intrinsic geometry of the dataset corresponds to a 2-dimensional flat torus. The original images of size 320 × 240 × 3 are first normalized to the range\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 15\\n\\n(a) Clean images and their noisy counterparts (image size = (120, 160, 3))\\n\\n(b) Variance explained in the data by first 30 PCs\\n\\n(¢) Visualization of first 3 PCs\\n\\n-5\\n\\n( LEGO assisted 2d parameterization B\\n\\n(d) 2d parameterizations obtained by estimated tangent spaces\\n\\nLPCA (ky = 14)\\n\\nLEGO (knn =\\n\\n14)\\n\\n(f) Functional\\n\\neach\\n\\ndirection\\n\\nFigure 5. (a) Sample clean images from the Yoda and Bulldog dataset [66] (first and third columns), along with their noise-perturbed versions (second and fourth columns). (b) Explained variance ratio for the first 30 principal directions obtained via PCA. As the variance saturates after 10 dimensions, we project the noisy images into R10 using PCA. (c) Visualization of the noisy data using its first three principal components. The colorbar corresponds to the third component. (d) Two-dimensional torn embeddings of the noisy data using the estimated tangent spaces (see Section C.1 and [5] for details). (e) The torn 2d embedding obtained using LEGO estimates, equipped with the gluing instructions that identify the same colored points along the tear, reveals a toroidal topology. The corresponding clean images along the opposite edges further confirm this structure. (f) Functional variance explained by each of the 10 principal directions obtained from LPCA and LEGO (see Section C.3).\\n\\n[0,1], followed by addition of uniformly distributed noise in (−1,1) to each pixel channel. Examples of both clean and noisy images are shown in Figure 5a (the pixel values are clipped between [0,1] for visualization). Due to computational constraints, we first reduce the dimensionality of the noisy dataset. Based on the explained variance ratio shown in Figure 5b, we project the data to p = 10 dimensions, resulting in the final dataset X which is utilized for tangent space estimation.\\n\\nWe then estimate the 2-dimensional tangent spaces using both LPCA (knn = 14) and LEGO (knn = 14, m0 = 100, m = 20). These estimates are used to compute a 2-dimensional embedding of the noisy data. Because the data lies on a closed manifold, directly aligning the local intrinsic-dimensional embeddings derived from the tangent space estimates using standard methods leads to a collapse—specifically, the resulting intrinsic parameterization is non-injective. To obtain an injective embedding, we adopt the tear- enabled alignment framework introduced in [5], which produces a torn 2d embedding of the data. As shown in Figure 5d, the embedding based on LPCA estimates is non-interpretable, whereas LEGO produces a clear rectangular embedding. When visualized with gluing instructions (Figure 5e)—which identifies the same-colored points along the tear—it becomes evident that opposite edges of the rectangle should be glued, revealing the underlying toroidal topology. Moreover, examining the clean images corresponding to the points on opposite edges shows that only one of the two puppets undergoes rotation, further supporting the toroidal structure.\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 15\\n\\n(a) Clean images and their noisy counterparts (image size = (120, 160, 3))\\n\\n(b) Variance explained in the data by first 30 PCs\\n\\n(¢) Visualization of first 3 PCs\\n\\n-5\\n\\n( LEGO assisted 2d parameterization B\\n\\n(d) 2d parameterizations obtained by estimated tangent spaces\\n\\nLPCA (ky = 14)\\n\\nLEGO (knn =\\n\\n14)\\n\\n(f) Functional\\n\\neach\\n\\ndirection\\n\\nFigure 5. (a) Sample clean images from the Yoda and Bulldog dataset [66] (first and third columns), along with their noise-perturbed versions (second and fourth columns). (b) Explained variance ratio for the first 30 principal directions obtained via PCA. As the variance saturates after 10 dimensions, we project the noisy images into R10 using PCA. (c) Visualization of the noisy data using its first three principal components. The colorbar corresponds to the third component. (d) Two-dimensional torn embeddings of the noisy data using the estimated tangent spaces (see Section C.1 and [5] for details). (e) The torn 2d embedding obtained using LEGO estimates, equipped with the gluing instructions that identify the same colored points along the tear, reveals a toroidal topology. The corresponding clean images along the opposite edges further confirm this structure. (f) Functional variance explained by each of the 10 principal directions obtained from LPCA and LEGO (see Section C.3).\\n\\n[0,1], followed by addition of uniformly distributed noise in (−1,1) to each pixel channel. Examples of both clean and noisy images are shown in Figure 5a (the pixel values are clipped between [0,1] for visualization). Due to computational constraints, we first reduce the dimensionality of the noisy dataset. Based on the explained variance ratio shown in Figure 5b, we project the data to p = 10 dimensions, resulting in the final dataset X which is utilized for tangent space estimation.\\n\\nWe then estimate the 2-dimensional tangent spaces using both LPCA (knn = 14) and LEGO (knn = 14, m0 = 100, m = 20). These estimates are used to compute a 2-dimensional embedding of the noisy data. Because the data lies on a closed manifold, directly aligning the local intrinsic-dimensional embeddings derived from the tangent space estimates using standard methods leads to a collapse—specifically, the resulting intrinsic parameterization is non-injective. To obtain an injective embedding, we adopt the tear- enabled alignment framework introduced in [5], which produces a torn 2d embedding of the data. As shown in Figure 5d, the embedding based on LPCA estimates is non-interpretable, whereas LEGO produces a clear rectangular embedding. When visualized with gluing instructions (Figure 5e)—which identifies the same-colored points along the tear—it becomes evident that opposite edges of the rectangle should be glued, revealing the underlying toroidal topology. Moreover, examining the clean images corresponding to the points on opposite edges shows that only one of the two puppets undergoes rotation, further supporting the toroidal structure.\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nFinally, by setting d = p = 10, we compute the functional variance explained by each of the 10 principal directions obtained by applying LPCA and LEGO to the noisy data (see Section C.3). As shown in Figure 5f, LEGO concentrates the functional variance in the first two directions, faithfully capturing the underlying 2d structure. In contrast, LPCA distributes the variance across multiple dimensions, highlighting its sensitivity to noise and its inability to accurately recover the local intrinsic geometry in the noisy setting.\\n\\nAppendix A. Proofs from 3\\n\\nProof of Lemma 1. The following form of the pullback metric g = Ψ∗δd+k with respect to the coordinate vector fields defined in Eq. 7, and as derived in [63, Lemma 4.1], is given by\\n\\n(45)\\n\\n(46)\\n\\n(47)\\n\\nfor i,j ∈ [1,d], α,β ∈ [1,k] and where (i) Weα : X(B) → X(B) is the Weingarten map that captures the projection of ∇∂xieα(x) on TB i.e. Weα(∂xi) = −(∇Rd+k ∂xi eα(x))⊤, and (ii) ∇⊥ : X(B)×X(NB) → X(NB) is the normal connection that captures the projection of ∇Rd+k ∂xi eα(x) on NB, and is given by ∇⊥ ∂xi eα(x) = (∇Rd+k eα(x))⊥. Since Πν(∂xi,∂xj) = g⊥ B(ν,Π(∂xi,∂xj)) = gB(∂xi,Wν(∂xj)) = gB(∂xj,Wν(∂xi)), there- ∂xi fore, using the definitions of hj αi and γβ iα in Eq. 13 and 14,\\n\\n(48)\\n\\n(49)\\n\\n(50)\\n\\nTherefore,\\n\\n(51)\\n\\n(52)\\n\\n(53)\\n\\n(54)\\n\\nConsequently, the scaled pullback metric gε = D∗ εΨ∗δd+k with respect to the coordinate vector fields in Eq. 7 is given by,\\n\\n(55)\\n\\n(56)\\n\\n(57)\\n\\nNow, the new basis vector ∂H i |(x,n) obtained by projecting ∂i|(x,n) orthogonal to the span of {∂d+α|(x,n)}k 1 is given by,\\n\\n(58)\\n\\nk\\n\\nConsequently, the pullback metric gε in the new local coordinate fields {∂H i |(x,n)}d 1 and {∂d+α|(x,n)}k 1, is given by\\n\\n(59)\\n\\n(60)\\n\\n(61)\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nFinally, by setting d = p = 10, we compute the functional variance explained by each of the 10 principal directions obtained by applying LPCA and LEGO to the noisy data (see Section C.3). As shown in Figure 5f, LEGO concentrates the functional variance in the first two directions, faithfully capturing the underlying 2d structure. In contrast, LPCA distributes the variance across multiple dimensions, highlighting its sensitivity to noise and its inability to accurately recover the local intrinsic geometry in the noisy setting.\\n\\nAppendix A. Proofs from 3\\n\\nProof of Lemma 1. The following form of the pullback metric g = Ψ∗δd+k with respect to the coordinate vector fields defined in Eq. 7, and as derived in [63, Lemma 4.1], is given by\\n\\n(45)\\n\\n(46)\\n\\n(47)\\n\\nfor i,j ∈ [1,d], α,β ∈ [1,k] and where (i) Weα : X(B) → X(B) is the Weingarten map that captures the projection of ∇∂xieα(x) on TB i.e. Weα(∂xi) = −(∇Rd+k ∂xi eα(x))⊤, and (ii) ∇⊥ : X(B)×X(NB) → X(NB) is the normal connection that captures the projection of ∇Rd+k ∂xi eα(x) on NB, and is given by ∇⊥ ∂xi eα(x) = (∇Rd+k eα(x))⊥. Since Πν(∂xi,∂xj) = g⊥ B(ν,Π(∂xi,∂xj)) = gB(∂xi,Wν(∂xj)) = gB(∂xj,Wν(∂xi)), there- ∂xi fore, using the definitions of hj αi and γβ iα in Eq. 13 and 14,\\n\\n(48)\\n\\n(49)\\n\\n(50)\\n\\nTherefore,\\n\\n(51)\\n\\n(52)\\n\\n(53)\\n\\n(54)\\n\\nConsequently, the scaled pullback metric gε = D∗ εΨ∗δd+k with respect to the coordinate vector fields in Eq. 7 is given by,\\n\\n(55)\\n\\n(56)\\n\\n(57)\\n\\nNow, the new basis vector ∂H i |(x,n) obtained by projecting ∂i|(x,n) orthogonal to the span of {∂d+α|(x,n)}k 1 is given by,\\n\\n(58)\\n\\nk\\n\\nConsequently, the pullback metric gε in the new local coordinate fields {∂H i |(x,n)}d 1 and {∂d+α|(x,n)}k 1, is given by\\n\\n(59)\\n\\n(60)\\n\\n(61)\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 17\\n\\n(62)\\n\\ngε\\n\\nd+α,d+β(x,n) = g⊥\\n\\nB(eα,eβ) = ε2δαβ.\\n\\nUsing the definition of Hα in Eq. 15,\\n\\n(63)\\n\\n(64)\\n\\n(65)\\n\\non Finally, we decompose grad¢ into a component (grade) on 7(TB) and a component (grade ker(,). Specifically, grad¢ = (grade)? grad) where\\n\\n(66)\\n\\n(67)\\n\\nUsing the definition of Γβ in Eq. 16,\\n\\n(68)\\n\\nProof of Lemma 2. Using the expression of gε we obtain,\\n\\n(69)\\n\\nUsing Cauchy-Schwarz inequality, we obtain\\n\\n(70)\\n\\nSince, for each x ∈ B the maximum value of rκ(x) can be realized for some v dependent on x, therefore det(gε) > 0 if and only if εrκ(x) < 1. Under this constraint, it follows that\\n\\n(71)\\n\\nProof of Theorem 3. First note that for f ∈ C∞ 0 (T εr),\\n\\n72) fdVs,, AWVas V Br ( 1 )dVe Therefore, if (A, is an eigenpair of As,,, on T°\" with Neumann or Dirichlet boundary conditions then it follows that 1 is an eigenfunction of Age with the same eigenvalue. Specifically,\\n\\n(73)\\n\\nUsing Lemma 1, the definition of κ∗ in Lemma 2 and Cauchy-Schwarz inequality, we obtain\\n\\nand\\n\\n□\\n\\n□\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 17\\n\\n(62)\\n\\ngε\\n\\nd+α,d+β(x,n) = g⊥\\n\\nB(eα,eβ) = ε2δαβ.\\n\\nUsing the definition of Hα in Eq. 15,\\n\\n(63)\\n\\n(64)\\n\\n(65)\\n\\non Finally, we decompose grad¢ into a component (grade) on 7(TB) and a component (grade ker(,). Specifically, grad¢ = (grade)? grad) where\\n\\n(66)\\n\\n(67)\\n\\nUsing the definition of Γβ in Eq. 16,\\n\\n(68)\\n\\nProof of Lemma 2. Using the expression of gε we obtain,\\n\\n(69)\\n\\nUsing Cauchy-Schwarz inequality, we obtain\\n\\n(70)\\n\\nSince, for each x ∈ B the maximum value of rκ(x) can be realized for some v dependent on x, therefore det(gε) > 0 if and only if εrκ(x) < 1. Under this constraint, it follows that\\n\\n(71)\\n\\nProof of Theorem 3. First note that for f ∈ C∞ 0 (T εr),\\n\\n72) fdVs,, AWVas V Br ( 1 )dVe Therefore, if (A, is an eigenpair of As,,, on T°\" with Neumann or Dirichlet boundary conditions then it follows that 1 is an eigenfunction of Age with the same eigenvalue. Specifically,\\n\\n(73)\\n\\nUsing Lemma 1, the definition of κ∗ in Lemma 2 and Cauchy-Schwarz inequality, we obtain\\n\\nand\\n\\n□\\n\\n□\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nβ and κ⊥∗ provided in the statement ≤ r and the definitions of κ⊥ In the last two equations we used ∥n∥δk of the theorem. Combining the above with the bounds on det(gε) in Lemma 2, we obtain\\n\\nThe result follows from the definitions of normalized horizontal and vertical energies in Eq. 12. Similarly,\\n\\nD. KOHLI, S.J. ROBERTSON, G. MISHNE, A. CLONINGER\\n\\nβ and κ⊥∗ provided in the statement ≤ r and the definitions of κ⊥ In the last two equations we used ∥n∥δk of the theorem. Combining the above with the bounds on det(gε) in Lemma 2, we obtain\\n\\nThe result follows from the definitions of normalized horizontal and vertical energies in Eq. 12. Similarly,\\n\\nROBUST TANGENT SPACE ESTIMATION VIA LAPLACIAN EIGENVECTOR GRADIENT ORTHOGONALIZATION 19\\n\\nCombining the above with the bounds on det(gε) in Lemma 2, we obtain\\n\\nThe result follows from the definitions of horizontal and vertical energies in Eq. 12.\\n\\nProof of Theorem 6. Using the fact that Vo 0, the proof is similar to the proof of Theorem 3.\\n\\n√\\n\\nProof of Lemma 8. Note that ∇σs(z) = − 2 s2zσs(z). Thus, ∥∇σs(z)∥2 ≤ 2 follows from the properties of Lipschitz functions. s2σs(z)∥z∥2 ≤ 2/e s . The claim □\\n\\nLemma 15 (Specialized version of Theorem 2.1 of [64]). Suppose Z is a centered sub-Gaussian random vector with parameter ε ≥ 0. Then for all t > 0, it holds\\n\\n(74)\\n\\nProof of Theorem ', 'text_length': 100000, 'message': 'Retrieved 54 pages for 2510v02308v1.pdf'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `generate_summary` with `Estimating tangent spaces on data manifolds is crucial in data analysis, yet traditional methods like Local Principal Component Analysis (LPCA) falter in noisy environments due to their reliance on local neighborhood sizes, which can be difficult to optimize without prior knowledge. This paper introduces a novel approach called Laplacian Eigenvector Gradient Orthogonalization (LEGO), which leverages the global structure of data via graph Laplacian eigenvectors to improve tangent space estimation. LEGO orthogonalizes gradients of low-frequency eigenvectors, which are robust to noise, thereby enhancing the accuracy of tangent space estimates. Theoretical analyses, including differential geometry and random matrix theory, support LEGO's robustness against noise. Empirical results demonstrate LEGO's superiority over LPCA in tasks such as manifold learning, boundary detection, and local intrinsic dimension estimation. This advancement is significant for AI professionals as it offers a more reliable method for geometric data analysis, potentially improving applications in machine learning and data science where noise is prevalent.`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m{'status': 'success', 'summary': \"The paper introduces a novel method called Laplacian Eigenvector Gradient Orthogonalization (LEGO) for estimating tangent spaces on data manifolds, addressing the limitations of traditional methods like Local Principal Component Analysis (LPCA) in noisy environments. LEGO utilizes graph Laplacian eigenvectors to leverage the global structure of data, orthogonalizing gradients of low-frequency eigenvectors to enhance the accuracy of tangent space estimates. Theoretical analyses confirm LEGO's robustness against noise, supported by differential geometry and random matrix theory. Empirical results show that LEGO outperforms LPCA in manifold learning, boundary detection, and local intrinsic dimension estimation. This advancement is significant for AI and data science professionals, offering a more reliable method for geometric data analysis in noisy conditions. Consequently, LEGO has the potential to improve machine learning applications where accurate data manifold estimation is crucial, impacting industries reliant on precise data analysis.\", 'length': 1054, 'message': 'Generated summary (1054 characters)'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_document_text` with `2510v02312v1.pdf`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m{'status': 'success', 'filename': '2510v02312v1.pdf', 'pages': 42, 'text': 'Preprint. Under Review.\\n\\n5 2 0 2 t c O 2 ] G L . s c [ 1 v 2 1 3 2 0 . 0 1 5 2 : v i X r a\\n\\nKAVA: LATENT REASONING VIA COMPRESSED KV-CACHE DISTILLATION\\n\\nAnna Kuzina∗\\n\\nQualcomm AI Research†\\n\\nakuzina@qti.qualcomm.com\\n\\nPaul N. Whatmough\\n\\nQualcomm AI Research pwhatmou@qti.qualcomm.com\\n\\nMaciej Pioro∗‡ IDEAS NCBR / IPPT PAN maciej.pioro@gmail.com\\n\\nBabak Ehteshami Bejnordi\\n\\nQualcomm AI Research behtesha@qti.qualcomm.com\\n\\nABSTRACT\\n\\nLarge Language Models (LLMs) excel at multi-step reasoning problems with ex- plicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought pro- cess, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work we propose KAVA, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self- distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowl- edge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, com- bining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference.\\n\\n1 INTRODUCTION\\n\\nRecent advancements in Large Language Models (LLMs) have demonstrated remarkable capabil- ities in solving complex problems across domains such as mathematics (Zhang et al., 2025), sci- ence (Phan et al., 2025), and code generation (Hui et al., 2024). A key driver of this progress is “chain-of-thought” (CoT) training that elicits intermediate steps before the final answer, improving accuracy on long-horizon inference problems (DeepSeek-AI et al., 2025). Yet, explicit CoT often in- curs substantial inference cost due to long, verbose traces and the associated key–value (KV) cache growth, making deployment on memory- and compute-constrained devices difficult. Furthermore, CoT traces, especially those distilled from larger models, can inherit and amplify biases or contain plausible-sounding but fallacious logic, limiting their reliability.\\n\\nRecent studies show that the KV-caches underlying CoT are highly redundant and can be aggres- sively compressed with little to no loss in accuracy (Cai et al., 2025; Park et al., 2025), indicating that much of CoT’s signal resides in compressible structure rather than indispensable text. This observation suggests an alternative supervisory path: if the essential dynamics of reasoning live in the cache, perhaps models can be trained to internalize those dynamics without verbose traces at inference time. However, this compressed KV-cache presents a significant challenge for knowledge\\n\\n∗Equal contribution\\n\\n†Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.\\n\\n‡Work done during internship at Qualcomm AI Research.\\n\\nPreprint. Under Review.\\n\\n5 2 0 2 t c O 2 ] G L . s c [ 1 v 2 1 3 2 0 . 0 1 5 2 : v i X r a\\n\\nKAVA: LATENT REASONING VIA COMPRESSED KV-CACHE DISTILLATION\\n\\nAnna Kuzina∗\\n\\nQualcomm AI Research†\\n\\nakuzina@qti.qualcomm.com\\n\\nPaul N. Whatmough\\n\\nQualcomm AI Research pwhatmou@qti.qualcomm.com\\n\\nMaciej Pioro∗‡ IDEAS NCBR / IPPT PAN maciej.pioro@gmail.com\\n\\nBabak Ehteshami Bejnordi\\n\\nQualcomm AI Research behtesha@qti.qualcomm.com\\n\\nABSTRACT\\n\\nLarge Language Models (LLMs) excel at multi-step reasoning problems with ex- plicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought pro- cess, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work we propose KAVA, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self- distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowl- edge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, com- bining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference.\\n\\n1 INTRODUCTION\\n\\nRecent advancements in Large Language Models (LLMs) have demonstrated remarkable capabil- ities in solving complex problems across domains such as mathematics (Zhang et al., 2025), sci- ence (Phan et al., 2025), and code generation (Hui et al., 2024). A key driver of this progress is “chain-of-thought” (CoT) training that elicits intermediate steps before the final answer, improving accuracy on long-horizon inference problems (DeepSeek-AI et al., 2025). Yet, explicit CoT often in- curs substantial inference cost due to long, verbose traces and the associated key–value (KV) cache growth, making deployment on memory- and compute-constrained devices difficult. Furthermore, CoT traces, especially those distilled from larger models, can inherit and amplify biases or contain plausible-sounding but fallacious logic, limiting their reliability.\\n\\nRecent studies show that the KV-caches underlying CoT are highly redundant and can be aggres- sively compressed with little to no loss in accuracy (Cai et al., 2025; Park et al., 2025), indicating that much of CoT’s signal resides in compressible structure rather than indispensable text. This observation suggests an alternative supervisory path: if the essential dynamics of reasoning live in the cache, perhaps models can be trained to internalize those dynamics without verbose traces at inference time. However, this compressed KV-cache presents a significant challenge for knowledge\\n\\n∗Equal contribution\\n\\n†Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.\\n\\n‡Work done during internship at Qualcomm AI Research.\\n\\nPreprint. Under Review.\\n\\ndistillation. As pruning decisions are often made independently per layer and attention head, the re- sulting compressed KV vectors lose their direct correspondence to specific input tokens, rendering conventional distillation schemes that match token activations or layer-wise hidden states ill-posed and non-trivial.\\n\\nLatent reasoning is a nascent but promising direction in which reasoning occurs within the model’s continuous latent space rather than being explicitly externalized (Hao et al., 2024; Su et al., 2025). Latent approaches promise efficiency by reducing token generation and KV-cache footprint, po- tentially closing the gap between strong reasoning performance and deployability in constrained settings. However, current latent reasoning methods struggle with the absence of direct supervision for internal thoughts, and successes are often reported in restricted setups; performance can degrade when training data contain long, natural-language-style traces that better reflect real-world reason- ing workloads. In particular, compared to shorter, template-like traces, models trained on longer, natural-language reasoning sequences exhibit more fragile internal readouts and weaker generaliza- tion (Shen et al., 2025; Wu et al., 2025).\\n\\nIn this work, we bridge these gaps by introducing a novel framework that, for the first time, suc- cessfully distills the rich, abstract knowledge from a compressed teacher KV-cache into a latent rea- soning student. We posit that the continuous, high-dimensional nature of latent thoughts provides a unique representational power that can absorb abstract cache structure that cannot be aligned at the token level. Concretely, our method is composed of three components: (i) the backbone that alter- nates between a teacher mode that consumes a full CoT to build per-layer, per-head KV-caches and a student mode that generates continuous latent thoughts; (ii) a redundancy- and importance-aware eviction module that compresses the teacher cache to the latent budget; (iii) and a KV-matching loss aligns the student’s per-step latent K and V to the compressed target throughout the stack. This yields a strong, stepwise internal supervision signal that teaches the student to “think like” a compact cache of its own explicit reasoning while preserving the inference-time efficiency of latent reason- ing. By supervising the latent trajectory directly in KV space, the approach bridges the gap between template-like latent traces and natural-language reasoning, yielding strong gains on natural-language datasets and scaling smoothly to larger backbones while retaining the efficiency benefits of latent inference. Our primary contributions are:\\n\\n• We are the first to demonstrate that knowledge can be successfully distilled from a com- pressed KV-cache via self-distillation, despite the cache’s head-wise, layer-wise eviction that destroys token correspondence.\\n\\n• We show that by using the compressed KV-cache as a rich, step-by-step supervision signal, we can effectively train latent reasoners to learn directly from natural language traces where prior methods struggle to extract meaningful improvements.\\n\\n• Through empirical evaluations, we show that our approach consistently outperforms strong latent baselines on natural language settings, exhibits smaller degradation when moving from equation-only to natural-language traces, and scales to larger backbones.\\n\\n2 BACKGROUND AND RELATED WORKS\\n\\nLatent Reasoning. Traditional reasoning LLMs often rely on generating explicit intermediate steps in language to solve complex reasoning tasks. Recent work shifts reasoning from discrete text tokens to latent continuous tokens, where models perform iterative computation internally with- out generating external text (Chen et al., 2025; Zhu et al., 2025). Early work validated the benefit of extra computation through unstructured means, such as learnable pause tokens (Goyal et al., 2024) or even semantically meaningless filler tokens (Pfau et al., 2024), which improved performance on rea- soning tasks by simply extending the model’s processing time implicitly. Building on this implicit- compute view, iCoT moves from explicit to implicit CoT via distillation (Deng et al., 2023) and curriculum (Deng et al., 2024), progressively removing CoT while aligning internal states around answer prediction. This allows the model to internalize reasoning without generating text rationales at inference. Coconut (Hao et al., 2024) introduces “continuous thought” by feeding the last hidden state directly as the next input embedding, showing breadth-first search–like parallel exploration and fewer thinking tokens versus CoT on logical reasoning tasks. Follow-ups refine supervision and training dynamics: CODI (Shen et al., 2025) compresses CoT into continuous representations via self-distillation that supervises endpoints rather than full trajectories, while PCCoT (Wu et al.,\\n\\nPreprint. Under Review.\\n\\ndistillation. As pruning decisions are often made independently per layer and attention head, the re- sulting compressed KV vectors lose their direct correspondence to specific input tokens, rendering conventional distillation schemes that match token activations or layer-wise hidden states ill-posed and non-trivial.\\n\\nLatent reasoning is a nascent but promising direction in which reasoning occurs within the model’s continuous latent space rather than being explicitly externalized (Hao et al., 2024; Su et al., 2025). Latent approaches promise efficiency by reducing token generation and KV-cache footprint, po- tentially closing the gap between strong reasoning performance and deployability in constrained settings. However, current latent reasoning methods struggle with the absence of direct supervision for internal thoughts, and successes are often reported in restricted setups; performance can degrade when training data contain long, natural-language-style traces that better reflect real-world reason- ing workloads. In particular, compared to shorter, template-like traces, models trained on longer, natural-language reasoning sequences exhibit more fragile internal readouts and weaker generaliza- tion (Shen et al., 2025; Wu et al., 2025).\\n\\nIn this work, we bridge these gaps by introducing a novel framework that, for the first time, suc- cessfully distills the rich, abstract knowledge from a compressed teacher KV-cache into a latent rea- soning student. We posit that the continuous, high-dimensional nature of latent thoughts provides a unique representational power that can absorb abstract cache structure that cannot be aligned at the token level. Concretely, our method is composed of three components: (i) the backbone that alter- nates between a teacher mode that consumes a full CoT to build per-layer, per-head KV-caches and a student mode that generates continuous latent thoughts; (ii) a redundancy- and importance-aware eviction module that compresses the teacher cache to the latent budget; (iii) and a KV-matching loss aligns the student’s per-step latent K and V to the compressed target throughout the stack. This yields a strong, stepwise internal supervision signal that teaches the student to “think like” a compact cache of its own explicit reasoning while preserving the inference-time efficiency of latent reason- ing. By supervising the latent trajectory directly in KV space, the approach bridges the gap between template-like latent traces and natural-language reasoning, yielding strong gains on natural-language datasets and scaling smoothly to larger backbones while retaining the efficiency benefits of latent inference. Our primary contributions are:\\n\\n• We are the first to demonstrate that knowledge can be successfully distilled from a com- pressed KV-cache via self-distillation, despite the cache’s head-wise, layer-wise eviction that destroys token correspondence.\\n\\n• We show that by using the compressed KV-cache as a rich, step-by-step supervision signal, we can effectively train latent reasoners to learn directly from natural language traces where prior methods struggle to extract meaningful improvements.\\n\\n• Through empirical evaluations, we show that our approach consistently outperforms strong latent baselines on natural language settings, exhibits smaller degradation when moving from equation-only to natural-language traces, and scales to larger backbones.\\n\\n2 BACKGROUND AND RELATED WORKS\\n\\nLatent Reasoning. Traditional reasoning LLMs often rely on generating explicit intermediate steps in language to solve complex reasoning tasks. Recent work shifts reasoning from discrete text tokens to latent continuous tokens, where models perform iterative computation internally with- out generating external text (Chen et al., 2025; Zhu et al., 2025). Early work validated the benefit of extra computation through unstructured means, such as learnable pause tokens (Goyal et al., 2024) or even semantically meaningless filler tokens (Pfau et al., 2024), which improved performance on rea- soning tasks by simply extending the model’s processing time implicitly. Building on this implicit- compute view, iCoT moves from explicit to implicit CoT via distillation (Deng et al., 2023) and curriculum (Deng et al., 2024), progressively removing CoT while aligning internal states around answer prediction. This allows the model to internalize reasoning without generating text rationales at inference. Coconut (Hao et al., 2024) introduces “continuous thought” by feeding the last hidden state directly as the next input embedding, showing breadth-first search–like parallel exploration and fewer thinking tokens versus CoT on logical reasoning tasks. Follow-ups refine supervision and training dynamics: CODI (Shen et al., 2025) compresses CoT into continuous representations via self-distillation that supervises endpoints rather than full trajectories, while PCCoT (Wu et al.,\\n\\nPreprint. Under Review.\\n\\n(a)\\n\\n(b)\\n\\n()\\n\\nFigure 1: We propose KAVA , a latent reasoning model with KV-cache distillation loss. (a) Overview of our proposed compressed KV-cache distilled latent reasoning framework. (b) Teacher builds full KV-cache from a ground-truth CoT trace; a compression module produces a compressed cache to match the length of the latent trace; (c) a latent-reasoning student generates continuous thoughts zt and is trained to match compressed teacher KV at each layer/step via KV distillation.\\n\\n2025) parallelizes latent updates with Jacobi-style iterations to refine multiple continuous thoughts in tandem. In contrast to endpoint- or token-level supervision, our proposed approach distills a CoT teacher’s compressed KV-cache into the student’s latent trajectory, providing stepwise internal guid- ance that bridges the supervision gap in continuous-token reasoning without relying on explicit CoT text.\\n\\nComplementary directions emphasize soft or hybrid traces: SoftCoT (Xu et al., 2025) injects soft thought tokens projected into the backbone’s representation space to improve reasoning without altering hard-token generation, and Token Assorted (Su et al., 2025) mixes latent discrete tokens produced by a VQ-VAE with text tokens to shorten traces while maintaining accuracy. Our method is orthogonal, addressing the core challenge in latent reasoning, the absence of a direct supervision signal for these internal thoughts.\\n\\nKV-cache Compression. KV-cache compression for reasoning focuses on trimming long, redun- dant thinking while preserving accuracy and throughput. R-KV (Cai et al., 2025) compresses on- the-fly by jointly scoring importance and redundancy to retain near-full performance with roughly 10–30% of the KV-cache on math reasoning, while KeyDiff (Park et al., 2025) offers a key- similarity–based eviction rule that preserves salient semantics under tight budgets. Other strate- gies such HeadKV (Fu et al., 2025), PyramidKV (Cai et al., 2024), LESS (Dong et al., 2024), and Eigen Attention (Saxena et al., 2024), provide complementary reductions via head selection, hierarchical/pyramidal retention, importance-aware mixed-precision, and low-rank attention, yield- ing robust long-context and reasoning behavior. KV-Distill (Chari et al., 2025) instead learns a lightweight adaptor that compresses long-context KV-caches and trains a compressed-cache student to match a full-cache teacher via output-level KL alignment. In contrast, our proposed approach treats the teacher’s compressed KV-cache as supervision targets and distills them directly into the student’s latent reasoning steps, aligning internal KV trajectories across the thinking process and directly addressing the lack of supervision for continuous thoughts.\\n\\n3 KAVA: KV-CACHE DISTILLATION FOR LATENT REASONING\\n\\n3.1 OVERVIEW\\n\\nWe will split the common chat template into three parts named question Q, reasoning trace C and answer A, with NQ, NC and NA token correspondingly. Consider an autoregressive generative model (LLM) that predicts each subsequent token conditioned on all preceding tokens. Latent rea- soning introduces a set of unobserved intermediate steps, Z = {zi}M i=1, which act as a substitute for the explicit reasoning trace C (see Fig. 2). The latent reasoning sequence begins with a special\\n\\nPreprint. Under Review.\\n\\n(a)\\n\\n(b)\\n\\n()\\n\\nFigure 1: We propose KAVA , a latent reasoning model with KV-cache distillation loss. (a) Overview of our proposed compressed KV-cache distilled latent reasoning framework. (b) Teacher builds full KV-cache from a ground-truth CoT trace; a compression module produces a compressed cache to match the length of the latent trace; (c) a latent-reasoning student generates continuous thoughts zt and is trained to match compressed teacher KV at each layer/step via KV distillation.\\n\\n2025) parallelizes latent updates with Jacobi-style iterations to refine multiple continuous thoughts in tandem. In contrast to endpoint- or token-level supervision, our proposed approach distills a CoT teacher’s compressed KV-cache into the student’s latent trajectory, providing stepwise internal guid- ance that bridges the supervision gap in continuous-token reasoning without relying on explicit CoT text.\\n\\nComplementary directions emphasize soft or hybrid traces: SoftCoT (Xu et al., 2025) injects soft thought tokens projected into the backbone’s representation space to improve reasoning without altering hard-token generation, and Token Assorted (Su et al., 2025) mixes latent discrete tokens produced by a VQ-VAE with text tokens to shorten traces while maintaining accuracy. Our method is orthogonal, addressing the core challenge in latent reasoning, the absence of a direct supervision signal for these internal thoughts.\\n\\nKV-cache Compression. KV-cache compression for reasoning focuses on trimming long, redun- dant thinking while preserving accuracy and throughput. R-KV (Cai et al., 2025) compresses on- the-fly by jointly scoring importance and redundancy to retain near-full performance with roughly 10–30% of the KV-cache on math reasoning, while KeyDiff (Park et al., 2025) offers a key- similarity–based eviction rule that preserves salient semantics under tight budgets. Other strate- gies such HeadKV (Fu et al., 2025), PyramidKV (Cai et al., 2024), LESS (Dong et al., 2024), and Eigen Attention (Saxena et al., 2024), provide complementary reductions via head selection, hierarchical/pyramidal retention, importance-aware mixed-precision, and low-rank attention, yield- ing robust long-context and reasoning behavior. KV-Distill (Chari et al., 2025) instead learns a lightweight adaptor that compresses long-context KV-caches and trains a compressed-cache student to match a full-cache teacher via output-level KL alignment. In contrast, our proposed approach treats the teacher’s compressed KV-cache as supervision targets and distills them directly into the student’s latent reasoning steps, aligning internal KV trajectories across the thinking process and directly addressing the lack of supervision for continuous thoughts.\\n\\n3 KAVA: KV-CACHE DISTILLATION FOR LATENT REASONING\\n\\n3.1 OVERVIEW\\n\\nWe will split the common chat template into three parts named question Q, reasoning trace C and answer A, with NQ, NC and NA token correspondingly. Consider an autoregressive generative model (LLM) that predicts each subsequent token conditioned on all preceding tokens. Latent rea- soning introduces a set of unobserved intermediate steps, Z = {zi}M i=1, which act as a substitute for the explicit reasoning trace C (see Fig. 2). The latent reasoning sequence begins with a special\\n\\nPreprint. Under Review.\\n\\nFigure 2: Graphical model of the latent reasoning generative model. The ques- tion prompt is used to generate continu- ous latent thought Z. The answer tokens are generated from the question and la- tent reasoning trace.\\n\\nFigure 3: During training the student predicts the an- swer using latent tokens, teacher has the access to the full reasoning trace, and KV matching distills the in- formation from the full to the latent CoT.\\n\\ntoken <bot>, continues with M continuous tokens, and terminates with <eot>, marking the end of the reasoning stage. During inference, these continuous latent tokens are generated by the same autoregressive model, bypassing the mapping of the embeddings to hard tokens. Instead, a (train- able) projection layer maps these continuous embeddings to the input embeddings that are used to predict the next token. We use the terms latent CoT and Continuous CoT (CCoT) interchangeably throughout the paper to refer to the tokens from Z.\\n\\nTraining Objective. Unlike chain-of-thought (CoT) reasoning traces, latent reasoning lacks direct supervision because latent traces are unobserved during training. Consequently, its performance is typically inferior to models trained with full CoT supervision (Deng et al., 2023; 2024). To address this, we leverage the observed reasoning traces C to guide latent reasoning during training, as illustrated in Fig. 3. This guidance is realized through distillation from teacher to student. Following Shen et al. (2025), we adopt a self-supervised framework in which the same model learns from explicit reasoning traces (as the teacher) as well as latent tokens (as the student).\\n\\nWe introduce KAVA, model with a novel objective, KV-cache distillation, to transfer relevant infor- mation from the teacher’s reasoning trace to the student. An overview of this approach is depicted in Figure 1, with details provided in Section 3.2.\\n\\nOur proposed KV-cache distillation loss is complementary to the CODI distillation loss introduced by Shen et al. (2025). CODI uses a single distillation token and matches its hidden activations between the teacher and the student models:\\n\\nwhere L is the total number of layers in the model, sg is a stop-gradient operator and hl are model’s hidden activation from layer l. The distillation token is chosen as the one preceding the answer. For example, if the answer is formatted as \"The answer is:5\", the semicolon \":\" is used as the distillation token.\\n\\nWe combine KV-cache distillation with the CODI self-distillation to add a richer supervision signal to the latent reasoning trace. The total training objective is the following:\\n\\nwhere logp(·) stands for cross-entropy loss, α1 and α2 are the hyperparameters that are used to balance the distillation terms, NA and NC denote number of tokens in the answer and CoT trace.\\n\\nParallel Decoding. Since latent tokens are generated sequentially, they do not allow for parallel decoding during training, which limits scalability. To mitigate this issue, we use Jacobi iteration over latent tokens to improve training and inference efficiency as proposed by Wu et al. (2025). Instead of generating latent tokens one by one during training PCCoT performs iterative updates of all tokens simultaneously for a predefined number of iterations T. PCCoT uses T < M, so that total number of forward passes is reduced from the number of latent tokens M to the number of iterations T. For T = M the method recovers the CODI explicitly and for T = 0 it corresponds to the Pause Token (Goyal et al., 2024).\\n\\n(1)\\n\\n(2)\\n\\nPreprint. Under Review.\\n\\nFigure 2: Graphical model of the latent reasoning generative model. The ques- tion prompt is used to generate continu- ous latent thought Z. The answer tokens are generated from the question and la- tent reasoning trace.\\n\\nFigure 3: During training the student predicts the an- swer using latent tokens, teacher has the access to the full reasoning trace, and KV matching distills the in- formation from the full to the latent CoT.\\n\\ntoken <bot>, continues with M continuous tokens, and terminates with <eot>, marking the end of the reasoning stage. During inference, these continuous latent tokens are generated by the same autoregressive model, bypassing the mapping of the embeddings to hard tokens. Instead, a (train- able) projection layer maps these continuous embeddings to the input embeddings that are used to predict the next token. We use the terms latent CoT and Continuous CoT (CCoT) interchangeably throughout the paper to refer to the tokens from Z.\\n\\nTraining Objective. Unlike chain-of-thought (CoT) reasoning traces, latent reasoning lacks direct supervision because latent traces are unobserved during training. Consequently, its performance is typically inferior to models trained with full CoT supervision (Deng et al., 2023; 2024). To address this, we leverage the observed reasoning traces C to guide latent reasoning during training, as illustrated in Fig. 3. This guidance is realized through distillation from teacher to student. Following Shen et al. (2025), we adopt a self-supervised framework in which the same model learns from explicit reasoning traces (as the teacher) as well as latent tokens (as the student).\\n\\nWe introduce KAVA, model with a novel objective, KV-cache distillation, to transfer relevant infor- mation from the teacher’s reasoning trace to the student. An overview of this approach is depicted in Figure 1, with details provided in Section 3.2.\\n\\nOur proposed KV-cache distillation loss is complementary to the CODI distillation loss introduced by Shen et al. (2025). CODI uses a single distillation token and matches its hidden activations between the teacher and the student models:\\n\\nwhere L is the total number of layers in the model, sg is a stop-gradient operator and hl are model’s hidden activation from layer l. The distillation token is chosen as the one preceding the answer. For example, if the answer is formatted as \"The answer is:5\", the semicolon \":\" is used as the distillation token.\\n\\nWe combine KV-cache distillation with the CODI self-distillation to add a richer supervision signal to the latent reasoning trace. The total training objective is the following:\\n\\nwhere logp(·) stands for cross-entropy loss, α1 and α2 are the hyperparameters that are used to balance the distillation terms, NA and NC denote number of tokens in the answer and CoT trace.\\n\\nParallel Decoding. Since latent tokens are generated sequentially, they do not allow for parallel decoding during training, which limits scalability. To mitigate this issue, we use Jacobi iteration over latent tokens to improve training and inference efficiency as proposed by Wu et al. (2025). Instead of generating latent tokens one by one during training PCCoT performs iterative updates of all tokens simultaneously for a predefined number of iterations T. PCCoT uses T < M, so that total number of forward passes is reduced from the number of latent tokens M to the number of iterations T. For T = M the method recovers the CODI explicitly and for T = 0 it corresponds to the Pause Token (Goyal et al., 2024).\\n\\n(1)\\n\\n(2)\\n\\nPreprint. Under Review.\\n\\n3.2 KV-CACHE DISTILLATION\\n\\nTo provide an additional supervision signal from the full chain-of-thought (CoT) trace to the la- tent reasoning process, KAVA uses a distillation method based on matching the respective key- value (KV) caches (last term in Eq. 2). We apply redundancy-aware KV-cache compression to the teacher’s cache prior to distillation. This encourages the student to generate compressed and abstract representations, while preserving crucial reasoning information from the CoT trace.\\n\\nWe first extract the KV-cache for both the explicit reasoning trace (teacher) and the latent thought (student). Each cache consists of key and value tensors for every token i, layer l ∈ (1,...,L), and attention head h ∈ (1,...,H) of the transformer:\\n\\nwhere t stands for teacher and s for the student.We use the last Jacobi iteration T to extract the KV-cache of the student.\\n\\nAddressing the Length Mismatch. The teacher cache ( K V and student cache (Kj, V) dif- fer in sequence length, since M < Nc¢. To align them wh enforcing compression, we apply d obtain a compressed teacher redundancy-aware KV eviction (Park et al| [2025]|Cai et al} cache K, y € RMHxLxd gpecifically, we adapt R-KV C et al e to select the top M ih KV-pairs (see App based on a combined redundancy—importance score\\n\\nwhere λ is a hyperparameter controlling the balance between redundancy and importance. The evic- tion method is only applied during training, since the student is distilled to generate the compressed KV-cache. Since eviction method is not applied during inference, we leverage the answer tokens from the training data for the importance score computation. For each layer and head, we compute the attention score using the teacher’s keys K·,h,l ∈ RNC×d and queries corresponding to the answer t tokens tokens Q·,h,l ∈ RNA×d:\\n\\n√\\n\\nThe importance score is then aggregated over all answer tokens1:\\n\\nNote that this computation incurs negligible overhead, since the attention scores were computed during the teacher’s forward pass. Following R-KV2, we compute a redundancy score Ri,h,l as the average pairwise cosine similarity among all key vectors and normalize via softmax.\\n\\nFinally, we use the score values Si,h,l (Eq. 4) to select top-M keys (and their corresponding values) for each head and layer in the teacher’s KV-cache. Full details and pseudocode are provided in App. A.\\n\\nKV Matching. Independent KV-pair eviction across layers and heads alters the cache’s structure and contents, yet it remains usable by the original model (see Figure 1b). However, there no longer exists a correspondence between the resulting cache and hard tokens. For that reason, we cannot ap- ply standard ways of distillation, matching the activations of the teacher and student model. Instead, we propose distilling the keys and values directly.\\n\\nTo this end, we distill the latent reasoning cache to match the compressed teacher’s cache, effectively guiding the latent model to approximate the full reasoning process in a more efficient and abstract form. We combine the loss for the keys and values in equal weights to get the final term of Eq. 2:\\n\\n,\\n\\n1For the group-query attention setting multiple queries are sharing the same key-value pair. In this case we apply MaxPool operation over the group before computing the importance score.\\n\\n2Official R-KV implementation is available at https://github.com/Zefan-Cai/R-KV.\\n\\n(3)\\n\\n(4)\\n\\n(5)\\n\\n(6)\\n\\n(7)\\n\\nPreprint. Under Review.\\n\\n3.2 KV-CACHE DISTILLATION\\n\\nTo provide an additional supervision signal from the full chain-of-thought (CoT) trace to the la- tent reasoning process, KAVA uses a distillation method based on matching the respective key- value (KV) caches (last term in Eq. 2). We apply redundancy-aware KV-cache compression to the teacher’s cache prior to distillation. This encourages the student to generate compressed and abstract representations, while preserving crucial reasoning information from the CoT trace.\\n\\nWe first extract the KV-cache for both the explicit reasoning trace (teacher) and the latent thought (student). Each cache consists of key and value tensors for every token i, layer l ∈ (1,...,L), and attention head h ∈ (1,...,H) of the transformer:\\n\\nwhere t stands for teacher and s for the student.We use the last Jacobi iteration T to extract the KV-cache of the student.\\n\\nAddressing the Length Mismatch. The teacher cache ( K V and student cache (Kj, V) dif- fer in sequence length, since M < Nc¢. To align them wh enforcing compression, we apply d obtain a compressed teacher redundancy-aware KV eviction (Park et al| [2025]|Cai et al} cache K, y € RMHxLxd gpecifically, we adapt R-KV C et al e to select the top M ih KV-pairs (see App based on a combined redundancy—importance score\\n\\nwhere λ is a hyperparameter controlling the balance between redundancy and importance. The evic- tion method is only applied during training, since the student is distilled to generate the compressed KV-cache. Since eviction method is not applied during inference, we leverage the answer tokens from the training data for the importance score computation. For each layer and head, we compute the attention score using the teacher’s keys K·,h,l ∈ RNC×d and queries corresponding to the answer t tokens tokens Q·,h,l ∈ RNA×d:\\n\\n√\\n\\nThe importance score is then aggregated over all answer tokens1:\\n\\nNote that this computation incurs negligible overhead, since the attention scores were computed during the teacher’s forward pass. Following R-KV2, we compute a redundancy score Ri,h,l as the average pairwise cosine similarity among all key vectors and normalize via softmax.\\n\\nFinally, we use the score values Si,h,l (Eq. 4) to select top-M keys (and their corresponding values) for each head and layer in the teacher’s KV-cache. Full details and pseudocode are provided in App. A.\\n\\nKV Matching. Independent KV-pair eviction across layers and heads alters the cache’s structure and contents, yet it remains usable by the original model (see Figure 1b). However, there no longer exists a correspondence between the resulting cache and hard tokens. For that reason, we cannot ap- ply standard ways of distillation, matching the activations of the teacher and student model. Instead, we propose distilling the keys and values directly.\\n\\nTo this end, we distill the latent reasoning cache to match the compressed teacher’s cache, effectively guiding the latent model to approximate the full reasoning process in a more efficient and abstract form. We combine the loss for the keys and values in equal weights to get the final term of Eq. 2:\\n\\n,\\n\\n1For the group-query attention setting multiple queries are sharing the same key-value pair. In this case we apply MaxPool operation over the group before computing the importance score.\\n\\n2Official R-KV implementation is available at https://github.com/Zefan-Cai/R-KV.\\n\\n(3)\\n\\n(4)\\n\\n(5)\\n\\n(6)\\n\\n(7)\\n\\nPreprint. Under Review.\\n\\nTable 1: Test accuracy on in-distribution test dataset and zero-shot evaluation on out-of-distribution datasets. We use † to denote results copied from Shen et al. (2025) and Wu et al. (2025). We consider full CoT as an upper bound on the performance and denote best latent reasoning method in bold and second-best with the line. We denote out method as KAVA .\\n\\nMethod GSM8k-AUG GSM8k-AUG-NL GSM8k GSM8k-Hard SVAMP GSM8k GSM8k-Hard SVAMP QWEN2.5 - 0.5B - INSTRUCT FULL COT 50.6 12.6 54.3 48.5 12.6 57.3 NO-COT 31.5 7.4 34.5 31.5 7.4 34.5 CODI 37.5 8.1 47 20.2 4.9 33.3 PCCOT 20.5 4.1 33 19.1 4.2 30.2 KAVA (ours) 46.9 (1.4) 10.8 (0.1) 50.6 (0.4) 44.4 (1.8) 10.2 (0.4) 46.5 (0.1) LLAMA3.2 - 1B - INSTRUCT FULL COT 61.6† 15.6† 66.7† 53.2 13.3 62.9 NO-COT 30.9† 7.1† 44.1† 33.1 7.7 41.4 ICOT 19.0† 4.4† 40.9† 15.2† - - COCONUT 45.3† 9.9† 48.8† 27.2† - - CODI 55.6† 12.8† 61.1† 49.7† - - PCCOT 53.35† (0.18) - - 50.72† (1.39) - - KAVA (ours) 56.5 (0.4) 12.7 (0.1) 58.9 (0.5) 55.7 (0.4) 12.8 (0.2) 58.6 (0.3) LLAMA3.2 - 3B - INSTRUCT FULL COT 73.2 21.6 78.0 68.4 20.5 77.6 NO-COT 41.7 10.5 56.9 41.7 10.5 56.9 CODI 61.0 15.0 72.4 55.9 13.6 70.1 PCCOT 54.7 13.5 69.5 47.6 11.0 65.2 KAVA (ours) 65.7 15.2 72.7 60.0 14.8 66.1\\n\\nwhere ∥ · ∥p denotes an Lp-norm. That is, we have L1 loss for p = 1 and MSE loss for p = 2. Note, that we first generate the whole student sequence with Jacobi iterations and then perform the distillation.\\n\\n4 EXPERIMENTS\\n\\n4.1 SETUP\\n\\nWe follow the experimental setup of Shen et al. (2025) and Wu et al. (2025) and extend the evaluation to more LLM families. Below we discuss the setup in more detail.\\n\\nModel. We conduct experiments using the pretrained LLaMA3.2-1b-Instruct, LLaMA3.2-3b-Instruct and Qwen2.5-0.5b-Instruct (Grattafiori et al., 2024; Team, 2024) models and fine-tune them using LoRA (Hu et al., 2022). We follow Shen et al. (2025) and Wu et al. (2025) by using the same LoRA setup (rank 128 with alpha value 32 and dropout 0.1) for all the experiments. We employ PCCoT, the approach proposed by Wu et al. (2025), to generate latent thoughts; where 24 continuous latent tokens are generated in parallel with 3 iterations.\\n\\nWe fine-tune the models on two datasets: GSM8k-AUG, GSM8k-AUG-NL (Deng et al., 2023). Both datasets are augmented versions GSM8k (Cobbe et al., 2021), containing 385k training ex- amples, with traces generated by GPT-4. GSM8k-AUG is then further processed by keeping only equations and removing all natural language from the traces. We provide a detailed description of the datasets in Appendix B. For in-distribution evaluation, we assess all models on the test split of the original GSM8k dataset (Cobbe et al., 2021). For zero-shot evaluation, we assess model gener- alization on two benchmarks: GSM8k-Hard (Gao et al., 2023) and SVAMP (Patel et al., 2021).\\n\\nHyperparameters. For our method, we conduct a hyperameter sweep over the learning rate, KV- cache distillation loss coefficient (α2), Lp norm of the loss and the normalization method (layer-wise loss normalization or none). We choose the best-performing model on validation and run this setting with three random seeds. We report all hyperparameters in Appendix C.\\n\\nPreprint. Under Review.\\n\\nTable 1: Test accuracy on in-distribution test dataset and zero-shot evaluation on out-of-distribution datasets. We use † to denote results copied from Shen et al. (2025) and Wu et al. (2025). We consider full CoT as an upper bound on the performance and denote best latent reasoning method in bold and second-best with the line. We denote out method as KAVA .\\n\\nMethod GSM8k-AUG GSM8k-AUG-NL GSM8k GSM8k-Hard SVAMP GSM8k GSM8k-Hard SVAMP QWEN2.5 - 0.5B - INSTRUCT FULL COT 50.6 12.6 54.3 48.5 12.6 57.3 NO-COT 31.5 7.4 34.5 31.5 7.4 34.5 CODI 37.5 8.1 47 20.2 4.9 33.3 PCCOT 20.5 4.1 33 19.1 4.2 30.2 KAVA (ours) 46.9 (1.4) 10.8 (0.1) 50.6 (0.4) 44.4 (1.8) 10.2 (0.4) 46.5 (0.1) LLAMA3.2 - 1B - INSTRUCT FULL COT 61.6† 15.6† 66.7† 53.2 13.3 62.9 NO-COT 30.9† 7.1† 44.1† 33.1 7.7 41.4 ICOT 19.0† 4.4† 40.9† 15.2† - - COCONUT 45.3† 9.9† 48.8† 27.2† - - CODI 55.6† 12.8† 61.1† 49.7† - - PCCOT 53.35† (0.18) - - 50.72† (1.39) - - KAVA (ours) 56.5 (0.4) 12.7 (0.1) 58.9 (0.5) 55.7 (0.4) 12.8 (0.2) 58.6 (0.3) LLAMA3.2 - 3B - INSTRUCT FULL COT 73.2 21.6 78.0 68.4 20.5 77.6 NO-COT 41.7 10.5 56.9 41.7 10.5 56.9 CODI 61.0 15.0 72.4 55.9 13.6 70.1 PCCOT 54.7 13.5 69.5 47.6 11.0 65.2 KAVA (ours) 65.7 15.2 72.7 60.0 14.8 66.1\\n\\nwhere ∥ · ∥p denotes an Lp-norm. That is, we have L1 loss for p = 1 and MSE loss for p = 2. Note, that we first generate the whole student sequence with Jacobi iterations and then perform the distillation.\\n\\n4 EXPERIMENTS\\n\\n4.1 SETUP\\n\\nWe follow the experimental setup of Shen et al. (2025) and Wu et al. (2025) and extend the evaluation to more LLM families. Below we discuss the setup in more detail.\\n\\nModel. We conduct experiments using the pretrained LLaMA3.2-1b-Instruct, LLaMA3.2-3b-Instruct and Qwen2.5-0.5b-Instruct (Grattafiori et al., 2024; Team, 2024) models and fine-tune them using LoRA (Hu et al., 2022). We follow Shen et al. (2025) and Wu et al. (2025) by using the same LoRA setup (rank 128 with alpha value 32 and dropout 0.1) for all the experiments. We employ PCCoT, the approach proposed by Wu et al. (2025), to generate latent thoughts; where 24 continuous latent tokens are generated in parallel with 3 iterations.\\n\\nWe fine-tune the models on two datasets: GSM8k-AUG, GSM8k-AUG-NL (Deng et al., 2023). Both datasets are augmented versions GSM8k (Cobbe et al., 2021), containing 385k training ex- amples, with traces generated by GPT-4. GSM8k-AUG is then further processed by keeping only equations and removing all natural language from the traces. We provide a detailed description of the datasets in Appendix B. For in-distribution evaluation, we assess all models on the test split of the original GSM8k dataset (Cobbe et al., 2021). For zero-shot evaluation, we assess model gener- alization on two benchmarks: GSM8k-Hard (Gao et al., 2023) and SVAMP (Patel et al., 2021).\\n\\nHyperparameters. For our method, we conduct a hyperameter sweep over the learning rate, KV- cache distillation loss coefficient (α2), Lp norm of the loss and the normalization method (layer-wise loss normalization or none). We choose the best-performing model on validation and run this setting with three random seeds. We report all hyperparameters in Appendix C.\\n\\nPreprint. Under Review.\\n\\nTable 2: We measure the efficiency of different reasoning model by the average number of forward passes required to generate the reasoning trace and answer. We use † to denote results copied from Shen et al. (2025) and Wu et al. (2025). We report the improvement in efficiency compared to the Full CoT in (parentheses).\\n\\nMethod GSM8k-AUG GSM8k-AUG-NL GSM8k GSM8k-Hard SVAMP GSM8k GSM8k-Hard SVAMP QWEN2.5 - 0.5B - INSTRUCT FULL COT 40.4 59.6 23.3 82.4 105.2 44.9 NO-COT/ ICOT 7.4 10.1 7.0 7.4 10.1 7.0 CODI 14.4 20.7 14.1 14.0 19.0 13.4 KAVA (ours) 9.5 (-76%) 13.3 (-78%) 8.9 (-62%) 9.2 (-89%) 13.5 (-87%) 9.0 (-80%) LLAMA3.2 - 1B - INSTRUCT FULL COT 65† - - 71.9 80.2 40.6 NO-COT / ICOT - - - 6.2 7.3 6.2 CODI 9† - - - - - COCONUT 9† - - - - - KAVA (ours) 6.9 (-89%) 9.1 6.5 7 (-90%) 10 (-88%) 6.4 (-86%) LLAMA3.2 - 3B - INSTRUCT FULL COT 31.6 40.3 17.0 75.2 32.9 38.3 NO-COT / ICOT 6.1 7.4 6.1 6.1 7.4 6.1 CODI 11.5 14.2 11.0 11.1 13.1 10.7 KAVA (ours) 6.4 (-80%) 8.2 (-80%) 6 (-65%) 6 (-92%) 7.9 (-76%) 5.7 (-85%)\\n\\nWe report the results of baseline approaches from Shen et al. (2025) and Wu et al. (2025) where possible. For the models not used in prior work, we take the hyperparameters from LLaMA3.2-1b, sweep over learning rates and report the result for the best performing model. We compare our method to CODI (Shen et al., 2025), PCCoT (Wu et al., 2025), Implicit CoT (iCoT) (Deng et al., 2024) and Coconut (Hao et al., 2024). We report the Full CoT performance as an upper bound and No-CoT as a lower bound.\\n\\n4.2 RESULTS\\n\\nWe report the average performance with standard error in Table 1. KAVA consistently outperforms the baselines. Importantly, we observe that KAVA has a lower drop in performance when switching from artificial GSM8k-AUG to a more realistic GSM8k-AUG-NL dataset. In the latter scenario, compression of the Full CoT trace would be more substantial as the traces are considerably longer, while questions are kept the same. This demonstrates the better scalability of out approach.\\n\\nWe also measure the efficiency of the method by the number of forward passes a model makes to generate the reasoning trace and the answer, reported in Table 2. KAVA builds on top of PCCoT, where we only use T = 3 iterations (forward passes) to generate all the latent tokens. For that reason, we skip the PCCoT results in the table as they would be similar to ours. Our method achieves better efficiency than CoT, requiring between 62% and 92% fewer forward passes per question compared to Full CoT.\\n\\n4.3 ABLATION STUDIES\\n\\nWe select LLAMA3.2-1B-INSTRUCT to conduct ablation studies for our method. We run each experiment with three random seeds and report average test accuracy.\\n\\nModel Components. First, we study how different modeling choices influence the final perfor- mance. In Table 3 we report benchmark performance when trained without the distillation loss (Shen et al., 2025) or without projection layer. As can be seen, both components are quite crucial, but even without them the method considerably outperforms the no-CoT baseline.\\n\\nRemoving Last Step of the Trace. Following Shen et al. (2025); Wu et al. (2025) we remove the last step from the teacher’s reasoning trace. CODI demonstrates that this step is crucial for model performance, since otherwise the token that CODI chooses for distillation tends to be less informa- tive. In Table 4 we train our model (using both KV matching and distillation) and PCCoT (only distillation) on all steps. Performance of our method drops much lower, indicating that KV-cache\\n\\nPreprint. Under Review.\\n\\nTable 2: We measure the efficiency of different reasoning model by the average number of forward passes required to generate the reasoning trace and answer. We use † to denote results copied from Shen et al. (2025) and Wu et al. (2025). We report the improvement in efficiency compared to the Full CoT in (parentheses).\\n\\nMethod GSM8k-AUG GSM8k-AUG-NL GSM8k GSM8k-Hard SVAMP GSM8k GSM8k-Hard SVAMP QWEN2.5 - 0.5B - INSTRUCT FULL COT 40.4 59.6 23.3 82.4 105.2 44.9 NO-COT/ ICOT 7.4 10.1 7.0 7.4 10.1 7.0 CODI 14.4 20.7 14.1 14.0 19.0 13.4 KAVA (ours) 9.5 (-76%) 13.3 (-78%) 8.9 (-62%) 9.2 (-89%) 13.5 (-87%) 9.0 (-80%) LLAMA3.2 - 1B - INSTRUCT FULL COT 65† - - 71.9 80.2 40.6 NO-COT / ICOT - - - 6.2 7.3 6.2 CODI 9† - - - - - COCONUT 9† - - - - - KAVA (ours) 6.9 (-89%) 9.1 6.5 7 (-90%) 10 (-88%) 6.4 (-86%) LLAMA3.2 - 3B - INSTRUCT FULL COT 31.6 40.3 17.0 75.2 32.9 38.3 NO-COT / ICOT 6.1 7.4 6.1 6.1 7.4 6.1 CODI 11.5 14.2 11.0 11.1 13.1 10.7 KAVA (ours) 6.4 (-80%) 8.2 (-80%) 6 (-65%) 6 (-92%) 7.9 (-76%) 5.7 (-85%)\\n\\nWe report the results of baseline approaches from Shen et al. (2025) and Wu et al. (2025) where possible. For the models not used in prior work, we take the hyperparameters from LLaMA3.2-1b, sweep over learning rates and report the result for the best performing model. We compare our method to CODI (Shen et al., 2025), PCCoT (Wu et al., 2025), Implicit CoT (iCoT) (Deng et al., 2024) and Coconut (Hao et al., 2024). We report the Full CoT performance as an upper bound and No-CoT as a lower bound.\\n\\n4.2 RESULTS\\n\\nWe report the average performance with standard error in Table 1. KAVA consistently outperforms the baselines. Importantly, we observe that KAVA has a lower drop in performance when switching from artificial GSM8k-AUG to a more realistic GSM8k-AUG-NL dataset. In the latter scenario, compression of the Full CoT trace would be more substantial as the traces are considerably longer, while questions are kept the same. This demonstrates the better scalability of out approach.\\n\\nWe also measure the efficiency of the method by the number of forward passes a model makes to generate the reasoning trace and the answer, reported in Table 2. KAVA builds on top of PCCoT, where we only use T = 3 iterations (forward passes) to generate all the latent tokens. For that reason, we skip the PCCoT results in the table as they would be similar to ours. Our method achieves better efficiency than CoT, requiring between 62% and 92% fewer forward passes per question compared to Full CoT.\\n\\n4.3 ABLATION STUDIES\\n\\nWe select LLAMA3.2-1B-INSTRUCT to conduct ablation studies for our method. We run each experiment with three random seeds and report average test accuracy.\\n\\nModel Components. First, we study how different modeling choices influence the final perfor- mance. In Table 3 we report benchmark performance when trained without the distillation loss (Shen et al., 2025) or without projection layer. As can be seen, both components are quite crucial, but even without them the method considerably outperforms the no-CoT baseline.\\n\\nRemoving Last Step of the Trace. Following Shen et al. (2025); Wu et al. (2025) we remove the last step from the teacher’s reasoning trace. CODI demonstrates that this step is crucial for model performance, since otherwise the token that CODI chooses for distillation tends to be less informa- tive. In Table 4 we train our model (using both KV matching and distillation) and PCCoT (only distillation) on all steps. Performance of our method drops much lower, indicating that KV-cache\\n\\nPreprint. Under Review.\\n\\nTable 4: Test accuracy on GSM8k dataset when the teacher is trained on all the steps.\\n\\nLKD LKV Drop Last All Steps ✓ ✓ ✓ ✗ 56.5 (0.4) 53.35 (0.18) 51.2 (0.8) 47.2 (2.9)\\n\\nFigure 6: Test accuracy (%) of KAVA with different number of iterations and latent tokens.\\n\\nTable 3: Test accuracy on GSM8k dataset without pro- jection layer and distillation loss (α1 = 0).\\n\\nLKD PRJ. GSM8k GSM-Hard SVAMP ✓ ✗ ✓ ✓ ✓ ✗ 56.5 (0.4) 52.8 (0.1) 52.2 (0.6) 12.7 (0.1) 12.2 (0.1) 12.3 (0.2) 58.9 (0.5) 56.2 (0.2) 58.3 (0.3)\\n\\nFigure 4: Test accuracy (%) of KAVA for different KV match- ing coefficient and loss.\\n\\nFigure 5: Test accuracy (%) of KAVA with different eviction methods.\\n\\ndistillation loss compensates for the lack of usefulness of a distillation token in a fully automatic manner.\\n\\nKV Loss Sensitivity. Matching keys and values of the KV-cache is a non-standard way of dis- tillation. Therefore, we study the model sensitivity to the distillation loss type and coefficient. In Figure 4 we plot the test accuracy for two losses and three different coefficients. The model per- forms consistently better with L1 loss when trained on GSM8k-AUG and with Llama-1b. However, we observed that better performance may be achieved when using MSE loss on other datasets (see Appendix C for the detailed hyperparameters used for all models and datasets).\\n\\nKV Eviction. We follow Cai et al. (2025) in using λ = 0.1 (see Eq. 4) in R-KV eviction for all the experiments. As an ablation study we consider the two extremes: cosine-only (λ = 0) and attention-only (λ = 1). These cases correspond to choosing the keys and values based on diversity or importance only. Furthermore, we use a simple baseline of cropping the full CoT trace from the right, that is we only keep first M tokens of the teacher’s cache for distillation. We report the results in Figure 5. We observe that combining both attention-based and similarity-based criteria enhances the performance for both datasets.\\n\\nNumber of Tokens and Iterations. Similarly to Wu et al. (2025), we observe that the number of iterations can have a different impact on accuracy depending on the number of latent tokens (Fig. 6). For larger numbers of latents (12, 24) we observe reduced performance beyond a certain number of iterations.\\n\\n5 INTERPRETABILITY OF LATENT REASONING TRACES\\n\\n5.1 DECODING THE LATENT TRACE\\n\\nAlthough the latent CoT is not directly interpretable, one can still attempt to decode the reasoning trace from latent tokens. A straightforward approach is to project the final hidden state of the latent tokens via the language modeling head. An example of a decoded trace is shown in Table 5. More examples of the decoded traces are given in the Appendix E. Interestingly, the decoded latent trace is often identical to the trace generated by the teacher model, underlining the importance of the teacher guidance. In particular cases, as shown in the table, a reasoning step can be expressed in two equivalent forms (e.g. <<650*2=1300>> and <<2*650=1300>>). In regular CoT, this ambiguity is resolved after sampling a unique prefix of one of the variants, however, there is no explicit mechanism allowing for such resolution in a latent CoT. Nevertheless, the student arrives at the correct answer.\\n\\nPreprint. Under Review.\\n\\nTable 4: Test accuracy on GSM8k dataset when the teacher is trained on all the steps.\\n\\nLKD LKV Drop Last All Steps ✓ ✓ ✓ ✗ 56.5 (0.4) 53.35 (0.18) 51.2 (0.8) 47.2 (2.9)\\n\\nFigure 6: Test accuracy (%) of KAVA with different number of iterations and latent tokens.\\n\\nTable 3: Test accuracy on GSM8k dataset without pro- jection layer and distillation loss (α1 = 0).\\n\\nLKD PRJ. GSM8k GSM-Hard SVAMP ✓ ✗ ✓ ✓ ✓ ✗ 56.5 (0.4) 52.8 (0.1) 52.2 (0.6) 12.7 (0.1) 12.2 (0.1) 12.3 (0.2) 58.9 (0.5) 56.2 (0.2) 58.3 (0.3)\\n\\nFigure 4: Test accuracy (%) of KAVA for different KV match- ing coefficient and loss.\\n\\nFigure 5: Test accuracy (%) of KAVA with different eviction methods.\\n\\ndistillation loss compensates for the lack of usefulness of a distillation token in a fully automatic manner.\\n\\nKV Loss Sensitivity. Matching keys and values of the KV-cache is a non-standard way of dis- tillation. Therefore, we study the model sensitivity to the distillation loss type and coefficient. In Figure 4 we plot the test accuracy for two losses and three different coefficients. The model per- forms consistently better with L1 loss when trained on GSM8k-AUG and with Llama-1b. However, we observed that better performance may be achieved when using MSE loss on other datasets (see Appendix C for the detailed hyperparameters used for all models and datasets).\\n\\nKV Eviction. We follow Cai et al. (2025) in using λ = 0.1 (see Eq. 4) in R-KV eviction for all the experiments. As an ablation study we consider the two extremes: cosine-only (λ = 0) and attention-only (λ = 1). These cases correspond to choosing the keys and values based on diversity or importance only. Furthermore, we use a simple baseline of cropping the full CoT trace from the right, that is we only keep first M tokens of the teacher’s cache for distillation. We report the results in Figure 5. We observe that combining both attention-based and similarity-based criteria enhances the performance for both datasets.\\n\\nNumber of Tokens and Iterations. Similarly to Wu et al. (2025), we observe that the number of iterations can have a different impact on accuracy depending on the number of latent tokens (Fig. 6). For larger numbers of latents (12, 24) we observe reduced performance beyond a certain number of iterations.\\n\\n5 INTERPRETABILITY OF LATENT REASONING TRACES\\n\\n5.1 DECODING THE LATENT TRACE\\n\\nAlthough the latent CoT is not directly interpretable, one can still attempt to decode the reasoning trace from latent tokens. A straightforward approach is to project the final hidden state of the latent tokens via the language modeling head. An example of a decoded trace is shown in Table 5. More examples of the decoded traces are given in the Appendix E. Interestingly, the decoded latent trace is often identical to the trace generated by the teacher model, underlining the importance of the teacher guidance. In particular cases, as shown in the table, a reasoning step can be expressed in two equivalent forms (e.g. <<650*2=1300>> and <<2*650=1300>>). In regular CoT, this ambiguity is resolved after sampling a unique prefix of one of the variants, however, there is no explicit mechanism allowing for such resolution in a latent CoT. Nevertheless, the student arrives at the correct answer.\\n\\nPreprint. Under Review.\\n\\nTable 5: Decoding the latent thoughts. A validation prompt is used: “Mrs. Taylor bought two smart televisions that cost $650 each. If the total sales price had a 25% discount, how much did Mrs. Taylor pay for the two televisions?”. Latent thoughts 16-24 are not shown due to their limited semantic value. 3 tokens with the highest logits are shown for each latent thought. Tokens T1, T2, T3, T4, T5, T6, T7 stand for total, cost, dollars, discount, original, gross, and price respectively. Following CODI, the teacher is trained on traces omitting the last step.\\n\\nTopK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Answer GSM8K-Aug 1 650 * 2 = 130 0 >> << of 0 * * >> = = 2 2 + 650 * 650 >> . The . * % % = * 325 975 3 65 - 0 =$ 125 00 | <<( and k *. = 0 = 125 Teacher <<650*2=1300>><<1300*25/100=325>> 975 Golden <<650*2=1300>> <<1300*25/100=325>><<1300-325=975>> 975 GSM8K-Aug-NL 1 T1 of of 0 $ $ $ $ $ $ $ T4 T4 2 T2 T2 T2 T3 $ $ $ $ $ $ $ $ T4 975 3 T5 T7 was T6 was $ The , , Teacher The total cost of the two televisions is 2 x $650 = $1300 [...] $1300 x 25/100 = $325. 975 Golden The total cost of the two smart televisions is [...] $975 for the two smart televisions. 975\\n\\nModels trained on the GSM8k-AUG dataset tend to produce latent CoT’s that are easily inter- pretable. In contrast, models trained on the GSM8k-AUG-NL dataset resist this straightforward read-out method. We hypothesize that this is caused by the KV-cache distillation employed by KAVA —in a dataset with shorter traces, such as GSM8k-AUG, most of the time the KV-cache re- tains all of its content after eviction. On longer traces, such as the ones found in GSM8k-AUG-NL, not all content of the KV-cache is preserved, and, furthermore, each latent thought’s distillation tar- get may consist of keys and values originating from different tokens of the teacher’s CoT. This can prevent latent thought to hard token correspondence from arising.\\n\\n5.2 TEACHER-STUDENT KV-CACHE CORRESPONDENCE\\n\\nFigure 7: Cosine similarity of Keys in the latent CoT with Keys of the ground truth averaged across heads and layers. We use the same prompt and ground truth CoT as in Table 5.\\n\\nWe compute the cosine similarity of the keys and values in the latent CoT with (1) the ground truth KV-cache, and (2) the ground truth KV-cache after eviction. The results for Keys, averaged over attention heads and layers are presented in the Fig. 7. We observe that when comparing to the KV- cache after eviction, the similarities near the diagonal (x = y) tend to be higher, which is expected, as it is encouraged by the KV distillation. Furthermore, the values to the right of the diagonal are higher when comparing with the full CoT, which is desired, as this represents the compression of the original CoT (i.e. the key of some n-th latent token is similar to the key of an m-th hard token where n < m). The figure for Values and the full visualization of the similarities across layers and heads can be found in the Appendix D.\\n\\nPreprint. Under Review.\\n\\nTable 5: Decoding the latent thoughts. A validation prompt is used: “Mrs. Taylor bought two smart televisions that cost $650 each. If the total sales price had a 25% discount, how much did Mrs. Taylor pay for the two televisions?”. Latent thoughts 16-24 are not shown due to their limited semantic value. 3 tokens with the highest logits are shown for each latent thought. Tokens T1, T2, T3, T4, T5, T6, T7 stand for total, cost, dollars, discount, original, gross, and price respectively. Following CODI, the teacher is trained on traces omitting the last step.\\n\\nTopK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Answer GSM8K-Aug 1 650 * 2 = 130 0 >> << of 0 * * >> = = 2 2 + 650 * 650 >> . The . * % % = * 325 975 3 65 - 0 =$ 125 00 | <<( and k *. = 0 = 125 Teacher <<650*2=1300>><<1300*25/100=325>> 975 Golden <<650*2=1300>> <<1300*25/100=325>><<1300-325=975>> 975 GSM8K-Aug-NL 1 T1 of of 0 $ $ $ $ $ $ $ T4 T4 2 T2 T2 T2 T3 $ $ $ $ $ $ $ $ T4 975 3 T5 T7 was T6 was $ The , , Teacher The total cost of the two televisions is 2 x $650 = $1300 [...] $1300 x 25/100 = $325. 975 Golden The total cost of the two smart televisions is [...] $975 for the two smart televisions. 975\\n\\nModels trained on the GSM8k-AUG dataset tend to produce latent CoT’s that are easily inter- pretable. In contrast, models trained on the GSM8k-AUG-NL dataset resist this straightforward read-out method. We hypothesize that this is caused by the KV-cache distillation employed by KAVA —in a dataset with shorter traces, such as GSM8k-AUG, most of the time the KV-cache re- tains all of its content after eviction. On longer traces, such as the ones found in GSM8k-AUG-NL, not all content of the KV-cache is preserved, and, furthermore, each latent thought’s distillation tar- get may consist of keys and values originating from different tokens of the teacher’s CoT. This can prevent latent thought to hard token correspondence from arising.\\n\\n5.2 TEACHER-STUDENT KV-CACHE CORRESPONDENCE\\n\\nFigure 7: Cosine similarity of Keys in the latent CoT with Keys of the ground truth averaged across heads and layers. We use the same prompt and ground truth CoT as in Table 5.\\n\\nWe compute the cosine similarity of the keys and values in the latent CoT with (1) the ground truth KV-cache, and (2) the ground truth KV-cache after eviction. The results for Keys, averaged over attention heads and layers are presented in the Fig. 7. We observe that when comparing to the KV- cache after eviction, the similarities near the diagonal (x = y) tend to be higher, which is expected, as it is encouraged by the KV distillation. Furthermore, the values to the right of the diagonal are higher when comparing with the full CoT, which is desired, as this represents the compression of the original CoT (i.e. the key of some n-th latent token is similar to the key of an m-th hard token where n < m). The figure for Values and the full visualization of the similarities across layers and heads can be found in the Appendix D.\\n\\nPreprint. Under Review.\\n\\n6 CONCLUSION AND DISCUSSION\\n\\nWe introduce KAVA, a novel framework that bridges the supervision gap in latent reasoning by distilling knowledge from a teacher model’s compressed Key-Value (KV) cache. Our central con- tribution is the demonstration that a compressed KV-cache, despite losing direct token correspon- dence, can serve as a rich, stepwise supervisory signal for a latent reasoning student. By aligning the student’s latent trajectory with the teacher’s internal reasoning dynamics in KV space, KAVA over- comes the limitations of token-level distillation and the inefficiencies of verbose Chain-of-Thought (CoT) traces. KAVA consistently outperforms strong latent reasoning baselines, scales effectively to larger backbones, and shows robust performance on natural-language reasoning datasets where prior methods often struggle. While the advancement of latent reasoning is linked to the availability of large-scale training data to instill novel reasoning dynamics, our work establishes compressed KV-cache distillation as a scalable and effective supervision technique for developing efficient and powerful reasoning models.\\n\\nREFERENCES\\n\\nZefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Yucheng Li, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Junjie Hu, et al. Pyramidkv: Dynamic kv cache compression based on pyra- midal information funneling. arXiv preprint arXiv:2406.02069, 2024.\\n\\nZefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li- Wen Chang, Jiuxiang Gu, et al. R-kv: Redundancy-aware kv cache compression for training-free reasoning models acceleration. arXiv preprint arXiv:2505.24133, 2025.\\n\\nVivek Chari, Guanghui Qin, and Benjamin Van Durme. Kv-distill: Nearly lossless learnable context compression for llms. arXiv preprint arXiv:2503.10337, 2025.\\n\\nXinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: A comprehensive survey on latent chain-of-thought reasoning. arXiv preprint arXiv:2505.16782, 2025.\\n\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n\\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xi- aosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying\\n\\nPreprint. Under Review.\\n\\n6 CONCLUSION AND DISCUSSION\\n\\nWe introduce KAVA, a novel framework that bridges the supervision gap in latent reasoning by distilling knowledge from a teacher model’s compressed Key-Value (KV) cache. Our central con- tribution is the demonstration that a compressed KV-cache, despite losing direct token correspon- dence, can serve as a rich, stepwise supervisory signal for a latent reasoning student. By aligning the student’s latent trajectory with the teacher’s internal reasoning dynamics in KV space, KAVA over- comes the limitations of token-level distillation and the inefficiencies of verbose Chain-of-Thought (CoT) traces. KAVA consistently outperforms strong latent reasoning baselines, scales effectively to larger backbones, and shows robust performance on natural-language reasoning datasets where prior methods often struggle. While the advancement of latent reasoning is linked to the availability of large-scale training data to instill novel reasoning dynamics, our work establishes compressed KV-cache distillation as a scalable and effective supervision technique for developing efficient and powerful reasoning models.\\n\\nREFERENCES\\n\\nZefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Yucheng Li, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Junjie Hu, et al. Pyramidkv: Dynamic kv cache compression based on pyra- midal information funneling. arXiv preprint arXiv:2406.02069, 2024.\\n\\nZefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li- Wen Chang, Jiuxiang Gu, et al. R-kv: Redundancy-aware kv cache compression for training-free reasoning models acceleration. arXiv preprint arXiv:2505.24133, 2025.\\n\\nVivek Chari, Guanghui Qin, and Benjamin Van Durme. Kv-distill: Nearly lossless learnable context compression for llms. arXiv preprint arXiv:2503.10337, 2025.\\n\\nXinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: A comprehensive survey on latent chain-of-thought reasoning. arXiv preprint arXiv:2505.16782, 2025.\\n\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n\\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xi- aosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying\\n\\nPreprint. Under Review.\\n\\nTang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforce- ment learning, 2025. URL https://arxiv.org/abs/2501.12948.\\n\\nYuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stu- art Shieber. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023.\\n\\nYuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to inter- nalize cot step by step. arXiv preprint arXiv:2405.14838, 2024.\\n\\nHarry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, and Beidi Chen. Get more with less: Synthesizing recurrence with kv cache compression for efficient llm inference. ICML, 2024. URL https://openreview.net/forum?id=uhHDhVKFMW. In\\n\\nYu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao. Not all heads mat- ter: A head-level KV cache compression method with integrated retrieval and reasoning. The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=FJFVmeXusW. In\\n\\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764–10799. PMLR, 2023.\\n\\nSachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=ph04CRkPdC.\\n\\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\\n\\nShibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in a continuous latent space, 2024. URL https: //arxiv.org/abs/2412.06769.\\n\\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.\\n\\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xu- ancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024. URL https://arxiv.org/abs/2409.12186.\\n\\nJunyoung Park, Dalton Jones, Matthew J Morse, Raghavv Goel, Mingu Lee, and Chris Lott. Keydiff: Key similarity-based kv cache eviction for long-context llm inference in resource-constrained environments. arXiv preprint arXiv:2504.15364, 2025.\\n\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080–2094, 2021.\\n\\nJacob Pfau, William Merrill, and Samuel R. Bowman. Let’s think dot by dot: Hidden computation in transformer language models. In First Conference on Language Modeling, 2024. URL https:\\n\\n//openreview.net/forum?id=NikbrdtYvG.\\n\\nPreprint. Under Review.\\n\\nTang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforce- ment learning, 2025. URL https://arxiv.org/abs/2501.12948.\\n\\nYuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stu- art Shieber. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023.\\n\\nYuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to inter- nalize cot step by step. arXiv preprint arXiv:2405.14838, 2024.\\n\\nHarry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, and Beidi Chen. Get more with less: Synthesizing recurrence with kv cache compression for efficient llm inference. ICML, 2024. URL https://openreview.net/forum?id=uhHDhVKFMW. In\\n\\nYu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao. Not all heads mat- ter: A head-level KV cache compression method with integrated retrieval and reasoning. The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=FJFVmeXusW. In\\n\\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764–10799. PMLR, 2023.\\n\\nSachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=ph04CRkPdC.\\n\\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\\n\\nShibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in a continuous latent space, 2024. URL https: //arxiv.org/abs/2412.06769.\\n\\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.\\n\\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xu- ancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024. URL https://arxiv.org/abs/2409.12186.\\n\\nJunyoung Park, Dalton Jones, Matthew J Morse, Raghavv Goel, Mingu Lee, and Chris Lott. Keydiff: Key similarity-based kv cache eviction for long-context llm inference in resource-constrained environments. arXiv preprint arXiv:2504.15364, 2025.\\n\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080–2094, 2021.\\n\\nJacob Pfau, William Merrill, and Samuel R. Bowman. Let’s think dot by dot: Hidden computation in transformer language models. In First Conference on Language Modeling, 2024. URL https:\\n\\n//openreview.net/forum?id=NikbrdtYvG.\\n\\nPreprint. Under Review.\\n\\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Dmitry Dodonov, Tung Nguyen, Jaeho Lee, Daron Anderson, Mikhail Doroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, John-Clark Levin, Mstyslav Kazakov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Serguei Popov, Robert Gerbicz, Geoff Galgon, Johannes Schmitt, Will Yeadon, Yongki Lee, Scott Sauers, Alvaro Sanchez, Fabian Giska, Marc Roth, Søren Riis, Saiteja Utpala, Noah Burns, Gashaw M. Goshu, Mohinder Maheshbhai Naiya, Chidozie Agu, Zachary Giboney, Antrell Cheatom, Francesco Fournier-Facio, Sarah-Jane Crowson, Lennart Finke, Zerui Cheng, Jennifer Zampese, Ryan G. Hoerr, Mark Nandor, Hyunwoo Park, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Alexis C Garretson, Edwin Taylor, Damien Sileo, Qiuyu Ren, Usman Qazi, Lianghui Li, Jungbae Nam, and John B. Wydallis et al. Humanity’s last exam, 2025. URL https://arxiv.org/abs/2501.14249.\\n\\nUtkarsh Saxena, Gobinda Saha, Sakshi Choudhary, and Kaushik Roy. Eigen attention: Attention in low-rank space for KV cache compression. In Yaser Al-Onaizan, Mohit Bansal, and Yun- Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 15332–15344, Miami, Florida, USA, November 2024. Association for Computational Lin- guistics. doi: 10.18653/v1/2024.findings-emnlp.899. URL https://aclanthology.org/ 2024.findings-emnlp.899/.\\n\\nZhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Com- pressing chain-of-thought into continuous space via self-distillation, 2025. URL https: //arxiv.org/abs/2502.21074.\\n\\nDiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, and Qinqing Zheng. Token assorted: Mixing latent and text tokens for improved language model reasoning, 2025. URL https://arxiv.org/abs/2502.03275.\\n\\nQwen Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2, 2024.\\n\\nHaoyi Wu, Zhihao Teng, and Kewei Tu. Parallel continuous chain-of-thought with jacobi iteration. arXiv preprint arXiv:2506.18582, 2025.\\n\\nYige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. SoftCoT: Soft chain-of-thought for effi- cient reasoning with LLMs. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mo- hammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 23336–23351, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/ 2025.acl-long.1137. URL https://aclanthology.org/2025.acl-long.1137/.\\n\\nZiyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhenwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deeptheorem: Ad- vancing llm reasoning for theorem proving through natural language and reinforcement learning, 2025. URL https://arxiv.org/abs/2505.23754.\\n\\nRui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, et al. A survey on latent reasoning. arXiv preprint arXiv:2507.06203, 2025.\\n\\nPreprint. Under Review.\\n\\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Dmitry Dodonov, Tung Nguyen, Jaeho Lee, Daron Anderson, Mikhail Doroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, John-Clark Levin, Mstyslav Kazakov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Serguei Popov, Robert Gerbicz, Geoff Galgon, Johannes Schmitt, Will Yeadon, Yongki Lee, Scott Sauers, Alvaro Sanchez, Fabian Giska, Marc Roth, Søren Riis, Saiteja Utpala, Noah Burns, Gashaw M. Goshu, Mohinder Maheshbhai Naiya, Chidozie Agu, Zachary Giboney, Antrell Cheatom, Francesco Fournier-Facio, Sarah-Jane Crowson, Lennart Finke, Zerui Cheng, Jennifer Zampese, Ryan G. Hoerr, Mark Nandor, Hyunwoo Park, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Alexis C Garretson, Edwin Taylor, Damien Sileo, Qiuyu Ren, Usman Qazi, Lianghui Li, Jungbae Nam, and John B. Wydallis et al. Humanity’s last exam, 2025. URL https://arxiv.org/abs/2501.14249.\\n\\nUtkarsh Saxena, Gobinda Saha, Sakshi Choudhary, and Kaushik Roy. Eigen attention: Attention in low-rank space for KV cache compression. In Yaser Al-Onaizan, Mohit Bansal, and Yun- Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 15332–15344, Miami, Florida, USA, November 2024. Association for Computational Lin- guistics. doi: 10.18653/v1/2024.findings-emnlp.899. URL https://aclanthology.org/ 2024.findings-emnlp.899/.\\n\\nZhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Com- pressing chain-of-thought into continuous space via self-distillation, 2025. URL https: //arxiv.org/abs/2502.21074.\\n\\nDiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, and Qinqing Zheng. Token assorted: Mixing latent and text tokens for improved language model reasoning, 2025. URL https://arxiv.org/abs/2502.03275.\\n\\nQwen Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2, 2024.\\n\\nHaoyi Wu, Zhihao Teng, and Kewei Tu. Parallel continuous chain-of-thought with jacobi iteration. arXiv preprint arXiv:2506.18582, 2025.\\n\\nYige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. SoftCoT: Soft chain-of-thought for effi- cient reasoning with LLMs. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mo- hammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 23336–23351, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/ 2025.acl-long.1137. URL https://aclanthology.org/2025.acl-long.1137/.\\n\\nZiyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhenwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deeptheorem: Ad- vancing llm reasoning for theorem proving through natural language and reinforcement learning, 2025. URL https://arxiv.org/abs/2505.23754.\\n\\nRui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, et al. A survey on latent reasoning. arXiv preprint arXiv:2507.06203, 2025.\\n\\nPreprint. Under Review.\\n\\nA KV EVICTION DETAILS\\n\\nWe provide pseudocode to compute the r-KV score in Listing 1. The function takes as input a key-value pair and the attention scores between the CoT and and Answer tokens. There are several implementation differences from the original R-KV method.\\n\\nPadding Tokens First, we need to take into account padding tokens since we evict KV-cache in a batch during training. We do that by always assigning the lowest possible redundancy and importance score to the value-key pairs corresponding to the padding tokens\\n\\nImportance Score To compute the importance score, we use the attention score that answer tokens get when attending to the full CoT. We extract those value during the normal teacher forward pass and reuse to compute the\\n\\nRetention of Recent Tokens R-KV implementation adjust the redundancy score by always keep- ing β the most recent tokens. This is important for a reliable model performance during generation. We only use our method during training and apply it to the whole reasoning trace, therefore we skip this adjustment and only rely on selecting the most diverse keys with high attention to the answer tokens.\\n\\nListing 1: Pseudocode to implement the eviction score for a given key-value pair.\\n\\n1 def r_kv_score(key: torch.tensor, attn: torch.tensor, lbd: float): 2 \"\"\" 3 key: torch.tensor [bs, N_c, d] - CoT keys for a single head and layer 4 attn: torch.tensor [bs, N_A, N_c] - attenton scores 5 lbd: float - the weight of the importance score 6 \"\"\" 7 # compute redundancy score 8 key_norm = key / (key.norm(dim=-1, keepdim=True) + 1e-8) 9 cosine_sim = torch.einsum(\"...id,...jd->...ij\", key_norm, key_norm) for i in range(cosine_sim.shape[0]): cosine_sim[i].fill_diagonal_(0) cos_score = torch.sum(-cosine_sim, dim=-2) / torch.sum( ~pad_tokens, dim=-1, keepdim=True ) # Normalize to 1 R = cos_score.softmax(dim=-1) pad_tokens = key.sum(-1) == 0 R[pad_tokens] = 0\\n\\n11\\n\\n20\\n\\n21\\n\\n22\\n\\n23\\n\\n24\\n\\n25\\n\\n26\\n\\n27\\n\\n# compute importance score # sofmax over CoT dimention and avrage over answer tokens I = F.softmax(attn, dim=-1).mean(-2) # Assign the lowest score to the padding tokens I[pad_tokens] = 0 S = lbd * I + (1 - lbd) * R return S\\n\\n28\\n\\nPreprint. Under Review.\\n\\nA KV EVICTION DETAILS\\n\\nWe provide pseudocode to compute the r-KV score in Listing 1. The function takes as input a key-value pair and the attention scores between the CoT and and Answer tokens. There are several implementation differences from the original R-KV method.\\n\\nPadding Tokens First, we need to take into account padding tokens since we evict KV-cache in a batch during training. We do that by always assigning the lowest possible redundancy and importance score to the value-key pairs corresponding to the padding tokens\\n\\nImportance Score To compute the importance score, we use the attention score that answer tokens get when attending to the full CoT. We extract those value during the normal teacher forward pass and reuse to compute the\\n\\nRetention of Recent Tokens R-KV implementation adjust the redundancy score by always keep- ing β the most recent tokens. This is important for a reliable model performance during generation. We only use our method during training and apply it to the whole reasoning trace, therefore we skip this adjustment and only rely on selecting the most diverse keys with high attention to the answer tokens.\\n\\nListing 1: Pseudocode to implement the eviction score for a given key-value pair.\\n\\n1 def r_kv_score(key: torch.tensor, attn: torch.tensor, lbd: float): 2 \"\"\" 3 key: torch.tensor [bs, N_c, d] - CoT keys for a single head and layer 4 attn: torch.tensor [bs, N_A, N_c] - attenton scores 5 lbd: float - the weight of the importance score 6 \"\"\" 7 # compute redundancy score 8 key_norm = key / (key.norm(dim=-1, keepdim=True) + 1e-8) 9 cosine_sim = torch.einsum(\"...id,...jd->...ij\", key_norm, key_norm) for i in range(cosine_sim.shape[0]): cosine_sim[i].fill_diagonal_(0) cos_score = torch.sum(-cosine_sim, dim=-2) / torch.sum( ~pad_tokens, dim=-1, keepdim=True ) # Normalize to 1 R = cos_score.softmax(dim=-1) pad_tokens = key.sum(-1) == 0 R[pad_tokens] = 0\\n\\n11\\n\\n20\\n\\n21\\n\\n22\\n\\n23\\n\\n24\\n\\n25\\n\\n26\\n\\n27\\n\\n# compute importance score # sofmax over CoT dimention and avrage over answer tokens I = F.softmax(attn, dim=-1).mean(-2) # Assign the lowest score to the padding tokens I[pad_tokens] = 0 S = lbd * I + (1 - lbd) * R return S\\n\\n28\\n\\nPreprint. Under Review.\\n\\nB DATASETS\\n\\nOur models are trained using the GSM8k-Aug and GSM8k-Aug-NL datasets introduced by Deng et al. (2023), which augment the training set of the GSM8k (Cobbe et al., 2021) using GPT4 and provide a separate validation split. The golden traces in the datasets are split into discrete steps. GSM8k-Aug traces consist only of succinct statements such as <<600*30/100=180>>; <<600*10/100=60>>. The questions and answers in the NL (Natural Language) subset are identical, however the steps are formulated in natural language: 600 x 30/100 = 180 employees were promoted.; 600 x 10/100 = 60 employees received a bonus.\\n\\nGSM8K-Aug GSM8K-Aug-NL Huggingface Path whynlp/gsm8k-aug whynlp/gsm8k-aug-nl No. of Train Sample 385,620 No. of Validation Samples 500 No. of Test Samples 1319\\n\\nPreprint. Under Review.\\n\\nB DATASETS\\n\\nOur models are trained using the GSM8k-Aug and GSM8k-Aug-NL datasets introduced by Deng et al. (2023), which augment the training set of the GSM8k (Cobbe et al., 2021) using GPT4 and provide a separate validation split. The golden traces in the datasets are split into discrete steps. GSM8k-Aug traces consist only of succinct statements such as <<600*30/100=180>>; <<600*10/100=60>>. The questions and answers in the NL (Natural Language) subset are identical, however the steps are formulated in natural language: 600 x 30/100 = 180 employees were promoted.; 600 x 10/100 = 60 employees received a bonus.\\n\\nGSM8K-Aug GSM8K-Aug-NL Huggingface Path whynlp/gsm8k-aug whynlp/gsm8k-aug-nl No. of Train Sample 385,620 No. of Validation Samples 500 No. of Test Samples 1319\\n\\nPreprint. Under Review.\\n\\nC HYPERPARAMETERS\\n\\nTable 6: All the hyperparameters used for our method.\\n\\nHyperparameter GSM8k-AUG GSM8k-AUG-NL LLAMA3.2 - 1B - INSTRUCT α1 (CODI) 10 10 KV loss Smooth L1 MSE Layer-wise std True True α2 (KV) 1 1 r-kv λ 0.1 0.1 Use Projection True True learning rate 8e-4 8e-4 lr scheduler Cosine Cosine optimizer AdamW AdamW batch size 128 128 weight decay 0.1 0.1 gradient clipping 2 2 epochs 10 10 QWEN2.5 - 0.5B - INSTRUCT α1 (CODI) 10 10 KV loss MSE MSE Layer-wise std False True α2 (KV) 1 1 r-kv λ 0.1 0.1 Use Projection True True learning rate 5e-4 8e-4 lr scheduler Cosine Cosine optimizer AdamW AdamW batch size 128 128 weight decay 0.01 0.1 gradient clipping 2 2 epochs 10 10 LLAMA3.2 - 3B - INSTRUCT α1 (CODI) 20 20 KV loss Smooth L1 Smooth L1 Layer-wise std False False α2 (KV) 2 2 r-kv λ 0.1 0.0 Use Projection True False learning rate 2e-4 2e-4 lr scheduler Cosine Cosine optimizer AdamW AdamW batch size 128 128 weight decay 0.1 0.1 gradient clipping 2 2 epochs 5 5\\n\\nPreprint. Under Review.\\n\\nC HYPERPARAMETERS\\n\\nTable 6: All the hyperparameters used for our method.\\n\\nHyperparameter GSM8k-AUG GSM8k-AUG-NL LLAMA3.2 - 1B - INSTRUCT α1 (CODI) 10 10 KV loss Smooth L1 MSE Layer-wise std True True α2 (KV) 1 1 r-kv λ 0.1 0.1 Use Projection True True learning rate 8e-4 8e-4 lr scheduler Cosine Cosine optimizer AdamW AdamW batch size 128 128 weight decay 0.1 0.1 gradient clipping 2 2 epochs 10 10 QWEN2.5 - 0.5B - INSTRUCT α1 (CODI) 10 10 KV loss MSE MSE Layer-wise std False True α2 (KV) 1 1 r-kv λ 0.1 0.1 Use Projection True True learning rate 5e-4 8e-4 lr scheduler Cosine Cosine optimizer AdamW AdamW batch size 128 128 weight decay 0.01 0.1 gradient clipping 2 2 epochs 10 10 LLAMA3.2 - 3B - INSTRUCT α1 (CODI) 20 20 KV loss Smooth L1 Smooth L1 Layer-wise std False False α2 (KV) 2 2 r-kv λ 0.1 0.0 Use Projection True False learning rate 2e-4 2e-4 lr scheduler Cosine Cosine optimizer AdamW AdamW batch size 128 128 weight decay 0.1 0.1 gradient clipping 2 2 epochs 5 5\\n\\nPreprint. Under Review.\\n\\nD KV-CACHE COSINE SIMILARITY BETWEEN THE LATENT COT AND THE GROUND-TRUTH COT\\n\\nWe investigate the similarity between the KV-cache representing the latent CoT and the KV-cache of the ground-truth CoT. Figures 8 and 7 present the similarities averaged over layers and heads, while figures 9, 10, 11, and 12 show the similarities in individual heads and layers.\\n\\nFigure 8: Cosine similarity of Values in the latent CoT with Values of the ground truth averaged across heads and layers. We use the same prompt and ground truth CoT as in Table 5.\\n\\nPreprint. Under Review.\\n\\nD KV-CACHE COSINE SIMILARITY BETWEEN THE LATENT COT AND THE GROUND-TRUTH COT\\n\\nWe investigate the similarity between the KV-cache representing the latent CoT and the KV-cache of the ground-truth CoT. Figures 8 and 7 present the similarities averaged over layers and heads, while figures 9, 10, 11, and 12 show the similarities in individual heads and layers.\\n\\nFigure 8: Cosine similarity of Values in the latent CoT with Values of the ground truth averaged across heads and layers. We use the same prompt and ground truth CoT as in Table 5.\\n\\nPreprint. Under Review.\\n\\nlayers.\\n\\nE DECODED LATENT TRACES\\n\\nIn this section we present two additional examples of traces decoded in the same manner as described in section 5.1.\\n\\nPreprint. Under Review.\\n\\nlayers.\\n\\nE DECODED LATENT TRACES\\n\\nIn this section we present two additional examples of traces decoded in the same manner as described in section 5.1.\\n\\nPreprint. Under Review.\\n\\nSudentvs GT vaes -Detala\\n\\nFigure 10: Cosine similarity between Values in the latent CoT and Values of the ground truth across layers.\\n\\nPreprint. Under Review.\\n\\nSudentvs GT vaes -Detala\\n\\nFigure 10: Cosine similarity between Values in the latent CoT and Values of the ground truth across layers.\\n\\nPreprint. Under Review.\\n\\nS Keys Afer Evicion - Dtalled\\n\\nFigure 11: Cosine similarity between Keys in the latent CoT and Keys of the ground truth after eviction across layers.\\n\\nPreprint. Under Review.\\n\\nS Keys Afer Evicion - Dtalled\\n\\nFigure 11: Cosine similarity between Keys in the latent CoT and Keys of the ground truth after eviction across layers.\\n\\nPreprint. Under Review.\\n\\nFigure 12: Cosine similarity between Values in the latent CoT and Values of the ground truth after eviction across layers.\\n\\nPreprint. Under Review.\\n\\nFigure 12: Cosine similarity between Values in the latent CoT and Values of the ground truth after eviction across layers.\\n\\nPreprint. Under Review.\\n\\nTopK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 GSM8K-Aug 1 24 * 50 = 120 0 >> << 120 * 0 0 0 = = 2 50 *. 0 * 150 >> . The 0 *. *. 10 >> >> 0 3 . *( 30 *. 600 00 << <<( . 0 * 00 00 0 >> Teacher <<50*0.10=5>><<5*24=120>> Golden <<50*.10=5>><<5*24=120>><<120*30=3600>> GSM8K-Aug-NL 1 T6 50 T9 * * , , , , 0 2 T7 T6 0 0 * * * , T11 T10 T10 T10 per 3 T8 a * * T11 T11 T11 T11 , * per per 00 Teacher He gets 0.10*50=5 dollars a hour Golden He makes 50*$.10=$5 per hour [...] $120*30=$3600 a month Answer 3600 3600 3600 3600 1800 3600\\n\\nTable 7: Prompt: “Jon runs a website where he gets paid for every person who visits. He gets paid $0.10 for every person who visits. Each hour he gets 50 visits. His website operates 24 hours a day. How many dollars does he make in a 30 day month?”. T6 – T11 stand for gets, makes, operates, visits, hourly, and hour respectively. Tokens 16-24 are omitted due to low semantic content.\\n\\nTopK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 GSM8K-Aug 1 150 * 2 = 300 >> The as as as as as as as 2 2 + 1 * 150 . << T15 T15 T15 T15 T15 T15 T15 3 300 * 5 = 30 >> T16 of of of of of of of Teacher <<150*2=300>> Golden <<150*2=300>><<300*5=1500>> GSM8K-Aug-NL 1 T13 T11 T11 T17 T11 T11 T11 T11 T11 T11 T11 T11 T11 T11 2 T11 to T14 T12 to T14 T14 T14 T14 3 T14 T18 to T11 T14 to T14 T14 T14 T14 T14 , , Teacher Raine takes 150 x 2 = 300 steps walking to and from school in one day. Golden Raine takes 150 x 2 = 300 steps walking [...] her 300 x 5 = 1500 steps in five days. Answer 1500 1500 1500 1500 1500 1500\\n\\nTable 8: Prompt: “Raine’s house is just a walking distance from her school. It takes her 150 steps to walk to the school. How many steps does she take walking to and from school in five days?”. T11 – T18 stand for walking, footsteps, walks, walk, but, This, steps, and going respectively.\\n\\nPreprint. Under Review.\\n\\nTopK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 GSM8K-Aug 1 24 * 50 = 120 0 >> << 120 * 0 0 0 = = 2 50 *. 0 * 150 >> . The 0 *. *. 10 >> >> 0 3 . *( 30 *. 600 00 << <<( . 0 * 00 00 0 >> Teacher <<50*0.10=5>><<5*24=120>> Golden <<50*.10=5>><<5*24=120>><<120*30=3600>> GSM8K-Aug-NL 1 T6 50 T9 * * , , , , 0 2 T7 T6 0 0 * * * , T11 T10 T10 T10 per 3 T8 a * * T11 T11 T11 T11 , * per per 00 Teacher He gets 0.10*50=5 dollars a hour Golden He makes 50*$.10=$5 per hour [...] $120*30=$3600 a month Answer 3600 3600 3600 3600 1800 3600\\n\\nTable 7: Prompt: “Jon runs a website where he gets paid for every person who visits. He gets paid $0.10 for every person who visits. Each hour he gets 50 visits. His website operates 24 hours a day. How many dollars does he make in a 30 day month?”. T6 – T11 stand for gets, makes, operates, visits, hourly, and hour respectively. Tokens 16-24 are omitted due to low semantic content.\\n\\nTopK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 GSM8K-Aug 1 150 * 2 = 300 >> The as as as as as as as 2 2 + 1 * 150 . << T15 T15 T15 T15 T15 T15 T15 3 300 * 5 = 30 >> T16 of of of of of of of Teacher <<150*2=300>> Golden <<150*2=300>', 'text_length': 100000, 'message': 'Retrieved 42 pages for 2510v02312v1.pdf'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `generate_summary` with `The paper introduces KAVA, a novel framework for latent reasoning in large language models (LLMs) that leverages compressed Key-Value (KV) cache distillation. Traditional chain-of-thought (CoT) reasoning in LLMs is computationally expensive due to verbose traces, while latent reasoning offers efficiency but lacks direct supervision. KAVA bridges this gap by distilling knowledge from a teacher model's compressed KV-cache into a latent reasoning student, using continuous latent tokens to align stepwise KV trajectories. This method maintains the accuracy of CoT-trained models while enhancing efficiency and scalability. KAVA outperforms existing latent reasoning methods, showing less performance degradation when transitioning from equation-only to natural-language traces and scaling effectively to larger models. The approach provides a scalable supervision signal for latent reasoning, combining CoT accuracy with latent inference efficiency, and is significant for AI professionals seeking efficient, deployable reasoning models.`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m{'status': 'success', 'summary': \"The paper presents KAVA, a groundbreaking framework for latent reasoning in large language models (LLMs) that utilizes compressed Key-Value (KV) cache distillation. This approach addresses the high computational cost of traditional chain-of-thought (CoT) reasoning by offering an efficient alternative without sacrificing accuracy. KAVA achieves this by transferring knowledge from a teacher model's compressed KV-cache to a latent reasoning student, aligning stepwise KV trajectories with continuous latent tokens. The framework not only maintains the accuracy of CoT-trained models but also enhances efficiency and scalability. KAVA demonstrates superior performance compared to existing latent reasoning methods, with minimal performance loss when shifting from equation-only to natural-language traces. It effectively scales to larger models, providing a scalable supervision signal that combines CoT accuracy with latent inference efficiency. This advancement is crucial for AI professionals aiming to develop efficient and deployable reasoning models, impacting practical applications and industry by offering a more scalable and efficient reasoning framework.\", 'length': 1166, 'message': 'Generated summary (1166 characters)'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_document_text` with `blog_dvgodoy_fine-tuning-llm-hugging-face_20251003_161407.html`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m{'status': 'success', 'filename': 'blog_dvgodoy_fine-tuning-llm-hugging-face_20251003_161407.html', 'pages': 10, 'text': 'Back to Articles\\n\\nFine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face\\n\\nCommunity Article Published February 11, 2025\\n\\nUpvote\\n\\n72\\n\\nDaniel Voigt Godoy\\n\\ndvgodoy\\n\\nThis blog post contains \"Chapter 0: TL;DR\" of my latest book A Hands-On Guide to Fine-Tuning Large Language Models with PyTorch and Hugging Face.\\n\\nSpoilers\\n\\nIn this blog post, we\\'ll get right to it and fine-tune a small language model, Microsoft\\'s Phi-3 Mini 4K Instruct, to translate English into Yoda-speak. You can think of this initial chapter as a recipe you can just follow. It\\'s a \"shoot first, ask questions later\" kind of post.\\n\\nYou\\'ll learn how to:\\n\\nLoad a quantized model using BitsAndBytes\\n\\nConfigure low-rank adapters (LoRA) using Hugging Face\\'s peft\\n\\nLoad and format a dataset\\n\\nFine-tune the model using the supervised fine-tuning trainer (SFTTrainer) from Hugging Face\\'s trl\\n\\nUse the fine-tuned model to generate a sentence\\n\\nJupyter Notebook\\n\\nThe Jupyter notebook corresponding to this post is part of the official Fine-Tuning LLMs repository on GitHub. You can also run it directly in Google Colab\\n\\nSetup\\n\\nIf you\\'re running it on Colab, you\\'ll need to pip install a few libraries: datasets, bitsandbytes, and trl.\\n\\nFor better reproducibility during training, however, use the pinned versions instead:\\n\\n#!pip install datasets bitsandbytes trl !pip install transformers==4.55.2 peft==0.17.0 accelerate==1.10.0 trl==0.21.0 bitsandbytes==0.47.0 datasets==4.0.0 huggingface-hub==0.34.4 safetensors==0.6.2 pandas==2.2.2 matplotlib==3.10.0 numpy==2.0.2\\n\\nImports\\n\\nFor the sake of organization, all libraries needed throughout the code used are imported at its very start. For this post, we\\'ll need the following imports:\\n\\nimport os import torch from datasets import load_dataset from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig from trl import SFTConfig, SFTTrainer\\n\\nLoading a Quantized Base Model\\n\\nWe start by loading a quantized model, so it takes up less space in the GPU\\'s RAM. A quantized model replaces the original weights with approximate values that are represented by fewer bits. The simplest and most straightforward way to quantize a model is to turn its weights from 32-bit floating-point (FP32) numbers into 4-bit floating-point numbers (NF4). This simple yet powerful change already reduces the model\\'s memory footprint by roughly a factor of eight.\\n\\nWe can use an instance of BitsAndBytesConfig as the quantization_config argument while loading a model using the from_pretrained() method. To keep it flexible, so you can try it out with any other model of your choice, we\\'re using Hugging Face\\'s AutoModelForCausalLM. The repo you choose to use determines the model being loaded.\\n\\nWithout further ado, here\\'s our quantized model being loaded:\\n\\nbnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float32 ) repo_id = \\'microsoft/Phi-3-mini-4k-instruct\\' model = AutoModelForCausalLM.from_pretrained( repo_id, device_map=\"cuda:0\", quantization_config=bnb_config )\\n\\n\"The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support.\" Source: Hugging Face Hub\\n\\nOnce the model is loaded, you can see how much space it occupies in memory using the get_memory_footprint() method.\\n\\nprint(model.get_memory_footprint()/1e6)\\n\\n2206.347264\\n\\nEven though it\\'s been quantized, the model still takes up a bit more than 2 gigabytes of RAM. The quantization procedure focuses on the linear layers within the Transformer decoder blocks (also referred to as \"layers\" in some cases):\\n\\nmodel\\n\\nPhi3ForCausalLM( (model): Phi3Model( (embed_tokens): Embedding(32064, 3072, padding_idx=32000) (embed_dropout): Dropout(p=0.0, inplace=False) (layers): ModuleList( (0-31): 32 x Phi3DecoderLayer( (self_attn): Phi3Attention( (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False) <1> (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False) <1> (rotary_emb): Phi3RotaryEmbedding() ) (mlp): Phi3MLP( (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False) <1> (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False) <1> (activation_fn): SiLU() ) (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05) (resid_attn_dropout): Dropout(p=0.0, inplace=False) (resid_mlp_dropout): Dropout(p=0.0, inplace=False) (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05) ) ) (norm): Phi3RMSNorm((3072,), eps=1e-05) ) (lm_head): Linear(in_features=3072, out_features=32064, bias=False) )\\n\\n<1> Quantized layers\\n\\nA quantized model can be used directly for inference, but it cannot be trained any further. Those pesky Linear4bit layers take up much less space, which is the whole point of quantization; however, we cannot update them.\\n\\nWe need to add something else to our mix, a sprinkle of adapters.\\n\\nSetting Up Low-Rank Adapters (LoRA)\\n\\nLow-rank adapters can be attached to each and every one of the quantized layers. The adapters are mostly regular Linear layers that can be easily updated as usual. The clever trick in this case is that these adapters are significantly smaller than the layers that have been quantized.\\n\\nSince the quantized layers are frozen (they cannot be updated), setting up LoRA adapters on a quantized model drastically reduces the total number of trainable parameters to just 1% (or less) of its original size.\\n\\nWe can set up LoRA adapters in three easy steps:\\n\\nCall prepare_model_for_kbit_training() to improve numerical stability during training.\\n\\nCreate an instance of LoraConfig.\\n\\nApply the configuration to the quantized base model using the get_peft_model() method.\\n\\nLet\\'s try it out with our model:\\n\\nmodel = prepare_model_for_kbit_training(model) config = LoraConfig( # the rank of the adapter, the lower the fewer parameters you\\'ll need to train r=8, lora_alpha=16, # multiplier, usually 2*r bias=\"none\", lora_dropout=0.05, task_type=\"CAUSAL_LM\", # Newer models, such as Phi-3 at time of writing, may require # manually setting target modules target_modules=[\\'o_proj\\', \\'qkv_proj\\', \\'gate_up_proj\\', \\'down_proj\\'], ) model = get_peft_model(model, config) model\\n\\nPeftModelForCausalLM( (base_model): LoraModel( (model): Phi3ForCausalLM( (model): Phi3Model( (embed_tokens): Embedding(32064, 3072, padding_idx=32000) (embed_dropout): Dropout(p=0.0, inplace=False) (layers): ModuleList( (0-31): 32 x Phi3DecoderLayer( (self_attn): Phi3Attention( (o_proj): lora.Linear4bit( <1> (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False) (lora_dropout): ModuleDict((default): Dropout(p=0.05, inplace=False)) (lora_A): ModuleDict( (default): Linear(in_features=3072, out_features=8, bias=False) ) (lora_B): ModuleDict( (default): Linear(in_features=8, out_features=3072, bias=False) ) (lora_embedding_A): ParameterDict() (lora_embedding_B): ParameterDict() (lora_magnitude_vector): ModuleDict() ) (qkv_proj): lora.Linear4bit(...) <1> (rotary_emb): Phi3RotaryEmbedding() ) (mlp): Phi3MLP( (gate_up_proj): lora.Linear4bit(...) <1> (down_proj): lora.Linear4bit(...) <1> (activation_fn): SiLU() ) (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05) (resid_attn_dropout): Dropout(p=0.0, inplace=False) (resid_mlp_dropout): Dropout(p=0.0, inplace=False) (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05) ) ) (norm): Phi3RMSNorm((3072,), eps=1e-05) ) (lm_head): Linear(in_features=3072, out_features=32064, bias=False) ) ) )\\n\\n<1> LoRA adapters\\n\\nThe output of the other three LoRA layers (qkv_proj, gate_up_proj, and down_proj) was suppressed to shorten the output.\\n\\nDid you get the following error? ValueError: Please specify `target_modules` in `peft_config` Most likely, you don\\'t need to specify the target_modules if you\\'re using one of the well-known models. The peft library takes care of it by automatically choosing the appropriate targets. However, there may be a gap between the time a popular model is released and the time the library gets updated. So, if you get the error above, look for the quantized layers in your model and list their names in the target_modules argument.\\n\\nThe quantized layers (Linear4bit) have turned into lora.Linear4bit modules where the quantized layer itself became the base_layer with some regular Linear layers (lora_A and lora_B) added to the mix.\\n\\nThese extra layers would make the model only slightly larger. However, the model preparation function (prepare_model_for_kbit_training()) turned every non-quantized layer to full precision (FP32), thus resulting in a 30% larger model:\\n\\nprint(model.get_memory_footprint()/1e6)\\n\\n2651.080704\\n\\nSince most parameters are frozen, only a tiny fraction of the total number of parameters are currently trainable, thanks to LoRA!\\n\\ntrain_p, tot_p = model.get_nb_trainable_parameters() print(f\\'Trainable parameters: {train_p/1e6:.2f}M\\') print(f\\'Total parameters: {tot_p/1e6:.2f}M\\') print(f\\'% of trainable parameters: {100*train_p/tot_p:.2f}%\\')\\n\\nTrainable parameters: 12.58M Total parameters: 3833.66M % of trainable parameters: 0.33%\\n\\nThe model is ready to be fine-tuned, but we are still missing one key component: our dataset.\\n\\nFormatting Your Dataset\\n\\n\"Like Yoda, speak, you must. Hrmmm.\" Master Yoda\\n\\nThe dataset yoda_sentences consists of 720 sentences translated from English to Yoda-speak. The dataset is hosted on the Hugging Face Hub and we can easily load it using the load_dataset() method from the Hugging Face datasets library:\\n\\ndataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\") dataset\\n\\nDataset({ features: [\\'sentence\\', \\'translation\\', \\'translation_extra\\'], num_rows: 720 })\\n\\nThe dataset has three columns:\\n\\noriginal English sentence (sentence)\\n\\nbasic translation to Yoda-speak (translation)\\n\\nenhanced translation including typical Yesss and Hrrmm interjections (translation_extra)\\n\\ndataset[0]\\n\\n{\\'sentence\\': \\'The birch canoe slid on the smooth planks.\\', \\'translation\\': \\'On the smooth planks, the birch canoe slid.\\', \\'translation_extra\\': \\'On the smooth planks, the birch canoe slid. Yes, hrrrm.\\'}\\n\\nThe SFTTrainer we\\'ll be using to fine-tune the model can automatically handle datasets in conversational format.\\n\\n{\"messages\":[ {\"role\": \"system\", \"content\": \"<general directives>\"}, {\"role\": \"user\", \"content\": \"<prompt text>\"}, {\"role\": \"assistant\", \"content\": \"<ideal generated text>\"} ]}\\n\\nIMPORTANT UPDATE: unfortunately, in more recent versions of the trl library, the \"instruction\" format is not properly supported anymore, thus leading to the chat template not being applied to the dataset. In order to avoid this issue, we can convert the dataset to the \"conversational\" format.\\n\\nSo, we\\'ll convert the dataset to the conversational format using the format_dataset() function below:\\n\\n# Adapted from trl.extras.dataset_formatting.instructions_formatting_function # Converts dataset from prompt/completion format (not supported anymore) # to the conversational format def format_dataset(examples): if isinstance(examples[\"prompt\"], list): output_texts = [] for i in range(len(examples[\"prompt\"])): converted_sample = [ {\"role\": \"user\", \"content\": examples[\"prompt\"][i]}, {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]}, ] output_texts.append(converted_sample) return {\\'messages\\': output_texts} else: converted_sample = [ {\"role\": \"user\", \"content\": examples[\"prompt\"]}, {\"role\": \"assistant\", \"content\": examples[\"completion\"]}, ] return {\\'messages\\': converted_sample}\\n\\ndataset = dataset.rename_column(\"sentence\", \"prompt\") dataset = dataset.rename_column(\"translation_extra\", \"completion\") dataset = dataset.map(format_dataset) dataset = dataset.remove_columns([\\'prompt\\', \\'completion\\', \\'translation\\']) messages = dataset[0][\\'messages\\'] messages\\n\\n[{\\'role\\': \\'user\\', \\'content\\': \\'The birch canoe slid on the smooth planks.\\'}, {\\'role\\': \\'assistant\\', \\'content\\': \\'On the smooth planks, the birch canoe slid. Yes, hrrrm.\\'}]\\n\\nTokenizer\\n\\nBefore moving into the actual training, we still need to load the tokenizer that corresponds to our model. The tokenizer is an important part of this process, determining how to convert text into tokens in the same way used to train the model.\\n\\nFor instruction/chat models, the tokenizer also contains its corresponding chat template that specifies:\\n\\nWhich special tokens should be used, and where they should be placed.\\n\\nWhere the system directives, user prompt, and model response should be placed.\\n\\nWhat is the generation prompt, that is, the special token that triggers the model\\'s response (more on that in the \"Querying the Model\" section)\\n\\nIMPORTANT UPDATE: due to changes in the default collator used by the SFTTrainer class while building the dataset, the EOS token (which is, in Phi-3, the same as the PAD token) was masked in the labels too thus leading to the model not being able to properly stop token generation.\\n\\nIn order to address this change, we can assign the UNK token to the PAD token, so the EOS token becomes unique and therefore not masked as part of the labels.\\n\\ntokenizer = AutoTokenizer.from_pretrained(repo_id) tokenizer.pad_token = tokenizer.unk_token tokenizer.pad_token_id = tokenizer.unk_token_id tokenizer.chat_template\\n\\n\"{% for message in messages %} {% if message[\\'role\\'] ## \\'system\\' %} {{\\'<|system|>\\\\n\\' + message[\\'content\\'] + \\'<|end|>\\\\n\\'}} {% elif message[\\'role\\'] ## \\'user\\' %} {{\\'<|user|>\\\\n\\' + message[\\'content\\'] + \\'<|end|>\\\\n\\'}} {% elif message[\\'role\\'] ## \\'assistant\\' %} {{\\'<|assistant|>\\\\n\\' + message[\\'content\\'] + \\'<|end|>\\\\n\\'}} {% endif %} {% endfor %} {% if add_generation_prompt %} {{ \\'<|assistant|>\\\\n\\' }}{% else %}{{ eos_token }} {% endif %}\"\\n\\nNever mind the seemingly overcomplicated template (I have added line breaks and indentation to it so it\\'s easier to read). It simply organizes the messages into a coherent block with the appropriate tags, as shown below (tokenize=False ensures we get readable text back instead of a numeric sequence of token IDs):\\n\\nprint(tokenizer.apply_chat_template(messages, tokenize=False))\\n\\n<|user|> The birch canoe slid on the smooth planks.<|end|> <|assistant|> On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|> <|endoftext|>\\n\\nNotice that each interaction is wrapped in either <|user|> or <|assistant|> tokens at the beginning and <|end|> at the end. Moreover, the <|endoftext|> token indicates the end of the whole block.\\n\\nDifferent models will have different templates and tokens to indicate the beginning and end of sentences and blocks.\\n\\nWe\\'re now ready to tackle the actual fine-tuning!\\n\\nFine-Tuning with SFTTrainer\\n\\nFine-tuning a model, whether large or otherwise, follows exactly the same training procedure as training a model from scratch. We could write our own training loop in pure PyTorch, or we could use Hugging Face\\'s Trainer to fine-tune our model.\\n\\nIt is much easier, however, to use SFTTrainer instead (which uses Trainer underneath, by the way), since it takes care of most of the nitty-gritty details for us, as long as we provide it with the following four arguments:\\n\\na model\\n\\na tokenizer\\n\\na dataset\\n\\na configuration object\\n\\nWe\\'ve already got the first three elements; let\\'s work on the last one.\\n\\nSFTConfig\\n\\nThere are many parameters that we can set in the configuration object. We have divided them into four groups:\\n\\nMemory usage optimization parameters related to gradient accumulation and checkpointing\\n\\nDataset-related arguments, such as the max_seq_length required by your data, and whether you are packing or not the sequences\\n\\nTypical training parameters such as the learning_rate and the num_train_epochs\\n\\nEnvironment and logging parameters such as output_dir (this will be the name of the model if you choose to push it to the Hugging Face Hub once it\\'s trained), logging_dir, and logging_steps.\\n\\nWhile the learning rate is a very important parameter (as a starting point, you can try the learning rate used to train the base model in the first place), it\\'s actually the maximum sequence length that\\'s more likely to cause out-of-memory issues.\\n\\nMake sure to always pick the shortest possible max_seq_length that makes sense for your use case. In ours, the sentences—both in English and Yoda-speak—are quite short, and a sequence of 64 tokens is more than enough to cover the prompt, the completion, and the added special tokens.\\n\\nFlash attention (which, unfortunately, isn\\'t supported in Colab), allows for more flexibility in working with longer sequences, avoiding the potential issue of OOM errors.\\n\\nIMPORTANT UPDATE: The release of trl version 0.20 brought several changes to the SFTConfig:\\n\\npacking is performed differently than it was, unless packing_strategy=\\'wrapped\\' is set;\\n\\nthe max_seq_length argument was renamed to max_length;\\n\\nthe bf16 defaults to True but, at the time of this update (Aug/2025), it didn\\'t check if the BF16 type was actually available or not, so it\\'s included in the configuration now.\\n\\nsft_config = SFTConfig( ## GROUP 1: Memory usage # These arguments will squeeze the most out of your GPU\\'s RAM # Checkpointing gradient_checkpointing=True, # this saves a LOT of memory # Set this to avoid exceptions in newer versions of PyTorch gradient_checkpointing_kwargs={\\'use_reentrant\\': False}, # Gradient Accumulation / Batch size # Actual batch (for updating) is same (1x) as micro-batch size gradient_accumulation_steps=1, # The initial (micro) batch size to start off with per_device_train_batch_size=16, # If batch size would cause OOM, halves its size until it works auto_find_batch_size=True, ## GROUP 2: Dataset-related max_length=64, # renamed in v0.20 # Dataset # packing a dataset means no padding is needed packing=True, packing_strategy=\\'wrapped\\', # added to approximate original packing behavior ## GROUP 3: These are typical training parameters num_train_epochs=10, learning_rate=3e-4, # Optimizer # 8-bit Adam optimizer - doesn\\'t help much if you\\'re using LoRA! optim=\\'paged_adamw_8bit\\', ## GROUP 4: Logging parameters logging_steps=10, logging_dir=\\'./logs\\', output_dir=\\'./phi3-mini-yoda-adapter\\', report_to=\\'none\\'. # ensures bf16 (the new default) is only used when it is actually available bf16=torch.cuda.is_bf16_supported(including_emulation=False) )\\n\\nSFTTrainer\\n\\n\"It is training time!\" The Hulk\\n\\nIMPORTANT UPDATE: The current version of trl (0.21) has a known issue where training fails if the LoRA configuration has already been applied to the model, as the trainer freezes the whole model, including the adapters.\\n\\nHowever, it works as expected when the configuration is passed as the peft_config argument to the trainer, since it is applied after freezing the existing layers.\\n\\nIf the model already contains the adapters, as in our case, training still works, but we need to use the underlying original model instead (model.base_model.model) to ensure the save_model() method functions correctly.\\n\\nWe can now finally create an instance of the supervised fine-tuning trainer:\\n\\ntrainer = SFTTrainer( model=model.base_model.model, # the underlying Phi-3 model peft_config=config, # added to fix issue in TRL>=0.20 processing_class=tokenizer, args=sft_config, train_dataset=dataset, )\\n\\nThe SFTTrainer had already preprocessed our dataset, so we can take a look inside and see how each mini-batch was assembled:\\n\\ndl = trainer.get_train_dataloader() batch = next(iter(dl))\\n\\nLet\\'s check the labels; after all, we didn\\'t provide any, did we?\\n\\nbatch[\\'input_ids\\'][0], batch[\\'labels\\'][0]\\n\\n(tensor([ 1746, 29892, 278, 10435, 3147, 698, 287, 29889, 32007, 32000, 32000, 32010, 10987, 278, 3252, 262, 1058, 380, 1772, 278, 282, 799, 29880, 18873, 1265, 29889, 32007, 32001, 11644, 380, 1772, 278, 282, 799, 29880, 18873, 1265, 29892, 1284, 278, 3252, 262, 29892, 366, 1818, 29889, 3869, 29892, 298, 21478, 1758, 29889, 32007, 32000, 32000, 32010, 315, 329, 278, 13793, 393, 7868, 29879, 278], device=\\'cuda:0\\'), tensor([ 1746, 29892, 278, 10435, 3147, 698, 287, 29889, 32007, 32000, 32000, 32010, 10987, 278, 3252, 262, 1058, 380, 1772, 278, 282, 799, 29880, 18873, 1265, 29889, 32007, 32001, 11644, 380, 1772, 278, 282, 799, 29880, 18873, 1265, 29892, 1284, 278, 3252, 262, 29892, 366, 1818, 29889, 3869, 29892, 298, 21478, 1758, 29889, 32007, 32000, 32000, 32010, 315, 329, 278, 13793, 393, 7868, 29879, 278], device=\\'cuda:0\\'))\\n\\nThe labels were added automatically, and they\\'re exactly the same as the inputs. Thus, this is a case of self-supervised fine-tuning.\\n\\nThe shifting of the labels will be handled automatically as well; there\\'s no need to be concerned about it.\\n\\nAlthough this is a 3.8 billion-parameter model, the configuration above allows us to squeeze training, using a mini-batch of eight, into an old setup with a consumer-grade GPU such as a GTX 1060 with only 6 GB RAM. True story! It takes about 35 minutes to complete the training process.\\n\\nNext, we call the train() method and wait:\\n\\ntrainer.train()\\n\\nStep Training Loss 10 2.990700 20 1.789500 30 1.581700 40 1.458300 50 1.362300 100 0.607900 150 0.353600 200 0.277500 220 0.252400\\n\\nQuerying the Model\\n\\nNow, our model should be able to produce a Yoda-like sentence as a response to any short sentence we give it.\\n\\nSo, the model requires its inputs to be properly formatted. We need to build a list of \"messages\"—ours, from the user, in this case—and prompt the model to answer by indicating it\\'s its turn to write.\\n\\nThis is the purpose of the add_generation_prompt argument: it adds <|assistant|> to the end of the conversation, so the model can predict the next word—and continue doing so until it predicts an <|endoftext|> token.\\n\\nThe helper function below assembles a message (in the conversational format) and applies the chat template to it, appending the generation prompt to its end.\\n\\ndef gen_prompt(tokenizer, sentence): converted_sample = [{\"role\": \"user\", \"content\": sentence}] prompt = tokenizer.apply_chat_template( converted_sample, tokenize=False, add_generation_prompt=True ) return prompt\\n\\nLet\\'s try generating a prompt for an example sentence:\\n\\nsentence = \\'The Force is strong in you!\\' prompt = gen_prompt(tokenizer, sentence) print(prompt)\\n\\n<|user|> The Force is strong in you!<|end|> <|assistant|>\\n\\nThe prompt seems about right; let\\'s use it to generate a completion. The helper function below does the following:\\n\\nIt tokenizes the prompt into a tensor of token IDs (add_special_tokens is set to False because the tokens were already added by the chat template).\\n\\nIt sets the model to evaluation mode.\\n\\nIt calls the model\\'s generate() method to produce the output (generated token IDs).\\n\\nIf the model was trained using mixed-precision, we wrap the generation in the autocast() context manager, which automatically handles conversion between data types.\\n\\nIt decodes the generated token IDs back into readable text.\\n\\ndef generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False): tokenized_input = tokenizer( prompt, add_special_tokens=False, return_tensors=\"pt\" ).to(model.device) model.eval() # if it was trained using mixed precision, uses autocast context ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) \\\\ if model.dtype in [torch.float16, torch.bfloat16] else nullcontext() with ctx: gen_output = model.generate(**tokenized_input, eos_token_id=tokenizer.eos_token_id, max_new_tokens=max_new_tokens) output = tokenizer.batch_decode(gen_output, skip_special_tokens=skip_special_tokens) return output[0]\\n\\nNow, we can finally try out our model and see if it\\'s indeed capable of generating Yoda-speak.\\n\\nprint(generate(model, tokenizer, prompt))\\n\\n<|user|> The Force is strong in you!<|end|><|assistant|> Strong in you, the Force is. Yes, hrrmmm.<|end|>\\n\\nAwesome! It works! Like Yoda, the model speaks. Hrrrmm.\\n\\nCongratulations, you\\'ve fine-tuned your first LLM!\\n\\nNow, you\\'ve got a small adapter that can be loaded into an instance of the Phi-3 Mini 4K Instruct model to turn it into a Yoda translator! How cool is that?\\n\\nSaving the Adapter\\n\\nOnce the training is completed, you can save the adapter (and the tokenizer) to disk by calling the trainer\\'s save_model() method. It will save everything to the specified folder:\\n\\ntrainer.save_model(\\'local-phi3-mini-yoda-adapter\\')\\n\\nThe files that were saved include:\\n\\nthe adapter configuration (adapter_config.json) and weights (adapter_model.safetensors)—the adapter itself is just 50 MB in size\\n\\nthe training arguments (training_args.bin)\\n\\nthe tokenizer (tokenizer.json and tokenizer.model), its configuration (tokenizer_config.json), and its special tokens (added_tokens.json and speciak_tokens_map.json)\\n\\na README file\\n\\nIf you\\'d like to share your adapter with everyone, you can also push it to the Hugging Face Hub. First, log in using a token that has permission to write:\\n\\nfrom huggingface_hub import login login()\\n\\nThe code above will ask you to enter an access token:\\n\\nA successful login should look like this (pay attention to the permissions):\\n\\nThen, you can use the trainer\\'s push_to_hub() method to upload everything to your account in the Hub. The model will be named after the output_dir argument of the training arguments:\\n\\ntrainer.push_to_hub()\\n\\nThere you go! Our model is out there in the world, and anyone can use it to translate English into Yoda speak.\\n\\nThat\\'s a wrap!\\n\\nDid you like this post? You can learn much more about fine-tuning in my latest book: A Hands-On Guide to Fine-Tuning Large Language Models with PyTorch and Hugging Face.\\n\\nSubscribe Follow Connect\\n\\ndvgodoy.com\\n\\nX\\n\\nGitHub\\n\\nLinkedIn\\n\\nCommunity\\n\\nshoveling42\\n\\nFeb 19\\n\\nThank you for sharing your tutorial!!\\n\\nghostoverflow\\n\\nMar 13\\n\\nHeree\\'s a requirements.txt that worked for me, slightly updated:\\n\\nflash-attn==2.7.4.post1 scipy==1.15.2 torch==2.6.0 cffi==1.17.1 transformers==4.49.0 peft==0.14.0 accelerate==1.5.1 trl==0.15.2 bitsandbytes==0.45.3 datasets==3.3.2 huggingface-hub==0.29.3 safetensors==0.5.3 pandas==2.2.3 matplotlib==3.10.1 numpy==1.26.4\\n\\nsmolgabs\\n\\nJul 2\\n\\nedited Jul 2\\n\\ntysm, can\\'t wait to play around with this!\\n\\nyou explained everything amazingly!\\n\\nit works it works it works!!! <3 <3 <3\\n\\nfor anyone having issues with triton, do \\'pip install triton-windows\\'\\n\\nalso for bitsandbytes issues on windows saying it can\\'t find the cuda, version 0.46 should work.\\n\\nSign up or log in to comment\\n\\nUpvote\\n\\n72\\n\\nBack to Articles\\n\\nFine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face\\n\\nCommunity Article Published February 11, 2025\\n\\nUpvote\\n\\n72\\n\\nDaniel Voigt Godoy\\n\\ndvgodoy\\n\\nThis blog post contains \"Chapter 0: TL;DR\" of my latest book A Hands-On Guide to Fine-Tuning Large Language Models with PyTorch and Hugging Face.\\n\\nSpoilers\\n\\nIn this blog post, we\\'ll get right to it and fine-tune a small language model, Microsoft\\'s Phi-3 Mini 4K Instruct, to translate English into Yoda-speak. You can think of this initial chapter as a recipe you can just follow. It\\'s a \"shoot first, ask questions later\" kind of post.\\n\\nYou\\'ll learn how to:\\n\\nLoad a quantized model using BitsAndBytes\\n\\nConfigure low-rank adapters (LoRA) using Hugging Face\\'s peft\\n\\nLoad and format a dataset\\n\\nFine-tune the model using the supervised fine-tuning trainer (SFTTrainer) from Hugging Face\\'s trl\\n\\nUse the fine-tuned model to generate a sentence\\n\\nJupyter Notebook\\n\\nThe Jupyter notebook corresponding to this post is part of the official Fine-Tuning LLMs repository on GitHub. You can also run it directly in Google Colab\\n\\nSetup\\n\\nIf you\\'re running it on Colab, you\\'ll need to pip install a few libraries: datasets, bitsandbytes, and trl.\\n\\nFor better reproducibility during training, however, use the pinned versions instead:\\n\\n#!pip install datasets bitsandbytes trl !pip install transformers==4.55.2 peft==0.17.0 accelerate==1.10.0 trl==0.21.0 bitsandbytes==0.47.0 datasets==4.0.0 huggingface-hub==0.34.4 safetensors==0.6.2 pandas==2.2.2 matplotlib==3.10.0 numpy==2.0.2\\n\\nImports\\n\\nFor the sake of organization, all libraries needed throughout the code used are imported at its very start. For this post, we\\'ll need the following imports:\\n\\nimport os import torch from datasets import load_dataset from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig from trl import SFTConfig, SFTTrainer\\n\\nLoading a Quantized Base Model\\n\\nWe start by loading a quantized model, so it takes up less space in the GPU\\'s RAM. A quantized model replaces the original weights with approximate values that are represented by fewer bits. The simplest and most straightforward way to quantize a model is to turn its weights from 32-bit floating-point (FP32) numbers into 4-bit floating-point numbers (NF4). This simple yet powerful change already reduces the model\\'s memory footprint by roughly a factor of eight.\\n\\nWe can use an instance of BitsAndBytesConfig as the quantization_config argument while loading a model using the from_pretrained() method. To keep it flexible, so you can try it out with any other model of your choice, we\\'re using Hugging Face\\'s AutoModelForCausalLM. The repo you choose to use determines the model being loaded.\\n\\nWithout further ado, here\\'s our quantized model being loaded:\\n\\nbnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float32 ) repo_id = \\'microsoft/Phi-3-mini-4k-instruct\\' model = AutoModelForCausalLM.from_pretrained( repo_id, device_map=\"cuda:0\", quantization_config=bnb_config )\\n\\n\"The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support.\" Source: Hugging Face Hub\\n\\nOnce the model is loaded, you can see how much space it occupies in memory using the get_memory_footprint() method.\\n\\nprint(model.get_memory_footprint()/1e6)\\n\\n2206.347264\\n\\nEven though it\\'s been quantized, the model still takes up a bit more than 2 gigabytes of RAM. The quantization procedure focuses on the linear layers within the Transformer decoder blocks (also referred to as \"layers\" in some cases):\\n\\nmodel\\n\\nPhi3ForCausalLM( (model): Phi3Model( (embed_tokens): Embedding(32064, 3072, padding_idx=32000) (embed_dropout): Dropout(p=0.0, inplace=False) (layers): ModuleList( (0-31): 32 x Phi3DecoderLayer( (self_attn): Phi3Attention( (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False) <1> (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False) <1> (rotary_emb): Phi3RotaryEmbedding() ) (mlp): Phi3MLP( (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False) <1> (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False) <1> (activation_fn): SiLU() ) (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05) (resid_attn_dropout): Dropout(p=0.0, inplace=False) (resid_mlp_dropout): Dropout(p=0.0, inplace=False) (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05) ) ) (norm): Phi3RMSNorm((3072,), eps=1e-05) ) (lm_head): Linear(in_features=3072, out_features=32064, bias=False) )\\n\\n<1> Quantized layers\\n\\nA quantized model can be used directly for inference, but it cannot be trained any further. Those pesky Linear4bit layers take up much less space, which is the whole point of quantization; however, we cannot update them.\\n\\nWe need to add something else to our mix, a sprinkle of adapters.\\n\\nSetting Up Low-Rank Adapters (LoRA)\\n\\nLow-rank adapters can be attached to each and every one of the quantized layers. The adapters are mostly regular Linear layers that can be easily updated as usual. The clever trick in this case is that these adapters are significantly smaller than the layers that have been quantized.\\n\\nSince the quantized layers are frozen (they cannot be updated), setting up LoRA adapters on a quantized model drastically reduces the total number of trainable parameters to just 1% (or less) of its original size.\\n\\nWe can set up LoRA adapters in three easy steps:\\n\\nCall prepare_model_for_kbit_training() to improve numerical stability during training.\\n\\nCreate an instance of LoraConfig.\\n\\nApply the configuration to the quantized base model using the get_peft_model() method.\\n\\nLet\\'s try it out with our model:\\n\\nmodel = prepare_model_for_kbit_training(model) config = LoraConfig( # the rank of the adapter, the lower the fewer parameters you\\'ll need to train r=8, lora_alpha=16, # multiplier, usually 2*r bias=\"none\", lora_dropout=0.05, task_type=\"CAUSAL_LM\", # Newer models, such as Phi-3 at time of writing, may require # manually setting target modules target_modules=[\\'o_proj\\', \\'qkv_proj\\', \\'gate_up_proj\\', \\'down_proj\\'], ) model = get_peft_model(model, config) model\\n\\nPeftModelForCausalLM( (base_model): LoraModel( (model): Phi3ForCausalLM( (model): Phi3Model( (embed_tokens): Embedding(32064, 3072, padding_idx=32000) (embed_dropout): Dropout(p=0.0, inplace=False) (layers): ModuleList( (0-31): 32 x Phi3DecoderLayer( (self_attn): Phi3Attention( (o_proj): lora.Linear4bit( <1> (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False) (lora_dropout): ModuleDict((default): Dropout(p=0.05, inplace=False)) (lora_A): ModuleDict( (default): Linear(in_features=3072, out_features=8, bias=False) ) (lora_B): ModuleDict( (default): Linear(in_features=8, out_features=3072, bias=False) ) (lora_embedding_A): ParameterDict() (lora_embedding_B): ParameterDict() (lora_magnitude_vector): ModuleDict() ) (qkv_proj): lora.Linear4bit(...) <1> (rotary_emb): Phi3RotaryEmbedding() ) (mlp): Phi3MLP( (gate_up_proj): lora.Linear4bit(...) <1> (down_proj): lora.Linear4bit(...) <1> (activation_fn): SiLU() ) (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05) (resid_attn_dropout): Dropout(p=0.0, inplace=False) (resid_mlp_dropout): Dropout(p=0.0, inplace=False) (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05) ) ) (norm): Phi3RMSNorm((3072,), eps=1e-05) ) (lm_head): Linear(in_features=3072, out_features=32064, bias=False) ) ) )\\n\\n<1> LoRA adapters\\n\\nThe output of the other three LoRA layers (qkv_proj, gate_up_proj, and down_proj) was suppressed to shorten the output.\\n\\nDid you get the following error? ValueError: Please specify `target_modules` in `peft_config` Most likely, you don\\'t need to specify the target_modules if you\\'re using one of the well-known models. The peft library takes care of it by automatically choosing the appropriate targets. However, there may be a gap between the time a popular model is released and the time the library gets updated. So, if you get the error above, look for the quantized layers in your model and list their names in the target_modules argument.\\n\\nThe quantized layers (Linear4bit) have turned into lora.Linear4bit modules where the quantized layer itself became the base_layer with some regular Linear layers (lora_A and lora_B) added to the mix.\\n\\nThese extra layers would make the model only slightly larger. However, the model preparation function (prepare_model_for_kbit_training()) turned every non-quantized layer to full precision (FP32), thus resulting in a 30% larger model:\\n\\nprint(model.get_memory_footprint()/1e6)\\n\\n2651.080704\\n\\nSince most parameters are frozen, only a tiny fraction of the total number of parameters are currently trainable, thanks to LoRA!\\n\\ntrain_p, tot_p = model.get_nb_trainable_parameters() print(f\\'Trainable parameters: {train_p/1e6:.2f}M\\') print(f\\'Total parameters: {tot_p/1e6:.2f}M\\') print(f\\'% of trainable parameters: {100*train_p/tot_p:.2f}%\\')\\n\\nTrainable parameters: 12.58M Total parameters: 3833.66M % of trainable parameters: 0.33%\\n\\nThe model is ready to be fine-tuned, but we are still missing one key component: our dataset.\\n\\nFormatting Your Dataset\\n\\n\"Like Yoda, speak, you must. Hrmmm.\" Master Yoda\\n\\nThe dataset yoda_sentences consists of 720 sentences translated from English to Yoda-speak. The dataset is hosted on the Hugging Face Hub and we can easily load it using the load_dataset() method from the Hugging Face datasets library:\\n\\ndataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\") dataset\\n\\nDataset({ features: [\\'sentence\\', \\'translation\\', \\'translation_extra\\'], num_rows: 720 })\\n\\nThe dataset has three columns:\\n\\noriginal English sentence (sentence)\\n\\nbasic translation to Yoda-speak (translation)\\n\\nenhanced translation including typical Yesss and Hrrmm interjections (translation_extra)\\n\\ndataset[0]\\n\\n{\\'sentence\\': \\'The birch canoe slid on the smooth planks.\\', \\'translation\\': \\'On the smooth planks, the birch canoe slid.\\', \\'translation_extra\\': \\'On the smooth planks, the birch canoe slid. Yes, hrrrm.\\'}\\n\\nThe SFTTrainer we\\'ll be using to fine-tune the model can automatically handle datasets in conversational format.\\n\\n{\"messages\":[ {\"role\": \"system\", \"content\": \"<general directives>\"}, {\"role\": \"user\", \"content\": \"<prompt text>\"}, {\"role\": \"assistant\", \"content\": \"<ideal generated text>\"} ]}\\n\\nIMPORTANT UPDATE: unfortunately, in more recent versions of the trl library, the \"instruction\" format is not properly supported anymore, thus leading to the chat template not being applied to the dataset. In order to avoid this issue, we can convert the dataset to the \"conversational\" format.\\n\\nSo, we\\'ll convert the dataset to the conversational format using the format_dataset() function below:\\n\\n# Adapted from trl.extras.dataset_formatting.instructions_formatting_function # Converts dataset from prompt/completion format (not supported anymore) # to the conversational format def format_dataset(examples): if isinstance(examples[\"prompt\"], list): output_texts = [] for i in range(len(examples[\"prompt\"])): converted_sample = [ {\"role\": \"user\", \"content\": examples[\"prompt\"][i]}, {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]}, ] output_texts.append(converted_sample) return {\\'messages\\': output_texts} else: converted_sample = [ {\"role\": \"user\", \"content\": examples[\"prompt\"]}, {\"role\": \"assistant\", \"content\": examples[\"completion\"]}, ] return {\\'messages\\': converted_sample}\\n\\ndataset = dataset.rename_column(\"sentence\", \"prompt\") dataset = dataset.rename_column(\"translation_extra\", \"completion\") dataset = dataset.map(format_dataset) dataset = dataset.remove_columns([\\'prompt\\', \\'completion\\', \\'translation\\']) messages = dataset[0][\\'messages\\'] messages\\n\\n[{\\'role\\': \\'user\\', \\'content\\': \\'The birch canoe slid on the smooth planks.\\'}, {\\'role\\': \\'assistant\\', \\'content\\': \\'On the smooth planks, the birch canoe slid. Yes, hrrrm.\\'}]\\n\\nTokenizer\\n\\nBefore moving into the actual training, we still need to load the tokenizer that corresponds to our model. The tokenizer is an important part of this process, determining how to convert text into tokens in the same way used to train the model.\\n\\nFor instruction/chat models, the tokenizer also contains its corresponding chat template that specifies:\\n\\nWhich special tokens should be used, and where they should be placed.\\n\\nWhere the system directives, user prompt, and model response should be placed.\\n\\nWhat is the generation prompt, that is, the special token that triggers the model\\'s response (more on that in the \"Querying the Model\" section)\\n\\nIMPORTANT UPDATE: due to changes in the default collator used by the SFTTrainer class while building the dataset, the EOS token (which is, in Phi-3, the same as the PAD token) was masked in the labels too thus leading to the model not being able to properly stop token generation.\\n\\nIn order to address this change, we can assign the UNK token to the PAD token, so the EOS token becomes unique and therefore not masked as part of the labels.\\n\\ntokenizer = AutoTokenizer.from_pretrained(repo_id) tokenizer.pad_token = tokenizer.unk_token tokenizer.pad_token_id = tokenizer.unk_token_id tokenizer.chat_template\\n\\n\"{% for message in messages %} {% if message[\\'role\\'] ## \\'system\\' %} {{\\'<|system|>\\\\n\\' + message[\\'content\\'] + \\'<|end|>\\\\n\\'}} {% elif message[\\'role\\'] ## \\'user\\' %} {{\\'<|user|>\\\\n\\' + message[\\'content\\'] + \\'<|end|>\\\\n\\'}} {% elif message[\\'role\\'] ## \\'assistant\\' %} {{\\'<|assistant|>\\\\n\\' + message[\\'content\\'] + \\'<|end|>\\\\n\\'}} {% endif %} {% endfor %} {% if add_generation_prompt %} {{ \\'<|assistant|>\\\\n\\' }}{% else %}{{ eos_token }} {% endif %}\"\\n\\nNever mind the seemingly overcomplicated template (I have added line breaks and indentation to it so it\\'s easier to read). It simply organizes the messages into a coherent block with the appropriate tags, as shown below (tokenize=False ensures we get readable text back instead of a numeric sequence of token IDs):\\n\\nprint(tokenizer.apply_chat_template(messages, tokenize=False))\\n\\n<|user|> The birch canoe slid on the smooth planks.<|end|> <|assistant|> On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|> <|endoftext|>\\n\\nNotice that each interaction is wrapped in either <|user|> or <|assistant|> tokens at the beginning and <|end|> at the end. Moreover, the <|endoftext|> token indicates the end of the whole block.\\n\\nDifferent models will have different templates and tokens to indicate the beginning and end of sentences and blocks.\\n\\nWe\\'re now ready to tackle the actual fine-tuning!\\n\\nFine-Tuning with SFTTrainer\\n\\nFine-tuning a model, whether large or otherwise, follows exactly the same training procedure as training a model from scratch. We could write our own training loop in pure PyTorch, or we could use Hugging Face\\'s Trainer to fine-tune our model.\\n\\nIt is much easier, however, to use SFTTrainer instead (which uses Trainer underneath, by the way), since it takes care of most of the nitty-gritty details for us, as long as we provide it with the following four arguments:\\n\\na model\\n\\na tokenizer\\n\\na dataset\\n\\na configuration object\\n\\nWe\\'ve already got the first three elements; let\\'s work on the last one.\\n\\nSFTConfig\\n\\nThere are many parameters that we can set in the configuration object. We have divided them into four groups:\\n\\nMemory usage optimization parameters related to gradient accumulation and checkpointing\\n\\nDataset-related arguments, such as the max_seq_length required by your data, and whether you are packing or not the sequences\\n\\nTypical training parameters such as the learning_rate and the num_train_epochs\\n\\nEnvironment and logging parameters such as output_dir (this will be the name of the model if you choose to push it to the Hugging Face Hub once it\\'s trained), logging_dir, and logging_steps.\\n\\nWhile the learning rate is a very important parameter (as a starting point, you can try the learning rate used to train the base model in the first place), it\\'s actually the maximum sequence length that\\'s more likely to cause out-of-memory issues.\\n\\nMake sure to always pick the shortest possible max_seq_length that makes sense for your use case. In ours, the sentences—both in English and Yoda-speak—are quite short, and a sequence of 64 tokens is more than enough to cover the prompt, the completion, and the added special tokens.\\n\\nFlash attention (which, unfortunately, isn\\'t supported in Colab), allows for more flexibility in working with longer sequences, avoiding the potential issue of OOM errors.\\n\\nIMPORTANT UPDATE: The release of trl version 0.20 brought several changes to the SFTConfig:\\n\\npacking is performed differently than it was, unless packing_strategy=\\'wrapped\\' is set;\\n\\nthe max_seq_length argument was renamed to max_length;\\n\\nthe bf16 defaults to True but, at the time of this update (Aug/2025), it didn\\'t check if the BF16 type was actually available or not, so it\\'s included in the configuration now.\\n\\nsft_config = SFTConfig( ## GROUP 1: Memory usage # These arguments will squeeze the most out of your GPU\\'s RAM # Checkpointing gradient_checkpointing=True, # this saves a LOT of memory # Set this to avoid exceptions in newer versions of PyTorch gradient_checkpointing_kwargs={\\'use_reentrant\\': False}, # Gradient Accumulation / Batch size # Actual batch (for updating) is same (1x) as micro-batch size gradient_accumulation_steps=1, # The initial (micro) batch size to start off with per_device_train_batch_size=16, # If batch size would cause OOM, halves its size until it works auto_find_batch_size=True, ## GROUP 2: Dataset-related max_length=64, # renamed in v0.20 # Dataset # packing a dataset means no padding is needed packing=True, packing_strategy=\\'wrapped\\', # added to approximate original packing behavior ## GROUP 3: These are typical training parameters num_train_epochs=10, learning_rate=3e-4, # Optimizer # 8-bit Adam optimizer - doesn\\'t help much if you\\'re using LoRA! optim=\\'paged_adamw_8bit\\', ## GROUP 4: Logging parameters logging_steps=10, logging_dir=\\'./logs\\', output_dir=\\'./phi3-mini-yoda-adapter\\', report_to=\\'none\\'. # ensures bf16 (the new default) is only used when it is actually available bf16=torch.cuda.is_bf16_supported(including_emulation=False) )\\n\\nSFTTrainer\\n\\n\"It is training time!\" The Hulk\\n\\nIMPORTANT UPDATE: The current version of trl (0.21) has a known issue where training fails if the LoRA configuration has already been applied to the model, as the trainer freezes the whole model, including the adapters.\\n\\nHowever, it works as expected when the configuration is passed as the peft_config argument to the trainer, since it is applied after freezing the existing layers.\\n\\nIf the model already contains the adapters, as in our case, training still works, but we need to use the underlying original model instead (model.base_model.model) to ensure the save_model() method functions correctly.\\n\\nWe can now finally create an instance of the supervised fine-tuning trainer:\\n\\ntrainer = SFTTrainer( model=model.base_model.model, # the underlying Phi-3 model peft_config=config, # added to fix issue in TRL>=0.20 processing_class=tokenizer, args=sft_config, train_dataset=dataset, )\\n\\nThe SFTTrainer had already preprocessed our dataset, so we can take a look inside and see how each mini-batch was assembled:\\n\\ndl = trainer.get_train_dataloader() batch = next(iter(dl))\\n\\nLet\\'s check the labels; after all, we didn\\'t provide any, did we?\\n\\nbatch[\\'input_ids\\'][0], batch[\\'labels\\'][0]\\n\\n(tensor([ 1746, 29892, 278, 10435, 3147, 698, 287, 29889, 32007, 32000, 32000, 32010, 10987, 278, 3252, 262, 1058, 380, 1772, 278, 282, 799, 29880, 18873, 1265, 29889, 32007, 32001, 11644, 380, 1772, 278, 282, 799, 29880, 18873, 1265, 29892, 1284, 278, 3252, 262, 29892, 366, 1818, 29889, 3869, 29892, 298, 21478, 1758, 29889, 32007, 32000, 32000, 32010, 315, 329, 278, 13793, 393, 7868, 29879, 278], device=\\'cuda:0\\'), tensor([ 1746, 29892, 278, 10435, 3147, 698, 287, 29889, 32007, 32000, 32000, 32010, 10987, 278, 3252, 262, 1058, 380, 1772, 278, 282, 799, 29880, 18873, 1265, 29889, 32007, 32001, 11644, 380, 1772, 278, 282, 799, 29880, 18873, 1265, 29892, 1284, 278, 3252, 262, 29892, 366, 1818, 29889, 3869, 29892, 298, 21478, 1758, 29889, 32007, 32000, 32000, 32010, 315, 329, 278, 13793, 393, 7868, 29879, 278], device=\\'cuda:0\\'))\\n\\nThe labels were added automatically, and they\\'re exactly the same as the inputs. Thus, this is a case of self-supervised fine-tuning.\\n\\nThe shifting of the labels will be handled automatically as well; there\\'s no need to be concerned about it.\\n\\nAlthough this is a 3.8 billion-parameter model, the configuration above allows us to squeeze training, using a mini-batch of eight, into an old setup with a consumer-grade GPU such as a GTX 1060 with only 6 GB RAM. True story! It takes about 35 minutes to complete the training process.\\n\\nNext, we call the train() method and wait:\\n\\ntrainer.train()\\n\\nStep Training Loss 10 2.990700 20 1.789500 30 1.581700 40 1.458300 50 1.362300 100 0.607900 150 0.353600 200 0.277500 220 0.252400\\n\\nQuerying the Model\\n\\nNow, our model should be able to produce a Yoda-like sentence as a response to any short sentence we give it.\\n\\nSo, the model requires its inputs to be properly formatted. We need to build a list of \"messages\"—ours, from the user, in this case—and prompt the model to answer by indicating it\\'s its turn to write.\\n\\nThis is the purpose of the add_generation_prompt argument: it adds <|assistant|> to the end of the conversation, so the model can predict the next word—and continue doing so until it predicts an <|endoftext|> token.\\n\\nThe helper function below assembles a message (in the conversational format) and applies the chat template to it, appending the generation prompt to its end.\\n\\ndef gen_prompt(tokenizer, sentence): converted_sample = [{\"role\": \"user\", \"content\": sentence}] prompt = tokenizer.apply_chat_template( converted_sample, tokenize=False, add_generation_prompt=True ) return prompt\\n\\nLet\\'s try generating a prompt for an example sentence:\\n\\nsentence = \\'The Force is strong in you!\\' prompt = gen_prompt(tokenizer, sentence) print(prompt)\\n\\n<|user|> The Force is strong in you!<|end|> <|assistant|>\\n\\nThe prompt seems about right; let\\'s use it to generate a completion. The helper function below does the following:\\n\\nIt tokenizes the prompt into a tensor of token IDs (add_special_tokens is set to False because the tokens were already added by the chat template).\\n\\nIt sets the model to evaluation mode.\\n\\nIt calls the model\\'s generate() method to produce the output (generated token IDs).\\n\\nIf the model was trained using mixed-precision, we wrap the generation in the autocast() context manager, which automatically handles conversion between data types.\\n\\nIt decodes the generated token IDs back into readable text.\\n\\ndef generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False): tokenized_input = tokenizer( prompt, add_special_tokens=False, return_tensors=\"pt\" ).to(model.device) model.eval() # if it was trained using mixed precision, uses autocast context ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) \\\\ if model.dtype in [torch.float16, torch.bfloat16] else nullcontext() with ctx: gen_output = model.generate(**tokenized_input, eos_token_id=tokenizer.eos_token_id, max_new_tokens=max_new_tokens) output = tokenizer.batch_decode(gen_output, skip_special_tokens=skip_special_tokens) return output[0]\\n\\nNow, we can finally try out our model and see if it\\'s indeed capable of generating Yoda-speak.\\n\\nprint(generate(model, tokenizer, prompt))\\n\\n<|user|> The Force is strong in you!<|end|><|assistant|> Strong in you, the Force is. Yes, hrrmmm.<|end|>\\n\\nAwesome! It works! Like Yoda, the model speaks. Hrrrmm.\\n\\nCongratulations, you\\'ve fine-tuned your first LLM!\\n\\nNow, you\\'ve got a small adapter that can be loaded into an instance of the Phi-3 Mini 4K Instruct model to turn it into a Yoda translator! How cool is that?\\n\\nSaving the Adapter\\n\\nOnce the training is completed, you can save the adapter (and the tokenizer) to disk by calling the trainer\\'s save_model() method. It will save everything to the specified folder:\\n\\ntrainer.save_model(\\'local-phi3-mini-yoda-adapter\\')\\n\\nThe files that were saved include:\\n\\nthe adapter configuration (adapter_config.json) and weights (adapter_model.safetensors)—the adapter itself is just 50 MB in size\\n\\nthe training arguments (training_args.bin)\\n\\nthe tokenizer (tokenizer.json and tokenizer.model), its configuration (tokenizer_config.json), and its special tokens (added_tokens.json and speciak_tokens_map.json)\\n\\na README file\\n\\nIf you\\'d like to share your adapter with everyone, you can also push it to the Hugging Face Hub. First, log in using a token that has permission to write:\\n\\nfrom huggingface_hub import login login()\\n\\nThe code above will ask you to enter an access token:\\n\\nA successful login should look like this (pay attention to the permissions):\\n\\nThen, you can use the trainer\\'s push_to_hub() method to upload everything to your account in the Hub. The model will be named after the output_dir argument of the training arguments:\\n\\ntrainer.push_to_hub()\\n\\nThere you go! Our model is out there in the world, and anyone can use it to translate English into Yoda speak.\\n\\nThat\\'s a wrap!\\n\\nDid you like this post? You can learn much more about fine-tuning in my latest book: A Hands-On Guide to Fine-Tuning Large Language Models with PyTorch and Hugging Face.\\n\\nSubscribe Follow Connect\\n\\ndvgodoy.com\\n\\nX\\n\\nGitHub\\n\\nLinkedIn\\n\\nCommunity\\n\\nshoveling42\\n\\nFeb 19\\n\\nThank you for sharing your tutorial!!\\n\\nghostoverflow\\n\\nMar 13\\n\\nHeree\\'s a requirements.txt that worked for me, slightly updated:\\n\\nflash-attn==2.7.4.post1 scipy==1.15.2 torch==2.6.0 cffi==1.17.1 transformers==4.49.0 peft==0.14.0 accelerate==1.5.1 trl==0.15.2 bitsandbytes==0.45.3 datasets==3.3.2 huggingface-hub==0.29.3 safetensors==0.5.3 pandas==2.2.3 matplotlib==3.10.1 numpy==1.26.4\\n\\nsmolgabs\\n\\nJul 2\\n\\nedited Jul 2\\n\\ntysm, can\\'t wait to play around with this!\\n\\nyou explained everything amazingly!\\n\\nit works it works it works!!! <3 <3 <3\\n\\nfor anyone having issues with triton, do \\'pip install triton-windows\\'\\n\\nalso for bitsandbytes issues on windows saying it can\\'t find the cuda, version 0.46 should work.\\n\\nSign up or log in to comment\\n\\nUpvote\\n\\n72', 'text_length': 52998, 'message': 'Retrieved 10 pages for blog_dvgodoy_fine-tuning-llm-hugging-face_20251003_161407.html'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `generate_summary` with `This blog post by Daniel Voigt Godoy provides a practical guide to fine-tuning a large language model (LLM) using PyTorch and Hugging Face tools. The tutorial focuses on fine-tuning Microsoft's Phi-3 Mini 4K Instruct model to translate English into Yoda-speak. Key steps include loading a quantized model to reduce memory usage, setting up low-rank adapters (LoRA) to minimize trainable parameters, and using Hugging Face's SFTTrainer for supervised fine-tuning. The tutorial emphasizes the importance of dataset formatting and tokenizer configuration, particularly for conversational AI models. The process involves converting datasets to a conversational format and using a tokenizer that aligns with the model's training. The guide also highlights the significance of memory optimization and configuration settings in the fine-tuning process. After training, the model can generate Yoda-like sentences, demonstrating the effectiveness of the fine-tuning process. The post concludes with instructions on saving and sharing the fine-tuned model using Hugging Face Hub, making it accessible for broader use.`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m{'status': 'success', 'summary': \"This blog post by Daniel Voigt Godoy outlines a practical approach to fine-tuning a large language model (LLM) using PyTorch and Hugging Face tools, focusing on Microsoft's Phi-3 Mini 4K Instruct model for translating English into Yoda-speak. Notable advancements include the use of quantized models to reduce memory usage and low-rank adapters (LoRA) to minimize trainable parameters, enhancing efficiency. Performance improvements are achieved through Hugging Face's SFTTrainer for supervised fine-tuning, emphasizing the importance of dataset formatting and tokenizer configuration for conversational AI models. The process involves converting datasets to a conversational format and aligning the tokenizer with the model's training, optimizing memory and configuration settings. The fine-tuned model successfully generates Yoda-like sentences, showcasing the effectiveness of the approach. Practical applications include the potential for broader use in conversational AI, with the model being made accessible via Hugging Face Hub. This demonstrates significant industry impact by facilitating the development and sharing of specialized language models.\", 'length': 1157, 'message': 'Generated summary (1157 characters)'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `collapse_summaries` with `[{\"summary\":\"The paper introduces a novel method called Laplacian Eigenvector Gradient Orthogonalization (LEGO) for estimating tangent spaces on data manifolds, addressing the limitations of traditional methods like Local Principal Component Analysis (LPCA) in noisy environments. LEGO utilizes graph Laplacian eigenvectors to leverage the global structure of data, orthogonalizing gradients of low-frequency eigenvectors to enhance the accuracy of tangent space estimates. Theoretical analyses confirm LEGO's robustness against noise, supported by differential geometry and random matrix theory. Empirical results show that LEGO outperforms LPCA in manifold learning, boundary detection, and local intrinsic dimension estimation. This advancement is significant for AI and data science professionals, offering a more reliable method for geometric data analysis in noisy conditions. Consequently, LEGO has the potential to improve machine learning applications where accurate data manifold estimation is crucial, impacting industries reliant on precise data analysis.\"}, {\"summary\":\"The paper presents KAVA, a groundbreaking framework for latent reasoning in large language models (LLMs) that utilizes compressed Key-Value (KV) cache distillation. This approach addresses the high computational cost of traditional chain-of-thought (CoT) reasoning by offering an efficient alternative without sacrificing accuracy. KAVA achieves this by transferring knowledge from a teacher model's compressed KV-cache to a latent reasoning student, aligning stepwise KV trajectories with continuous latent tokens. The framework not only maintains the accuracy of CoT-trained models but also enhances efficiency and scalability. KAVA demonstrates superior performance compared to existing latent reasoning methods, with minimal performance loss when shifting from equation-only to natural-language traces. It effectively scales to larger models, providing a scalable supervision signal that combines CoT accuracy with latent inference efficiency. This advancement is crucial for AI professionals aiming to develop efficient and deployable reasoning models, impacting practical applications and industry by offering a more scalable and efficient reasoning framework.\"}, {\"summary\":\"This blog post by Daniel Voigt Godoy outlines a practical approach to fine-tuning a large language model (LLM) using PyTorch and Hugging Face tools, focusing on Microsoft's Phi-3 Mini 4K Instruct model for translating English into Yoda-speak. Notable advancements include the use of quantized models to reduce memory usage and low-rank adapters (LoRA) to minimize trainable parameters, enhancing efficiency. Performance improvements are achieved through Hugging Face's SFTTrainer for supervised fine-tuning, emphasizing the importance of dataset formatting and tokenizer configuration for conversational AI models. The process involves converting datasets to a conversational format and aligning the tokenizer with the model's training, optimizing memory and configuration settings. The fine-tuned model successfully generates Yoda-like sentences, showcasing the effectiveness of the approach. Practical applications include the potential for broader use in conversational AI, with the model being made accessible via Hugging Face Hub. This demonstrates significant industry impact by facilitating the development and sharing of specialized language models.\"} ]`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m{'status': 'success', 'collapsed_summaries': [{'summary': \"The paper introduces a novel method called Laplacian Eigenvector Gradient Orthogonalization (LEGO) for estimating tangent spaces on data manifolds, addressing the limitations of traditional methods like Local Principal Component Analysis (LPCA) in noisy environments. LEGO utilizes graph Laplacian eigenvectors to leverage the global structure of data, orthogonalizing gradients of low-frequency eigenvectors to enhance the accuracy of tangent space estimates. Theoretical analyses confirm LEGO's robustness against noise, supported by differential geometry and random matrix theory. Empirical results show that LEGO outperforms LPCA in manifold learning, boundary detection, and local intrinsic dimension estimation. This advancement is significant for AI and data science professionals, offering a more reliable method for geometric data analysis in noisy conditions. Consequently, LEGO has the potential to improve machine learning applications where accurate data manifold estimation is crucial, impacting industries reliant on precise data analysis.\"}, {'summary': \"The paper presents KAVA, a groundbreaking framework for latent reasoning in large language models (LLMs) that utilizes compressed Key-Value (KV) cache distillation. This approach addresses the high computational cost of traditional chain-of-thought (CoT) reasoning by offering an efficient alternative without sacrificing accuracy. KAVA achieves this by transferring knowledge from a teacher model's compressed KV-cache to a latent reasoning student, aligning stepwise KV trajectories with continuous latent tokens. The framework not only maintains the accuracy of CoT-trained models but also enhances efficiency and scalability. KAVA demonstrates superior performance compared to existing latent reasoning methods, with minimal performance loss when shifting from equation-only to natural-language traces. It effectively scales to larger models, providing a scalable supervision signal that combines CoT accuracy with latent inference efficiency. This advancement is crucial for AI professionals aiming to develop efficient and deployable reasoning models, impacting practical applications and industry by offering a more scalable and efficient reasoning framework.\"}, {'summary': \"This blog post by Daniel Voigt Godoy outlines a practical approach to fine-tuning a large language model (LLM) using PyTorch and Hugging Face tools, focusing on Microsoft's Phi-3 Mini 4K Instruct model for translating English into Yoda-speak. Notable advancements include the use of quantized models to reduce memory usage and low-rank adapters (LoRA) to minimize trainable parameters, enhancing efficiency. Performance improvements are achieved through Hugging Face's SFTTrainer for supervised fine-tuning, emphasizing the importance of dataset formatting and tokenizer configuration for conversational AI models. The process involves converting datasets to a conversational format and aligning the tokenizer with the model's training, optimizing memory and configuration settings. The fine-tuned model successfully generates Yoda-like sentences, showcasing the effectiveness of the approach. Practical applications include the potential for broader use in conversational AI, with the model being made accessible via Hugging Face Hub. This demonstrates significant industry impact by facilitating the development and sharing of specialized language models.\"}], 'message': 'Summaries already fit in context (844 tokens). No collapse needed.'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_document_text` with `2510v02307v1.pdf`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m{'status': 'success', 'filename': '2510v02307v1.pdf', 'pages': 20, 'text': 'NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation\\n\\nRuozhen He Moayed Haji-Ali Ziyan Yang Vicente Ordonez Rice University\\n\\n{catherine.he, mh155, zy47, vicenteor}@rice.edu\\n\\n5 2 0 2 t c O 2 ] V C . s c [ 1 v 7 0 3 2 0 . 0 1 5 2 : v i X r a\\n\\nAbstract\\n\\nText-to-image diffusion models trained on a fixed set of reso- lutions often fail to generalize, even when asked to generate images at lower resolutions than those seen during train- ing. High-resolution text-to-image generators are currently unable to easily offer an out-of-the-box budget-efficient al- ternative to their users who might not need high-resolution images. We identify a key technical insight in diffusion mod- els that when addressed can help tackle this limitation: Noise schedulers have unequal perceptual effects across resolu- tions. The same level of noise removes disproportionately more signal from lower-resolution images than from high- resolution images, leading to a train–test mismatch. We propose NoiseShift, a training-free method that recalibrates the noise level of the denoiser conditioned on resolution size. NoiseShift requires no changes to model architecture or sampling schedule and is compatible with existing mod- els. When applied to Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by 10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent artifacts and enhancing the quality of low-resolution image generation.\\n\\n1. Introduction\\n\\nDiffusion models [28, 29, 32] have emerged as a powerful class of generative models, achieving state-of-the-art perfor- mance in generating high-fidelity images [36–38]. However, they struggle to generalize to resolutions other than those seen during the final training stages. Despite being archi- tecturally capable of handling multiple resolutions, these models struggle at lower resolutions, producing low-fidelity images with artifacts and incoherent content [12]. Generat- ing images at lower resolutions reduces computational cost,\\n\\nmaking it preferable for real-world scenarios that demand fast, lightweight generation—such as mobile deployment, rapid prototyping, and large-scale sampling.\\n\\nLow-resolution generation is not just a fallback for limited compute environments, but it is foundational to efficiency and scalability. Diffusion models require iterative denoising over hundreds of steps, and the cost grows quadratically with resolution. A common training recipe to train such models in- volves progressively increasing resolution, starting with low- resolution inputs and fine-tuning on higher-resolution data. Despite being pre-trained on lower-resolution data, models fine-tuned at high resolution often lose their ability to gener- ate high-quality low-resolution images. Recent studies show that diffusion models degrade significantly when sampling at resolutions outside the final fine-tuning set—even when those low resolutions appeared earlier in training [7, 12, 40]. These failures are often attributed to architectural limitations in generalizing across spatial scales [28, 33].\\n\\nPrevious attempts aim to improve resolution general- ization by fine-tuning lightweight adapters for each spec- ified resolution [5, 30]. However, such methods are com- putationally expensive and impractical for current large scale state-of-the-art text-to-image models [6, 9]. Other research proposed training-free ad-hoc and aim to constrain the model prediction at resolutions from the post-training set [2, 12, 17, 31, 39, 40], or generate the images in multiple stages [7]. However, these solutions often increase inference cost and focus mostly on high-resolution synthesis.\\n\\nA more subtle but critical issue is exposure bias—the dis- crepancy between training-time noise distributions and those seen during inference [18, 21, 41]. Recent work mitigates this by altering the inference noise schedule [21]. These fixes apply across resolutions, but they do not fully resolve the underlying perceptual mismatch.\\n\\nIn this paper, we show that the exposure bias is magni- fied at lower resolutions. Although the signal-to-noise ratio (SNR) is nominally fixed across resolutions for all diffusion timesteps, the same absolute noise power erases a larger fraction of perceptual signal in a low-resolution image than in a high-resolution variant [16]. It has been shown that low-\\n\\nNoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation\\n\\nRuozhen He Moayed Haji-Ali Ziyan Yang Vicente Ordonez Rice University\\n\\n{catherine.he, mh155, zy47, vicenteor}@rice.edu\\n\\n5 2 0 2 t c O 2 ] V C . s c [ 1 v 7 0 3 2 0 . 0 1 5 2 : v i X r a\\n\\nAbstract\\n\\nText-to-image diffusion models trained on a fixed set of reso- lutions often fail to generalize, even when asked to generate images at lower resolutions than those seen during train- ing. High-resolution text-to-image generators are currently unable to easily offer an out-of-the-box budget-efficient al- ternative to their users who might not need high-resolution images. We identify a key technical insight in diffusion mod- els that when addressed can help tackle this limitation: Noise schedulers have unequal perceptual effects across resolu- tions. The same level of noise removes disproportionately more signal from lower-resolution images than from high- resolution images, leading to a train–test mismatch. We propose NoiseShift, a training-free method that recalibrates the noise level of the denoiser conditioned on resolution size. NoiseShift requires no changes to model architecture or sampling schedule and is compatible with existing mod- els. When applied to Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by 10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent artifacts and enhancing the quality of low-resolution image generation.\\n\\n1. Introduction\\n\\nDiffusion models [28, 29, 32] have emerged as a powerful class of generative models, achieving state-of-the-art perfor- mance in generating high-fidelity images [36–38]. However, they struggle to generalize to resolutions other than those seen during the final training stages. Despite being archi- tecturally capable of handling multiple resolutions, these models struggle at lower resolutions, producing low-fidelity images with artifacts and incoherent content [12]. Generat- ing images at lower resolutions reduces computational cost,\\n\\nmaking it preferable for real-world scenarios that demand fast, lightweight generation—such as mobile deployment, rapid prototyping, and large-scale sampling.\\n\\nLow-resolution generation is not just a fallback for limited compute environments, but it is foundational to efficiency and scalability. Diffusion models require iterative denoising over hundreds of steps, and the cost grows quadratically with resolution. A common training recipe to train such models in- volves progressively increasing resolution, starting with low- resolution inputs and fine-tuning on higher-resolution data. Despite being pre-trained on lower-resolution data, models fine-tuned at high resolution often lose their ability to gener- ate high-quality low-resolution images. Recent studies show that diffusion models degrade significantly when sampling at resolutions outside the final fine-tuning set—even when those low resolutions appeared earlier in training [7, 12, 40]. These failures are often attributed to architectural limitations in generalizing across spatial scales [28, 33].\\n\\nPrevious attempts aim to improve resolution general- ization by fine-tuning lightweight adapters for each spec- ified resolution [5, 30]. However, such methods are com- putationally expensive and impractical for current large scale state-of-the-art text-to-image models [6, 9]. Other research proposed training-free ad-hoc and aim to constrain the model prediction at resolutions from the post-training set [2, 12, 17, 31, 39, 40], or generate the images in multiple stages [7]. However, these solutions often increase inference cost and focus mostly on high-resolution synthesis.\\n\\nA more subtle but critical issue is exposure bias—the dis- crepancy between training-time noise distributions and those seen during inference [18, 21, 41]. Recent work mitigates this by altering the inference noise schedule [21]. These fixes apply across resolutions, but they do not fully resolve the underlying perceptual mismatch.\\n\\nIn this paper, we show that the exposure bias is magni- fied at lower resolutions. Although the signal-to-noise ratio (SNR) is nominally fixed across resolutions for all diffusion timesteps, the same absolute noise power erases a larger fraction of perceptual signal in a low-resolution image than in a high-resolution variant [16]. It has been shown that low-\\n\\n(a) Visual comparison of forward noise at fixed σt across resolutions at timesteps t = 10,15,20,25,30,35. The same noise level leads to perceptually stronger corruption at lower resolutions.\\n\\n(b) SSIM between clean image x0 and forward-noised image xt at different resolutions. The same σt induces lower SSIM at lower resolutions.\\n\\nFigure 1. Resolution-dependent perceptual effect of noise. At the same sampling noise level σt, lower-resolution images experience more severe visual and structural corruption than high-resolution counterparts.\\n\\nresolution images lose semantic details more rapidly due to pixel aggregation, while high-resolution images retain details due to spatial redundancy [16] (see Figure 1) Consequently, for any given timestep t, a model must denoise images drawn from a distribution increasingly distant from the one it was trained on, leading to a pronounced loss in fidelity. The time-shifting sampling schedule can provide a remedy to reduce the exposure bias for each resolution [6], but the timestep value passed as a conditioning input remains un- changed. Therefore, the model still gets a noise distribution that it never saw during training, leaving a residual train-test mismatch. This limitation motivates our resolution-aware method that goes beyond scheduler adjustments alone.\\n\\nWe propose NoiseShift, a simple, training-free test-time calibration method that aligns the reverse diffusion process with a resolution-dependent distribution. The key to our method is to control the noise level conditioning of the pre- trained diffusion model to align the predictions across reso- lutions. NoiseShift adjusts the noise-level condition to better reflect the perceptual noise in the image based on its res- olution, thereby reducing exposure bias when generating lower-resolution images.\\n\\nSpecifically, we perform a coarse-to-fine grid search to learn a resolution-specific mapping of the conditioning value that minimizes the denoising error prediction, effectively substituting the nominal timestep t with a surrogate timestep ˜t whose noise distribution matches those seen during train- ing. At inference, we preserve the standard schedule but feed the network this calibrated timestep conditioning, aligning the reverse process with the true noise distribution, mitigat- ing exposure bias, and improving overall quality. NoiseShift\\n\\nis lightweight, simple, training-free, and provides consis- tent improvements. When applied to SD3.5 [9], NoiseShift improves FID by an average of 15.89% on LAION-COCO and 10.36% on CelebA. We further validate NoiseShift on Flux [6] and obtain consistent improvements of an average 2.44% FID on LAION-COCO and 3.02% FID on CelebA.\\n\\n2. Related Work\\n\\nDiffusion Models at Arbitrary Resolutions. Most dif- fusion models are trained at a single, fixed size, but real applications demand flexible outputs. Multiple works have been proposed to adapt pretrained diffusion models to variable resolutions in a training or training-free man- ner [5, 7, 10, 31, 39, 40]. Multidiffusion [2] stitches mul- tiple overlapping diffusion windows, enabling panoramas and extreme aspect ratios without retraining. ElasticDiffu- sion [12] separates global structure and local detail during decoding, allowing a pretrained model to scale up or down in resolution while remaining artifact-free. ScaleCrafter [13] enlarges the pretrained diffusion model receptive field on- the-fly via re-dilation and couples it with noise-damped guid- ance, achieving 4K resolution from a 512p model. These methods highlight that inference-time adaptations can un- lock arbitrary-size synthesis, yet none address the noise-level misalignment that emerges when the same schedule is reused across resolutions.\\n\\nTest-Time Calibration and Denoising Consistency. A complementary direction of research focuses on inference- time strategies to calibrate the diffusion process and im- prove the generation quality. Chen et al. [4] show that\\n\\n(a) Visual comparison of forward noise at fixed σt across resolutions at timesteps t = 10,15,20,25,30,35. The same noise level leads to perceptually stronger corruption at lower resolutions.\\n\\n(b) SSIM between clean image x0 and forward-noised image xt at different resolutions. The same σt induces lower SSIM at lower resolutions.\\n\\nFigure 1. Resolution-dependent perceptual effect of noise. At the same sampling noise level σt, lower-resolution images experience more severe visual and structural corruption than high-resolution counterparts.\\n\\nresolution images lose semantic details more rapidly due to pixel aggregation, while high-resolution images retain details due to spatial redundancy [16] (see Figure 1) Consequently, for any given timestep t, a model must denoise images drawn from a distribution increasingly distant from the one it was trained on, leading to a pronounced loss in fidelity. The time-shifting sampling schedule can provide a remedy to reduce the exposure bias for each resolution [6], but the timestep value passed as a conditioning input remains un- changed. Therefore, the model still gets a noise distribution that it never saw during training, leaving a residual train-test mismatch. This limitation motivates our resolution-aware method that goes beyond scheduler adjustments alone.\\n\\nWe propose NoiseShift, a simple, training-free test-time calibration method that aligns the reverse diffusion process with a resolution-dependent distribution. The key to our method is to control the noise level conditioning of the pre- trained diffusion model to align the predictions across reso- lutions. NoiseShift adjusts the noise-level condition to better reflect the perceptual noise in the image based on its res- olution, thereby reducing exposure bias when generating lower-resolution images.\\n\\nSpecifically, we perform a coarse-to-fine grid search to learn a resolution-specific mapping of the conditioning value that minimizes the denoising error prediction, effectively substituting the nominal timestep t with a surrogate timestep ˜t whose noise distribution matches those seen during train- ing. At inference, we preserve the standard schedule but feed the network this calibrated timestep conditioning, aligning the reverse process with the true noise distribution, mitigat- ing exposure bias, and improving overall quality. NoiseShift\\n\\nis lightweight, simple, training-free, and provides consis- tent improvements. When applied to SD3.5 [9], NoiseShift improves FID by an average of 15.89% on LAION-COCO and 10.36% on CelebA. We further validate NoiseShift on Flux [6] and obtain consistent improvements of an average 2.44% FID on LAION-COCO and 3.02% FID on CelebA.\\n\\n2. Related Work\\n\\nDiffusion Models at Arbitrary Resolutions. Most dif- fusion models are trained at a single, fixed size, but real applications demand flexible outputs. Multiple works have been proposed to adapt pretrained diffusion models to variable resolutions in a training or training-free man- ner [5, 7, 10, 31, 39, 40]. Multidiffusion [2] stitches mul- tiple overlapping diffusion windows, enabling panoramas and extreme aspect ratios without retraining. ElasticDiffu- sion [12] separates global structure and local detail during decoding, allowing a pretrained model to scale up or down in resolution while remaining artifact-free. ScaleCrafter [13] enlarges the pretrained diffusion model receptive field on- the-fly via re-dilation and couples it with noise-damped guid- ance, achieving 4K resolution from a 512p model. These methods highlight that inference-time adaptations can un- lock arbitrary-size synthesis, yet none address the noise-level misalignment that emerges when the same schedule is reused across resolutions.\\n\\nTest-Time Calibration and Denoising Consistency. A complementary direction of research focuses on inference- time strategies to calibrate the diffusion process and im- prove the generation quality. Chen et al. [4] show that\\n\\ndefault linear or cosine schedules leave a residual signal and propose schedule rescaling to close this train–test gap. WSNR Sampling [11] refines the schedule to keep a con- stant weighted SNR across domains, boosting high-res fi- delity. ScaleCrafter [13] further introduces noise-dependent classifier-free guidance, lowering guidance in early noisy steps and increasing it later. Time-shifting [21] adjusts the sampling scheduler during inference to reduce exposure bias. High-order ODE solvers such as the DPM-Solver [25, 26] shorten the sampling trajectory while preserving consistency. Our work is complementary, instead of modifying the global schedule or the sampler, we re-index the timestep embedding to fool the model into operating at the correct noise level for each resolution.\\n\\nPerceptual Effects of Noise Across Resolutions. Reso- lution changes alter how noise corrupts perceptual content. Jin et al. [19] observe that when prompting a pretrained dif- fusion model to generate images outside their training set, low-res images lose fine details, whereas high-res outputs duplicate objects, and proposed an entropy-based attention scale to mitigate this issue. ScaleCrafter [13] and ElasticD- iffusion [12] report similar artifacts and attribute them to a limited receptive field. Chen et al. [3] quantify that a fixed noise percentage degrades 256×256 images far more than 1024 ×1024 images, motivating scale-aware schedules, which several subsequent work formalized [11, 21]. These studies underline that identical timesteps correspond to dif- ferent perceptual noise levels across resolutions, a mismatch NoiseShift explicitly corrects.\\n\\n3. Method\\n\\nThis section presents our resolution-aware test-time cali- bration approach for flow-matching diffusion models. We aim to optimize the forward–reverse misalignment observed when sampling at resolutions different from the training reso- lution. We begin by reviewing the flow matching framework (Sec. 3.1). We then analyze how resolution affects forward- reverse consistency (Sec. 3.2), and introduce NoiseShift, our training-free method to calibrate the conditioning noise level through coarse-to-fine grid search (Sec. 3.3). This calibration is performed once per resolution and reused during inference without modifying the model or the noise schedule.\\n\\n3.1. Preliminaries: Flow Matching\\n\\nFlow matching [22, 23] is a training paradigm for genera- tive models that learns a continuous transformation from a simple base distribution p0(x) (e.g., Gaussian noise) to a complex target distribution q(x). It directly regresses the velocity field of an ordinary differential equation (ODE), enabling simulation-free learning of the generative process. The trajectory is defined as a continuous interpolation be-\\n\\ntween a noisy sample x0 ∼ p0 and a data point x1 ∼ q along a predefined path.\\n\\nThe training objective minimizes the discrepancy between a predicted velocity vt(xt) and a target velocity ut(xt | x1), which is analytically derived from the interpolation path:\\n\\nwhere xt follows a time-dependent interpolant between x0 and x1, such as:\\n\\nThis framework has been adopted in recent diffusion transformers such as Stable Diffusion 3 [1] and Flux [6], which we study in this paper. These models generate images by integrating the learned velocity field over a predefined noise schedule {σt}, where our method intervenes at test time to correct resolution-specific inconsistencies.\\n\\n3.2.Train-TestMisalignmentinDiffusionSampling\\n\\nWhile flow-matching diffusion models are trained to sim- ulate noise removal, the reverse denoising process at test time can deviate from the original forward trajectory. This discrepancy introduces a hidden source of error, particularly at resolutions different from those used during training.\\n\\nForward–Reverse Inconsistency at Test Time. In prin- ciple, a diffusion model trained via flow matching learns to reverse the forward noise process. However, we observe a systematic discrepancy between the noisy sample xt ob- tained by adding noise to a clean image x0, and the predic- tion ˆxt produced by a single-step denoising operation:\\n\\nEven when the same sampling noise level σt is used, the denoised prediction does not align with the expected noisy image, revealing a time inconsistency in the learned velocity field.\\n\\nTo evaluate this effect, we progressively add noise to a clean image to obtain a trajectory x0,x1,...,xT. At a selected timestep t, we apply a single reverse Euler step to xt to obtain ˆxt−1, then compare it to the ground-truth forward image xt−1. We perform it on a set of random images and compute the mean squared error (MSE) between ˆxt−1 and xt−1.\\n\\nAs shown in Figure 2, this discrepancy exists even at the training resolution and becomes more severe as image resolution diverges. Panel (a) illustrates the conceptual gap between forward and reverse operations, while panel (b) shows that the MSE grows sharply at early steps and at lower resolutions. These findings suggest that the model’s denois- ing behavior is not intrinsically aligned with the resolution- invariant assumptions of the noise schedule.\\n\\n(1)\\n\\n(2)\\n\\n(3)\\n\\ndefault linear or cosine schedules leave a residual signal and propose schedule rescaling to close this train–test gap. WSNR Sampling [11] refines the schedule to keep a con- stant weighted SNR across domains, boosting high-res fi- delity. ScaleCrafter [13] further introduces noise-dependent classifier-free guidance, lowering guidance in early noisy steps and increasing it later. Time-shifting [21] adjusts the sampling scheduler during inference to reduce exposure bias. High-order ODE solvers such as the DPM-Solver [25, 26] shorten the sampling trajectory while preserving consistency. Our work is complementary, instead of modifying the global schedule or the sampler, we re-index the timestep embedding to fool the model into operating at the correct noise level for each resolution.\\n\\nPerceptual Effects of Noise Across Resolutions. Reso- lution changes alter how noise corrupts perceptual content. Jin et al. [19] observe that when prompting a pretrained dif- fusion model to generate images outside their training set, low-res images lose fine details, whereas high-res outputs duplicate objects, and proposed an entropy-based attention scale to mitigate this issue. ScaleCrafter [13] and ElasticD- iffusion [12] report similar artifacts and attribute them to a limited receptive field. Chen et al. [3] quantify that a fixed noise percentage degrades 256×256 images far more than 1024 ×1024 images, motivating scale-aware schedules, which several subsequent work formalized [11, 21]. These studies underline that identical timesteps correspond to dif- ferent perceptual noise levels across resolutions, a mismatch NoiseShift explicitly corrects.\\n\\n3. Method\\n\\nThis section presents our resolution-aware test-time cali- bration approach for flow-matching diffusion models. We aim to optimize the forward–reverse misalignment observed when sampling at resolutions different from the training reso- lution. We begin by reviewing the flow matching framework (Sec. 3.1). We then analyze how resolution affects forward- reverse consistency (Sec. 3.2), and introduce NoiseShift, our training-free method to calibrate the conditioning noise level through coarse-to-fine grid search (Sec. 3.3). This calibration is performed once per resolution and reused during inference without modifying the model or the noise schedule.\\n\\n3.1. Preliminaries: Flow Matching\\n\\nFlow matching [22, 23] is a training paradigm for genera- tive models that learns a continuous transformation from a simple base distribution p0(x) (e.g., Gaussian noise) to a complex target distribution q(x). It directly regresses the velocity field of an ordinary differential equation (ODE), enabling simulation-free learning of the generative process. The trajectory is defined as a continuous interpolation be-\\n\\ntween a noisy sample x0 ∼ p0 and a data point x1 ∼ q along a predefined path.\\n\\nThe training objective minimizes the discrepancy between a predicted velocity vt(xt) and a target velocity ut(xt | x1), which is analytically derived from the interpolation path:\\n\\nwhere xt follows a time-dependent interpolant between x0 and x1, such as:\\n\\nThis framework has been adopted in recent diffusion transformers such as Stable Diffusion 3 [1] and Flux [6], which we study in this paper. These models generate images by integrating the learned velocity field over a predefined noise schedule {σt}, where our method intervenes at test time to correct resolution-specific inconsistencies.\\n\\n3.2.Train-TestMisalignmentinDiffusionSampling\\n\\nWhile flow-matching diffusion models are trained to sim- ulate noise removal, the reverse denoising process at test time can deviate from the original forward trajectory. This discrepancy introduces a hidden source of error, particularly at resolutions different from those used during training.\\n\\nForward–Reverse Inconsistency at Test Time. In prin- ciple, a diffusion model trained via flow matching learns to reverse the forward noise process. However, we observe a systematic discrepancy between the noisy sample xt ob- tained by adding noise to a clean image x0, and the predic- tion ˆxt produced by a single-step denoising operation:\\n\\nEven when the same sampling noise level σt is used, the denoised prediction does not align with the expected noisy image, revealing a time inconsistency in the learned velocity field.\\n\\nTo evaluate this effect, we progressively add noise to a clean image to obtain a trajectory x0,x1,...,xT. At a selected timestep t, we apply a single reverse Euler step to xt to obtain ˆxt−1, then compare it to the ground-truth forward image xt−1. We perform it on a set of random images and compute the mean squared error (MSE) between ˆxt−1 and xt−1.\\n\\nAs shown in Figure 2, this discrepancy exists even at the training resolution and becomes more severe as image resolution diverges. Panel (a) illustrates the conceptual gap between forward and reverse operations, while panel (b) shows that the MSE grows sharply at early steps and at lower resolutions. These findings suggest that the model’s denois- ing behavior is not intrinsically aligned with the resolution- invariant assumptions of the noise schedule.\\n\\n(1)\\n\\n(2)\\n\\n(3)\\n\\n(a) Visual degradation at unfamiliar resolutions. Besides cropping artifacts likely caused by positional encodings, generated images often exhibit structural glitches and texture breakdowns (top). Our method (bottom) improves visual fidelity by calibrating the conditioning noise level without retraining or architectural changes.\\n\\n(b) MSE between forward and reverse trajectories across timesteps t at different resolutions. Misalign- ment increases with resolution shift, especially at early steps.\\n\\nFigure 2. Training-testing misalignment in diffusion sampling. The forward (noise addition) and reverse (denoising) processes are theoretically symmetric but diverge during test-time sampling. (a) illustrates the conceptual discrepancy. (b) plots the mean squared error between the predicted and actual noisy image across sampling steps.\\n\\nResolution-Dependent Misalignment. While minor for- ward–reverse discrepancies are expected due to training lim- itations and imperfect regression of the velocity field, we observe that these inconsistencies vary systematically with resolution. In particular, the misalignment becomes substan- tially worse when sampling at resolutions that deviate from the model’s training resolution.\\n\\nTo understand this phenomenon, we compare the outputs of the forward noise process across different resolutions using the same noise level σt. As shown in Figure 1(a), more signal is destroyed in the low-resolution images in this forward process compared to the high-resolution ones, despite the shared diffusion schedule {σt}. This suggests that the perceptual impact of noise is resolution-dependent.\\n\\nWe hypothesize that this effect arises from spatial com- pression. In low-resolution images, each pixel encodes a larger region of semantic content, so adding noise dispro- portionately disrupts meaningful structure. In contrast, high- resolution images have greater spatial redundancy, which makes the same level of noise less perceptually destructive. As a result, a fixed σt induces qualitatively different levels of corruption depending on resolution.\\n\\nTo quantify this, we analyze SSIM between clean images x0 and their forward-noised counterparts xt across multi- ple resolutions. Figure 1(b) shows that SSIM consistently degrades more rapidly at lower resolutions. These trends mir- ror the MSE-based forward-reverse misalignment described in Figure 2, further reinforcing the link between perceptual degradation and resolution.\\n\\nTogether, these findings reveal a fundamental sampling- time issue: diffusion models assume resolution-invariant\\n\\nAlgorithm 1 Coarse-to-Fine Conditioning Noise Calibration\\n\\nRequire: Model ϕ, clean image x0, noise schedule {σt}, timestep t, upper bound ˆσ∗ t+1\\n\\n1: xt+1 ← AddNoise(x0,σt+1)\\n\\n2: xt ← AddNoise(x0,σt)\\n\\n3: Initialize ˆσ∗ t ← σt, L∗ ← ∥ˆxt − xt∥2\\n\\n4: Set coarse search range: ˜σ ∈ [max(0,σt − ϵc),min(ˆσ∗ t+1,σt + ϵc)]\\n\\n5: for all ˜σ ∈ coarse candidates (stride δc) do\\n\\n6: ˆxt ← xt+1 + ϕ(xt+1, ˜σ) · ∆t\\n\\n7: L ← ∥ˆxt − xt∥2\\n\\n8: if L < L∗ then\\n\\n9: ˆσ∗ t ← ˜σ, L∗ ← L\\n\\n10: end if\\n\\n11: end for\\n\\n12: Set fine search range: ˜σ ∈ [max(0, ˆσ∗ t − ϵf),min(ˆσ∗ t+1, ˆσ∗ t + ϵf)]\\n\\n13: for all ˜σ ∈ fine candidates (stride δf, δf < δc) do\\n\\n14: Repeat lines 6–10\\n\\n15: end for\\n\\n16: return Optimized ˆσ∗ t\\n\\nnoise behavior, yet in practice, noise interacts with image structure in resolution-sensitive ways. While modern dif- fusion models can generate images at arbitrary resolutions, our analysis shows that quality degradation at unseen resolu- tions may stem from this overlooked resolution-dependent misalignment.\\n\\n(a) Visual degradation at unfamiliar resolutions. Besides cropping artifacts likely caused by positional encodings, generated images often exhibit structural glitches and texture breakdowns (top). Our method (bottom) improves visual fidelity by calibrating the conditioning noise level without retraining or architectural changes.\\n\\n(b) MSE between forward and reverse trajectories across timesteps t at different resolutions. Misalign- ment increases with resolution shift, especially at early steps.\\n\\nFigure 2. Training-testing misalignment in diffusion sampling. The forward (noise addition) and reverse (denoising) processes are theoretically symmetric but diverge during test-time sampling. (a) illustrates the conceptual discrepancy. (b) plots the mean squared error between the predicted and actual noisy image across sampling steps.\\n\\nResolution-Dependent Misalignment. While minor for- ward–reverse discrepancies are expected due to training lim- itations and imperfect regression of the velocity field, we observe that these inconsistencies vary systematically with resolution. In particular, the misalignment becomes substan- tially worse when sampling at resolutions that deviate from the model’s training resolution.\\n\\nTo understand this phenomenon, we compare the outputs of the forward noise process across different resolutions using the same noise level σt. As shown in Figure 1(a), more signal is destroyed in the low-resolution images in this forward process compared to the high-resolution ones, despite the shared diffusion schedule {σt}. This suggests that the perceptual impact of noise is resolution-dependent.\\n\\nWe hypothesize that this effect arises from spatial com- pression. In low-resolution images, each pixel encodes a larger region of semantic content, so adding noise dispro- portionately disrupts meaningful structure. In contrast, high- resolution images have greater spatial redundancy, which makes the same level of noise less perceptually destructive. As a result, a fixed σt induces qualitatively different levels of corruption depending on resolution.\\n\\nTo quantify this, we analyze SSIM between clean images x0 and their forward-noised counterparts xt across multi- ple resolutions. Figure 1(b) shows that SSIM consistently degrades more rapidly at lower resolutions. These trends mir- ror the MSE-based forward-reverse misalignment described in Figure 2, further reinforcing the link between perceptual degradation and resolution.\\n\\nTogether, these findings reveal a fundamental sampling- time issue: diffusion models assume resolution-invariant\\n\\nAlgorithm 1 Coarse-to-Fine Conditioning Noise Calibration\\n\\nRequire: Model ϕ, clean image x0, noise schedule {σt}, timestep t, upper bound ˆσ∗ t+1\\n\\n1: xt+1 ← AddNoise(x0,σt+1)\\n\\n2: xt ← AddNoise(x0,σt)\\n\\n3: Initialize ˆσ∗ t ← σt, L∗ ← ∥ˆxt − xt∥2\\n\\n4: Set coarse search range: ˜σ ∈ [max(0,σt − ϵc),min(ˆσ∗ t+1,σt + ϵc)]\\n\\n5: for all ˜σ ∈ coarse candidates (stride δc) do\\n\\n6: ˆxt ← xt+1 + ϕ(xt+1, ˜σ) · ∆t\\n\\n7: L ← ∥ˆxt − xt∥2\\n\\n8: if L < L∗ then\\n\\n9: ˆσ∗ t ← ˜σ, L∗ ← L\\n\\n10: end if\\n\\n11: end for\\n\\n12: Set fine search range: ˜σ ∈ [max(0, ˆσ∗ t − ϵf),min(ˆσ∗ t+1, ˆσ∗ t + ϵf)]\\n\\n13: for all ˜σ ∈ fine candidates (stride δf, δf < δc) do\\n\\n14: Repeat lines 6–10\\n\\n15: end for\\n\\n16: return Optimized ˆσ∗ t\\n\\nnoise behavior, yet in practice, noise interacts with image structure in resolution-sensitive ways. While modern dif- fusion models can generate images at arbitrary resolutions, our analysis shows that quality degradation at unseen resolu- tions may stem from this overlooked resolution-dependent misalignment.\\n\\n3.3. Resolution-Aware Test-Time Calibration\\n\\nWhile recent methods have extended diffusion models to ar- bitrary resolutions by adapting token lengths or position en- codings [28], we focus instead on correcting forward-reverse misalignment through test-time adjustment of the condition- ing noise level. Rather than modifying the sampling tra- jectory or retraining the model, we propose a lightweight post-processing procedure that adjusts only the input noise level ˆσt passed to the denoiser at each timestep.\\n\\nRole of Noise-Level Embedding. At each timestep, diffu- sion models receive not only the noisy latent xt but also a noise-level or timestep embedding that encodes the expected magnitude of corruption in the input. This embedding allows the model to adapt its prediction of the denoised image based on how noisy the input is: for low noise, the model performs minor corrections, whereas for high noise, it applies stronger denoising. In our framework, the conditioning noise ˆσt is directly mapped to this embedding. Consequently, by ma- nipulating ˆσt at test time, we can explicitly interfere how the model interprets the noise level in the input, enabling a resolution-aware correction without changing the learned weights or the sampling schedule.\\n\\nProblem Setup. Let ϕ denote a trained flow-matching diffusion model with a predefined noise schedule σtt = 0T. At each timestep t, the model receives a noisy latent xt+1 and a noise-level embedding derived from a conditioning noise ˆσt. This embedding informs the network how much noise to expect and guides its prediction of the velocity field ϕ(xt+1, ˆσt).\\n\\nIn standard sampling, ˆσt = σt follows the original noise schedule. In our approach, we allow ˆσt to differ from σt to correct forward–reverse misalignment at test time. By tuning ˆσt, we can adapt the model’s denoising behavior to different resolutions without changing the learned weights or sampling procedure.\\n\\nGiven a noisy latent input xt+1, we expect xt to match the forward sample obtained by applying Gaussian noise to a clean latent x0. Our goal is to find a conditioning noise level ˆσt such that the denoised prediction ˆxt minimizes the discrepancy from xt:\\n\\nCoarse-to-Fine Search Strategy. We estimate ˆσ∗ t via a coarse-to-fine grid search that minimizes the one-step reverse error at each timestep. Starting from the final step t = T−1, we proceed backward in time. For each t, we first evaluate the default conditioning ˆσt = σt, then perform a coarse\\n\\nAlgorithm 2 Euler Sampling with Calibrated Conditioning\\n\\nRequire: Model ϕ, initial {σt}T t=0, calibrated conditioning {ˆσ∗ 1: for t = T − 1 to 0 do 2: vt ← ϕ(xt+1, ˆσ∗ t ) 3: xt ← xt+1 + vt · (σt+1 − σt) 4: end for 5: return x0\\n\\nnoise schedule T—1 t=0\\n\\nsweep with a larger stride δc to identify a promising region. A subsequent fine-grained search with a smaller stride δf refines the estimate within a narrow window.\\n\\nTo ensure monotonic consistency with the diffusion tra- jectory, we constrain all candidate values to lie in [0, ˆσ∗ t+1]. This backward recursion allows us to progressively calibrate the denoising inputs while respecting the ordering of noise levels. The full procedure is detailed in Algorithm 1.\\n\\nWe perform this calibration over a small set of image-text pairs for each resolution. The resulting schedule {ˆσ∗ t } is resolution-specific and cached for reuse at inference.\\n\\nInference with Calibrated Conditioning. At inference time, we sample using the original noise schedule {σt} to preserve the intended diffusion trajectory. However, at each timestep t, we replace the model input conditioning with the precomputed value ˆσ∗ t obtained from the calibration procedure. These values are resolution-specific but fixed across inputs, and require no model retraining or architec- tural changes. As shown in Algorithm 2, the update step becomes:\\n\\n(6)\\n\\n4. Experimental Setup\\n\\nImplementation Details. For small-scale lightweight cal- ibration, we randomly sample 200 web-crawled images from SBU [27]. These samples are only used to estimate resolution-specific conditioning noise levels. For all experi- ments, we use a coarse-to-fine search strategy with a coarse search window ϵc = 0.1 and a fine search window ϵf = 0.01. Experiments are conducted on NVIDIA A40s. On a single A40 GPU, calibrating on a 128×128 image takes around 30 seconds for SD3, 70 seconds for SD3.5, and 100 seconds for Flux-Dev.\\n\\nDatasets. We evaluate on LAION-COCO [35], a diverse subset of LAION-5B [34] containing multi-resolution im- ages with BLIP-generated captions [20]. It includes a broad spectrum of content, such as objects, people, and natural scenes. We also include an evaluation on the CelebA bench- mark [24], a face-centric image–text dataset.\\n\\n3.3. Resolution-Aware Test-Time Calibration\\n\\nWhile recent methods have extended diffusion models to ar- bitrary resolutions by adapting token lengths or position en- codings [28], we focus instead on correcting forward-reverse misalignment through test-time adjustment of the condition- ing noise level. Rather than modifying the sampling tra- jectory or retraining the model, we propose a lightweight post-processing procedure that adjusts only the input noise level ˆσt passed to the denoiser at each timestep.\\n\\nRole of Noise-Level Embedding. At each timestep, diffu- sion models receive not only the noisy latent xt but also a noise-level or timestep embedding that encodes the expected magnitude of corruption in the input. This embedding allows the model to adapt its prediction of the denoised image based on how noisy the input is: for low noise, the model performs minor corrections, whereas for high noise, it applies stronger denoising. In our framework, the conditioning noise ˆσt is directly mapped to this embedding. Consequently, by ma- nipulating ˆσt at test time, we can explicitly interfere how the model interprets the noise level in the input, enabling a resolution-aware correction without changing the learned weights or the sampling schedule.\\n\\nProblem Setup. Let ϕ denote a trained flow-matching diffusion model with a predefined noise schedule σtt = 0T. At each timestep t, the model receives a noisy latent xt+1 and a noise-level embedding derived from a conditioning noise ˆσt. This embedding informs the network how much noise to expect and guides its prediction of the velocity field ϕ(xt+1, ˆσt).\\n\\nIn standard sampling, ˆσt = σt follows the original noise schedule. In our approach, we allow ˆσt to differ from σt to correct forward–reverse misalignment at test time. By tuning ˆσt, we can adapt the model’s denoising behavior to different resolutions without changing the learned weights or sampling procedure.\\n\\nGiven a noisy latent input xt+1, we expect xt to match the forward sample obtained by applying Gaussian noise to a clean latent x0. Our goal is to find a conditioning noise level ˆσt such that the denoised prediction ˆxt minimizes the discrepancy from xt:\\n\\nCoarse-to-Fine Search Strategy. We estimate ˆσ∗ t via a coarse-to-fine grid search that minimizes the one-step reverse error at each timestep. Starting from the final step t = T−1, we proceed backward in time. For each t, we first evaluate the default conditioning ˆσt = σt, then perform a coarse\\n\\nAlgorithm 2 Euler Sampling with Calibrated Conditioning\\n\\nRequire: Model ϕ, initial {σt}T t=0, calibrated conditioning {ˆσ∗ 1: for t = T − 1 to 0 do 2: vt ← ϕ(xt+1, ˆσ∗ t ) 3: xt ← xt+1 + vt · (σt+1 − σt) 4: end for 5: return x0\\n\\nnoise schedule T—1 t=0\\n\\nsweep with a larger stride δc to identify a promising region. A subsequent fine-grained search with a smaller stride δf refines the estimate within a narrow window.\\n\\nTo ensure monotonic consistency with the diffusion tra- jectory, we constrain all candidate values to lie in [0, ˆσ∗ t+1]. This backward recursion allows us to progressively calibrate the denoising inputs while respecting the ordering of noise levels. The full procedure is detailed in Algorithm 1.\\n\\nWe perform this calibration over a small set of image-text pairs for each resolution. The resulting schedule {ˆσ∗ t } is resolution-specific and cached for reuse at inference.\\n\\nInference with Calibrated Conditioning. At inference time, we sample using the original noise schedule {σt} to preserve the intended diffusion trajectory. However, at each timestep t, we replace the model input conditioning with the precomputed value ˆσ∗ t obtained from the calibration procedure. These values are resolution-specific but fixed across inputs, and require no model retraining or architec- tural changes. As shown in Algorithm 2, the update step becomes:\\n\\n(6)\\n\\n4. Experimental Setup\\n\\nImplementation Details. For small-scale lightweight cal- ibration, we randomly sample 200 web-crawled images from SBU [27]. These samples are only used to estimate resolution-specific conditioning noise levels. For all experi- ments, we use a coarse-to-fine search strategy with a coarse search window ϵc = 0.1 and a fine search window ϵf = 0.01. Experiments are conducted on NVIDIA A40s. On a single A40 GPU, calibrating on a 128×128 image takes around 30 seconds for SD3, 70 seconds for SD3.5, and 100 seconds for Flux-Dev.\\n\\nDatasets. We evaluate on LAION-COCO [35], a diverse subset of LAION-5B [34] containing multi-resolution im- ages with BLIP-generated captions [20]. It includes a broad spectrum of content, such as objects, people, and natural scenes. We also include an evaluation on the CelebA bench- mark [24], a face-centric image–text dataset.\\n\\nTable 1. Quantitative evaluation on CelebA across resolutions. We report CLIP score (↑) and FID (↓) for SD3 and SD3.5 with and without our calibration. Superscript percentages show relative improvements (violet) or degradations (gray).\\n\\nSD3 SD3.5 CLIP↑ FID↓ CLIP↑ FID↓ 128×128 Base 21.07 320.45 19.01 386.86 +Ours 21.86+3.75% 311.89+2.67% 20.56+8.15% 374.74+3.13% 256×256 Base 22.14 291.26 19.96 359.00 +Ours 23.76+7.32% 252.61+13.27% 20.21+1.25% 271.52+24.37% 512×512 Base 25.54 128.62 22.27 292.42 +Ours 25.74+0.78% 123.14+4.26% 23.51+5.57% 270.36+7.54% 768×768 Base 27.02 93.66 26.68 135.84 +Ours 27.03+0.04% 93.14+0.56% 26.91+0.86% 127.17+6.38% 1024×1024 Base 25.07 132.65 26.66 96.27 +Ours 25.08+0.04% 131.99+0.50% 26.66+0.00% 96.10+0.18%\\n\\nTable 2. Quantitative evaluation on LAION-COCO across reso- lutions. We report CLIP score (↑) and FID (↓) for SD3 and SD3.5 with and without our calibration. Superscript percentages show relative improvements (violet) or degradations (gray).\\n\\nSD3 SD3.5 CLIP↑ FID↓ CLIP↑ FID↓ 128×128 Base 19.80 203.23 19.18 310.40 +Ours 21.07+6.41% 170.93+15.89% 19.75+2.97% 276.90+10.79% 256×256 Base 22.24 159.13 19.46 256.31 +Ours 23.28+4.68% 130.84+17.78% 20.23+3.96% 175.14+31.67% 512×512 Base 28.52 76.49 22.26 203.55 +Ours 28.61+0.32% 75.86+0.82% 23.41+5.17% 174.20+14.42% 768×768 Base 30.10 55.13 31.15 45.05 +Ours 30.10+0.00% 55.07+0.11% 31.28+0.42% 42.05+6.66% 1024×1024 Base 29.26 94.53 31.62 47.65 +Ours 29.22−0.14% 95.24−0.75% 31.62+0.00% 47.69−0.08%\\n\\nMetrics. We evaluate models with standard text-to-image generation metrics: CLIP Score [14] for text–image align- ment and Frechet Inception Distance (FID) [15] for the dis- tance between feature distributions of real and generated images.\\n\\nModels. We validate our method on flow-matching diffu- sion models with various configurations. We integrate it to Stable Diffusion 3 [8] with a linear noise schedule and Stable Diffusion 3.5 [8] with a non-linear schedule that spends more time in low- or high-noise regimes. We also experiment with Flux-Dev [6], an open-sourced distilled flow-matching DiT model that incorporates a resolution-aware scheduler that applies a shift parameter based on image size.\\n\\n5. Experimental Results\\n\\n5.1. Fixed Linear or Non-linear Noise Schedules\\n\\nWe evaluate NoiseShift using two flow-matching diffusion models with fixed noise schedules (See Figure 2). Stable Diffusion 3 (SD3) uses a linear noise schedule shared across resolutions, while Stable Diffusion 3.5 (SD3.5) adopts a fixed non-linear schedule that biases sampling toward low- or high-noise regimes.\\n\\nWe conduct inference with calibrated noise schedulers across resolutions with SD3 and SD3.5. As shown in Table 1, NoiseShift consistently improves CLIP Score and FID for both SD3 and SD3.5 across non-default resolutions ranging from 128×128 to 768×768. At the default resolution of 1024×1024, performance remains unchanged, confirming that NoiseShift preserves the fidelity when no calibration is needed. Similar trends hold on LAION-COCO (Table 2), where resolution-aware conditioning improves the genera- tion quality without fine-tuning the underlying model.\\n\\nThese results demonstrate that NoiseShift effectively mit- igates forward–reverse mismatch at lower resolutions. Since\\n\\nit does not modify the model architecture or training objec- tive, it is naturally upper-bounded by the model’s original generative capability. Nonetheless, it provides measurable quality gains through simple post-hoc calibration when ap- plied to lower resolutions.\\n\\n5.2. Calibrated Noise Conditioning\\n\\nFigure 4 visualizes the calibrated conditioning noise lev- els ˆσt from our method across resolutions for SD3 (left) and SD3.5 (right). At the default resolution of 1024×1024, the calibrated curves closely match the original schedule, confirming that the forward–reverse alignment is intact at the training resolution. In contrast, for lower resolutions, optimal ˆσt trajectories consistently shift above the default schedule, indicating that higher conditioning noise levels are needed to correct resolution-specific misalignment.\\n\\nThis behavior supports our core hypothesis: the same sampling noise level σt has resolution-dependent perceptual effects, which can be effectively mitigated by adjusting only the conditioning noise level during inference. Notably, the magnitude of deviation from the default schedule increases as resolution decreases, aligning with the observed degrada- tion in visual quality (see Figure 1). This trend holds across both SD3 and SD3.5 despite their differing noise schedule shapes.\\n\\n5.3. Time Shifting Noise Scheduler\\n\\nWe further evaluate NoiseShift on Flux-Dev [6], a distilled flow-matching DiT model that incorporates a resolution- aware time-shift parameter. The noise schedule in Flux-Dev shifts depending on the target resolution: higher-resolution images retain larger σt values for longer, effectively extend- ing their time in high-signal regimes.\\n\\nWhile Flux-Dev adjusts the actual sampling schedule based on resolution, NoiseShift instead calibrates the con- ditioning input to the denoiser without altering the forward\\n\\nTable 1. Quantitative evaluation on CelebA across resolutions. We report CLIP score (↑) and FID (↓) for SD3 and SD3.5 with and without our calibration. Superscript percentages show relative improvements (violet) or degradations (gray).\\n\\nSD3 SD3.5 CLIP↑ FID↓ CLIP↑ FID↓ 128×128 Base 21.07 320.45 19.01 386.86 +Ours 21.86+3.75% 311.89+2.67% 20.56+8.15% 374.74+3.13% 256×256 Base 22.14 291.26 19.96 359.00 +Ours 23.76+7.32% 252.61+13.27% 20.21+1.25% 271.52+24.37% 512×512 Base 25.54 128.62 22.27 292.42 +Ours 25.74+0.78% 123.14+4.26% 23.51+5.57% 270.36+7.54% 768×768 Base 27.02 93.66 26.68 135.84 +Ours 27.03+0.04% 93.14+0.56% 26.91+0.86% 127.17+6.38% 1024×1024 Base 25.07 132.65 26.66 96.27 +Ours 25.08+0.04% 131.99+0.50% 26.66+0.00% 96.10+0.18%\\n\\nTable 2. Quantitative evaluation on LAION-COCO across reso- lutions. We report CLIP score (↑) and FID (↓) for SD3 and SD3.5 with and without our calibration. Superscript percentages show relative improvements (violet) or degradations (gray).\\n\\nSD3 SD3.5 CLIP↑ FID↓ CLIP↑ FID↓ 128×128 Base 19.80 203.23 19.18 310.40 +Ours 21.07+6.41% 170.93+15.89% 19.75+2.97% 276.90+10.79% 256×256 Base 22.24 159.13 19.46 256.31 +Ours 23.28+4.68% 130.84+17.78% 20.23+3.96% 175.14+31.67% 512×512 Base 28.52 76.49 22.26 203.55 +Ours 28.61+0.32% 75.86+0.82% 23.41+5.17% 174.20+14.42% 768×768 Base 30.10 55.13 31.15 45.05 +Ours 30.10+0.00% 55.07+0.11% 31.28+0.42% 42.05+6.66% 1024×1024 Base 29.26 94.53 31.62 47.65 +Ours 29.22−0.14% 95.24−0.75% 31.62+0.00% 47.69−0.08%\\n\\nMetrics. We evaluate models with standard text-to-image generation metrics: CLIP Score [14] for text–image align- ment and Frechet Inception Distance (FID) [15] for the dis- tance between feature distributions of real and generated images.\\n\\nModels. We validate our method on flow-matching diffu- sion models with various configurations. We integrate it to Stable Diffusion 3 [8] with a linear noise schedule and Stable Diffusion 3.5 [8] with a non-linear schedule that spends more time in low- or high-noise regimes. We also experiment with Flux-Dev [6], an open-sourced distilled flow-matching DiT model that incorporates a resolution-aware scheduler that applies a shift parameter based on image size.\\n\\n5. Experimental Results\\n\\n5.1. Fixed Linear or Non-linear Noise Schedules\\n\\nWe evaluate NoiseShift using two flow-matching diffusion models with fixed noise schedules (See Figure 2). Stable Diffusion 3 (SD3) uses a linear noise schedule shared across resolutions, while Stable Diffusion 3.5 (SD3.5) adopts a fixed non-linear schedule that biases sampling toward low- or high-noise regimes.\\n\\nWe conduct inference with calibrated noise schedulers across resolutions with SD3 and SD3.5. As shown in Table 1, NoiseShift consistently improves CLIP Score and FID for both SD3 and SD3.5 across non-default resolutions ranging from 128×128 to 768×768. At the default resolution of 1024×1024, performance remains unchanged, confirming that NoiseShift preserves the fidelity when no calibration is needed. Similar trends hold on LAION-COCO (Table 2), where resolution-aware conditioning improves the genera- tion quality without fine-tuning the underlying model.\\n\\nThese results demonstrate that NoiseShift effectively mit- igates forward–reverse mismatch at lower resolutions. Since\\n\\nit does not modify the model architecture or training objec- tive, it is naturally upper-bounded by the model’s original generative capability. Nonetheless, it provides measurable quality gains through simple post-hoc calibration when ap- plied to lower resolutions.\\n\\n5.2. Calibrated Noise Conditioning\\n\\nFigure 4 visualizes the calibrated conditioning noise lev- els ˆσt from our method across resolutions for SD3 (left) and SD3.5 (right). At the default resolution of 1024×1024, the calibrated curves closely match the original schedule, confirming that the forward–reverse alignment is intact at the training resolution. In contrast, for lower resolutions, optimal ˆσt trajectories consistently shift above the default schedule, indicating that higher conditioning noise levels are needed to correct resolution-specific misalignment.\\n\\nThis behavior supports our core hypothesis: the same sampling noise level σt has resolution-dependent perceptual effects, which can be effectively mitigated by adjusting only the conditioning noise level during inference. Notably, the magnitude of deviation from the default schedule increases as resolution decreases, aligning with the observed degrada- tion in visual quality (see Figure 1). This trend holds across both SD3 and SD3.5 despite their differing noise schedule shapes.\\n\\n5.3. Time Shifting Noise Scheduler\\n\\nWe further evaluate NoiseShift on Flux-Dev [6], a distilled flow-matching DiT model that incorporates a resolution- aware time-shift parameter. The noise schedule in Flux-Dev shifts depending on the target resolution: higher-resolution images retain larger σt values for longer, effectively extend- ing their time in high-signal regimes.\\n\\nWhile Flux-Dev adjusts the actual sampling schedule based on resolution, NoiseShift instead calibrates the con- ditioning input to the denoiser without altering the forward\\n\\nFigure 3. Qualitative comparison of Flux-Dev. Generated image examples before and after applying NoiseShift are on CelebA (left) and LAION-COCO (right).\\n\\n5D3.5: Default & Calibrated Sigma vs. Step across Resolutions\\n\\nFigure 4. Calibrated conditioning noise levels across resolutions. We plot the default sampling noise schedule (gray) alongside the resolution-specific calibrated conditioning ˆσt for SD3 (left) and SD3.5 (right). At the default resolution (1024×1024), the curves align closely. At lower resolutions, the optimal ˆσt curves consis- tently deviate upward, reflecting a need for stronger conditioning to compensate for perceptual degradation.\\n\\nnoise schedule itself. In that sense, our approach operates as a lightweight test-time correction, “cheating” the model’s\\n\\nTable 3. Quantitative evaluation of Flux-Dev across resolutions. We report CLIP score (↑) and FID (↓) on LAION-COCO and CelebA with and without our calibration. Superscript percentages show relative improvements (violet) or degradations (gray).\\n\\nLAION-COCO CelebA CLIP↑ FID↓ CLIP↑ FID↓ 64×64 Base 25.58 119.60 27.70 182.96 +Ours 25.81+0.90% 113.32+4.80% 27.77+0.25% 177.03+4.22% 128×128 Base 30.74 48.00 28.75 90.62 +Ours 30.83+0.29% 47.45+1.15% 28.64−0.38% 87.63+3.30% 256×256 Base 32.30 26.72 27.89 56.33 +Ours 32.33+0.09% 25.82+3.37% 27.89+0.00% 55.52+1.44% 512×512 Base 32.55 20.13 28.44 87.86 +Ours 32.55+0.00% 19.62+2.53% 28.45+0.04% 86.45+1.60% 768×768 Base 32.58 19.10 27.90 75.52 +Ours 32.60+0.06% 19.03+0.37% 28.10+0.72% 72.10+4.53% 1024×1024 Base 32.82 21.05 27.92 79.01 +Ours 32.80−0.06% 20.51+2.57% 28.12+0.72% 75.21+4.81%\\n\\nFigure 5. Ablation studies. Ablation studies on the number of samples used during calibration and the new sigmas obtained at 128×128 and 256×256.\\n\\nexpectations without modifying its architecture or training.\\n\\nAs shown in Table 3, NoiseShift provides consistent im- provements from 64×64 to 768×768 on LAION-COCO and CelebA. Although gains are modest, they are notable given that Flux-Dev is a distilled model and our method requires no finetunning. Interestingly, at 1024×1024, performance\\n\\nFigure 3. Qualitative comparison of Flux-Dev. Generated image examples before and after applying NoiseShift are on CelebA (left) and LAION-COCO (right).\\n\\n5D3.5: Default & Calibrated Sigma vs. Step across Resolutions\\n\\nFigure 4. Calibrated conditioning noise levels across resolutions. We plot the default sampling noise schedule (gray) alongside the resolution-specific calibrated conditioning ˆσt for SD3 (left) and SD3.5 (right). At the default resolution (1024×1024), the curves align closely. At lower resolutions, the optimal ˆσt curves consis- tently deviate upward, reflecting a need for stronger conditioning to compensate for perceptual degradation.\\n\\nnoise schedule itself. In that sense, our approach operates as a lightweight test-time correction, “cheating” the model’s\\n\\nTable 3. Quantitative evaluation of Flux-Dev across resolutions. We report CLIP score (↑) and FID (↓) on LAION-COCO and CelebA with and without our calibration. Superscript percentages show relative improvements (violet) or degradations (gray).\\n\\nLAION-COCO CelebA CLIP↑ FID↓ CLIP↑ FID↓ 64×64 Base 25.58 119.60 27.70 182.96 +Ours 25.81+0.90% 113.32+4.80% 27.77+0.25% 177.03+4.22% 128×128 Base 30.74 48.00 28.75 90.62 +Ours 30.83+0.29% 47.45+1.15% 28.64−0.38% 87.63+3.30% 256×256 Base 32.30 26.72 27.89 56.33 +Ours 32.33+0.09% 25.82+3.37% 27.89+0.00% 55.52+1.44% 512×512 Base 32.55 20.13 28.44 87.86 +Ours 32.55+0.00% 19.62+2.53% 28.45+0.04% 86.45+1.60% 768×768 Base 32.58 19.10 27.90 75.52 +Ours 32.60+0.06% 19.03+0.37% 28.10+0.72% 72.10+4.53% 1024×1024 Base 32.82 21.05 27.92 79.01 +Ours 32.80−0.06% 20.51+2.57% 28.12+0.72% 75.21+4.81%\\n\\nFigure 5. Ablation studies. Ablation studies on the number of samples used during calibration and the new sigmas obtained at 128×128 and 256×256.\\n\\nexpectations without modifying its architecture or training.\\n\\nAs shown in Table 3, NoiseShift provides consistent im- provements from 64×64 to 768×768 on LAION-COCO and CelebA. Although gains are modest, they are notable given that Flux-Dev is a distilled model and our method requires no finetunning. Interestingly, at 1024×1024, performance\\n\\nFigure 6. Qualitative comparison of SD3.5. Generated image examples before and after applying NoiseShift are on CelebA (top) and LAION-COCO (bottom).\\n\\nFigure 7. Qualitative comparison of SD3.5. Generated image examples before and after applying NoiseShift are on CelebA (top) and LAION-COCO (bottom).\\n\\nremains unchanged or slightly reduced, likely because it is the resolution of the final stage training, reducing the impact of calibration. These results suggest that NoiseShift comple- ments, but does not replace, the resolution-aware scheduling baked into the model itself.\\n\\n5.4. Ablation Studies\\n\\nThis section studies the number of samples used in the Noise- Shift calibration. As shown in Figure 5, even though the number of samples varies from 50 to 200, the calibrated sigmas always converge in almost the same range. Given the fact that the calibration already has a very low computation cost, reducing the number of samples can lead to an almost free re-calibration, which improves the low-resolution image generation quality.\\n\\n5.5. Qualitative Results\\n\\nIn Figure 7 and Figure 6, we present qualitative examples of SD3 and SD3.5 [9] before and after applying NoiseShift on CelebA and LAION-COCO. Across all resolutions, Noise- Shift improves the overall image quality, providing better structure and textures of the generated images. In Figure 3, we show how NoiseShift improves the Flux-Dev model down to 64×64 resolutions. Overall, we observe the quality im- provement and fewer artifacts across the resolutions.\\n\\n6. Conclusion\\n\\nWe identify resolution-dependent exposure bias as a key contributor to quality degradation in state-of-the-art diffusion models when sampling at lower resolutions. To address this, we introduced NoiseShift, a simple, training-free approach that recalibrates the denoising conditioning to better align with resolution-specific noise distributions. Our method consistently improves image quality across resolutions and is complementary to both noise schedule modifications and architectural enhancements, making it readily applicable to recent advances in text-to-image generation.\\n\\nLimitations and Future Work. While NoiseShift miti- gates exposure bias and enhances cross-resolution perfor- mance, it does not fully solve the challenge of low-resolution generalization. Future work may explore integrating Noise- Shift with learned adapters, dynamic token routing, or resolution-specific fine-tuning strategies.\\n\\nBroader Impact. By enhancing sample quality at low res- olutions, NoiseShift increases the adaptability of text-to- image systems to diverse deployment settings, including mobile and assistive applications. However, the ability to generate increasingly high-quality synthetic content may also exacerbate issues around misinformation and image provenance, highlighting the need for responsible use and effective detection mechanisms.\\n\\nFigure 6. Qualitative comparison of SD3.5. Generated image examples before and after applying NoiseShift are on CelebA (top) and LAION-COCO (bottom).\\n\\nFigure 7. Qualitative comparison of SD3.5. Generated image examples before and after applying NoiseShift are on CelebA (top) and LAION-COCO (bottom).\\n\\nremains unchanged or slightly reduced, likely because it is the resolution of the final stage training, reducing the impact of calibration. These results suggest that NoiseShift comple- ments, but does not replace, the resolution-aware scheduling baked into the model itself.\\n\\n5.4. Ablation Studies\\n\\nThis section studies the number of samples used in the Noise- Shift calibration. As shown in Figure 5, even though the number of samples varies from 50 to 200, the calibrated sigmas always converge in almost the same range. Given the fact that the calibration already has a very low computation cost, reducing the number of samples can lead to an almost free re-calibration, which improves the low-resolution image generation quality.\\n\\n5.5. Qualitative Results\\n\\nIn Figure 7 and Figure 6, we present qualitative examples of SD3 and SD3.5 [9] before and after applying NoiseShift on CelebA and LAION-COCO. Across all resolutions, Noise- Shift improves the overall image quality, providing better structure and textures of the generated images. In Figure 3, we show how NoiseShift improves the Flux-Dev model down to 64×64 resolutions. Overall, we observe the quality im- provement and fewer artifacts across the resolutions.\\n\\n6. Conclusion\\n\\nWe identify resolution-dependent exposure bias as a key contributor to quality degradation in state-of-the-art diffusion models when sampling at lower resolutions. To address this, we introduced NoiseShift, a simple, training-free approach that recalibrates the denoising conditioning to better align with resolution-specific noise distributions. Our method consistently improves image quality across resolutions and is complementary to both noise schedule modifications and architectural enhancements, making it readily applicable to recent advances in text-to-image generation.\\n\\nLimitations and Future Work. While NoiseShift miti- gates exposure bias and enhances cross-resolution perfor- mance, it does not fully solve the challenge of low-resolution generalization. Future work may explore integrating Noise- Shift with learned adapters, dynamic token routing, or resolution-specific fine-tuning strategies.\\n\\nBroader Impact. By enhancing sample quality at low res- olutions, NoiseShift increases the adaptability of text-to- image systems to diverse deployment settings, including mobile and assistive applications. However, the ability to generate increasingly high-quality synthetic content may also exacerbate issues around misinformation and image provenance, highlighting the need for responsible use and effective detection mechanisms.\\n\\nReferences\\n\\n[1] Stability AI. Stable diffusion 3. https://stability. ai/news/stable-diffusion-3-announcement, 2024. 3\\n\\n[2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. In ICML, 2023. 1, 2\\n\\n[3] Ting Chen. On the importance of noise scheduling for diffu- sion models. arXiv, 2023. 3\\n\\n[4] Ting Chen. On the importance of noise scheduling for diffu- sion models. arXiv preprint arXiv:2301.10972, 2023. 2\\n\\n[5] Jiaxiang Cheng, Pan Xie, Xin Xia, Jiashi Li, Jie Wu, Yuxi Ren, Huixia Li, Xuefeng Xiao, Min Zheng, and Lean Fu. Re- sadapter: Domain consistent resolution adapter for diffusion models. ArXiv, abs/2403.02084, 2024. 1, 2\\n\\n[6] Cody Crockett, Tushar Patil, Laura Weidinger, et al. Flux: A modern diffusion transformer. https://github.com/ fluxml/flux-diffusion, 2024. 1, 2, 3, 6\\n\\n[7] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. Demofusion: Democratising high- resolution image generation with no $$$. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6159–6168, 2024. 1, 2\\n\\n[8] Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high- resolution image synthesis. ArXiv, abs/2403.03206, 2024. 6\\n\\n[9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En- tezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow trans- formers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 1, 2, 8\\n\\n[10] Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia, Xiaodong Cun, Yufei Wang, Siyu Huang, Yong Zhang, Xin- tao Wang, Qifeng Chen, et al. Make a cheap scaling: A self-cascade diffusion model for higher-resolution adaptation. In European Conference on Computer Vision, pages 39–55. Springer, 2024. 2\\n\\n[11] Qiushan Guo, Sifei Liu, Yizhou Yu, and Ping Luo. Rethinking the noise schedule of diffusion-based generative models. 2023. 3\\n\\n[12] Moayed Haji-Ali, Guha Balakrishnan, and Vicente Ordonez. Elasticdiffusion: Training-free arbitrary size image generation through global-local content separation, 2024. 1, 2, 3\\n\\n[13] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higher- resolution visual generation with diffusion models. In The Twelfth International Conference on Learning Representa- tions, 2023. 2, 3\\n\\n[14] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. ArXiv, abs/2104.08718, 2021. 6\\n\\n[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern- hard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Neural Information Processing Systems, 2017. 6\\n\\n[16] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Sim- ple diffusion: End-to-end diffusion for high resolution images. In Proceedings of the 40th International Conference on Ma- chine Learning (ICML), 2023. 1, 2\\n\\n[17] Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, and Hongsheng Li. Fouriscale: A frequency perspective on training-free high-resolution im- age synthesis. In European Conference on Computer Vision, pages 196–212. Springer, 2024. 1\\n\\n[18] Juno Hwang, Yong-Hyun Park, and Junghyo Jo. Resolu- tion chromatography of diffusion models. arXiv preprint arXiv:2401.10247, 2023. 1\\n\\n[19] Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Training- free diffusion model adaptation for variable-sized text-to- image synthesis. Advances in Neural Information Processing Systems, 36:70847–70860, 2023. 3\\n\\n[20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. International Conference on Machine Learning, 2022. 5 In\\n\\n[21] Mingxiao Li, Tingyu Qu, Ruicong Yao, Wei Sun, and Marie- Francine Moens. Alleviating exposure bias in diffusion mod- els through sampling with shifted time steps. arXiv preprint arXiv:2305.15583, 2023. 1, 3\\n\\n[22] Yotam Lipman, Emiel Hoogeboom, Ajay Jain, Jacob Menick, Arash Vahdat, Tim Salimans, David J Fleet, and Jonathan Heek. Flow matching for generative modeling. arXiv preprint arXiv:2305.08891, 2023. 3\\n\\n[23] Hanyu Liu, Zhen Xu, Wei Shi, Yuntao Bai, Hongyuan Zhao, Stefano Ermon, and Xiao Wang. Flow matching models for learning reliable dynamics. arXiv preprint arXiv:2305.19591, 2023. 3\\n\\n[24] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015. 5\\n\\n[25] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775–5787, 2022. 3\\n\\n[26] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 3\\n\\n[27] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned pho- tographs. In Neural Information Processing Systems, 2011. 5\\n\\n[28] William Peebles and Saining Xie. Scalable Diffusion Mod- els with Transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4195–4205, 2023. 1, 5\\n\\nReferences\\n\\n[1] Stability AI. Stable diffusion 3. https://stability. ai/news/stable-diffusion-3-announcement, 2024. 3\\n\\n[2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. In ICML, 2023. 1, 2\\n\\n[3] Ting Chen. On the importance of noise scheduling for diffu- sion models. arXiv, 2023. 3\\n\\n[4] Ting Chen. On the importance of noise scheduling for diffu- sion models. arXiv preprint arXiv:2301.10972, 2023. 2\\n\\n[5] Jiaxiang Cheng, Pan Xie, Xin Xia, Jiashi Li, Jie Wu, Yuxi Ren, Huixia Li, Xuefeng Xiao, Min Zheng, and Lean Fu. Re- sadapter: Domain consistent resolution adapter for diffusion models. ArXiv, abs/2403.02084, 2024. 1, 2\\n\\n[6] Cody Crockett, Tushar Patil, Laura Weidinger, et al. Flux: A modern diffusion transformer. https://github.com/ fluxml/flux-diffusion, 2024. 1, 2, 3, 6\\n\\n[7] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. Demofusion: Democratising high- resolution image generation with no $$$. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6159–6168, 2024. 1, 2\\n\\n[8] Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high- resolution image synthesis. ArXiv, abs/2403.03206, 2024. 6\\n\\n[9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En- tezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow trans- formers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 1, 2, 8\\n\\n[10] Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia, Xiaodong Cun, Yufei Wang, Siyu Huang, Yong Zhang, Xin- tao Wang, Qifeng Chen, et al. Make a cheap scaling: A self-cascade diffusion model for higher-resolution adaptation. In European Conference on Computer Vision, pages 39–55. Springer, 2024. 2\\n\\n[11] Qiushan Guo, Sifei Liu, Yizhou Yu, and Ping Luo. Rethinking the noise schedule of diffusion-based generative models. 2023. 3\\n\\n[12] Moayed Haji-Ali, Guha Balakrishnan, and Vicente Ordonez. Elasticdiffusion: Training-free arbitrary size image generation through global-local content separation, 2024. 1, 2, 3\\n\\n[13] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higher- resolution visual generation with diffusion models. In The Twelfth International Conference on Learning Representa- tions, 2023. 2, 3\\n\\n[14] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. ArXiv, abs/2104.08718, 2021. 6\\n\\n[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern- hard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Neural Information Processing Systems, 2017. 6\\n\\n[16] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Sim- ple diffusion: End-to-end diffusion for high resolution images. In Proceedings of the 40th International Conference on Ma- chine Learning (ICML), 2023. 1, 2\\n\\n[17] Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, and Hongsheng Li. Fouriscale: A frequency perspective on training-free high-resolution im- age synthesis. In European Conference on Computer Vision, pages 196–212. Springer, 2024. 1\\n\\n[18] Juno Hwang, Yong-Hyun Park, and Junghyo Jo. Resolu- tion chromatography of diffusion models. arXiv preprint arXiv:2401.10247, 2023. 1\\n\\n[19] Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Training- free diffusion model adaptation for variable-sized text-to- image synthesis. Advances in Neural Information Processing Systems, 36:70847–70860, 2023. 3\\n\\n[20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. International Conference on Machine Learning, 2022. 5 In\\n\\n[21] Mingxiao Li, Tingyu Qu, Ruicong Yao, Wei Sun, and Marie- Francine Moens. Alleviating exposure bias in diffusion mod- els through sampling with shifted time steps. arXiv preprint arXiv:2305.15583, 2023. 1, 3\\n\\n[22] Yotam Lipman, Emiel Hoogeboom, Ajay Jain, Jacob Menick, Arash Vahdat, Tim Salimans, David J Fleet, and Jonathan Heek. Flow matching for generative modeling. arXiv preprint arXiv:2305.08891, 2023. 3\\n\\n[23] Hanyu Liu, Zhen Xu, Wei Shi, Yuntao Bai, Hongyuan Zhao, Stefano Ermon, and Xiao Wang. Flow matching models for learning reliable dynamics. arXiv preprint arXiv:2305.19591, 2023. 3\\n\\n[24] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015. 5\\n\\n[25] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775–5787, 2022. 3\\n\\n[26] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 3\\n\\n[27] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned pho- tographs. In Neural Information Processing Systems, 2011. 5\\n\\n[28] William Peebles and Saining Xie. Scalable Diffusion Mod- els with Transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4195–4205, 2023. 1, 5\\n\\n[29] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv, 2023. 1\\n\\n[30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 1\\n\\n[31] Haonan Qiu, Shiwei Zhang, Yujie Wei, Ruihang Chu, Hangjie Yuan, Xiang Wang, Yingya Zhang, and Ziwei Liu. Freescale: Unleashing the resolution of diffusion models via tuning-free scale fusion. arXiv preprint arXiv:2412.09626, 2024. 1, 2\\n\\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. 1\\n\\n[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interven- tion, 2015. 1\\n\\n[34] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Lud- wig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion- 5b: An open large-scale dataset for training next generation image-text models. ArXiv, abs/2210.08402, 2022. 5\\n\\n[35] Christoph Schuhmann, Andreas A. K¨ opf, Theo Coombes Richard Vencu, and Ross Beaumont. Laioncoco: 600m syn- thetic captions from laion2b-en, 2023. 5\\n\\n[36] Sitian Shen, Zilin Zhu, Linqian Fan, Harry Zhang, and Xinx- iao Wu. Diffclip: Leveraging stable diffusion for language grounded 3d classification. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3596–3605, 2024. 1\\n\\n[37] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xin- grun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024.\\n\\n[38] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF international conference on computer vision, pages 3836–3847, 2023. 1 In\\n\\n[39] Yiming Zhang, Yicheng Gu, Yanhong Zeng, Zhening Xing, Yuancheng Wang, Zhizheng Wu, and Kai Chen. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. arXiv preprint arXiv:2407.01494, 2024. 1, 2\\n\\n[40] Qingping Zheng, Yuanfan Guo, Jiankang Deng, Jianhua Han, Ying Li, Songcen Xu, and Hang Xu. Any-size-diffusion: To- ward efficient text-driven synthesis for any-size hd images. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 7571–7578, 2024. 1, 2\\n\\n[41] Zhen Zou, Hu Yu, Jie Xiao, and Feng Zhao. Exposure bias reduction for enhancing diffusion transformer feature caching. arXiv preprint arXiv:2503.07120, 2025. 1\\n\\n[29] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv, 2023. 1\\n\\n[30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 1\\n\\n[31] Haonan Qiu, Shiwei Zhang, Yujie Wei, Ruihang Chu, Hangjie Yuan, Xiang Wang, Yingya Zhang, and Ziwei Liu. Freescale: Unleashing the resolution of diffusion models via tuning-free scale fusion. arXiv preprint arXiv:2412.09626, 2024. 1, 2\\n\\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. 1\\n\\n[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interven- tion, 2015. 1\\n\\n[34] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Lud- wig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion- 5b: An open large-scale dataset for training next generation image-text models. ArXiv, abs/2210.08402, 2022. 5\\n\\n[35] Christoph Schuhmann, Andreas A. K¨ opf, Theo Coombes Richard Vencu, and Ross Beaumont. Laioncoco: 600m syn- thetic captions from laion2b-en, 2023. 5\\n\\n[36] Sitian Shen, Zilin Zhu, Linqian Fan, Harry Zhang, and Xinx- iao Wu. Diffclip: Leveraging stable diffusion for language grounded 3d classification. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3596–3605, 2024. 1\\n\\n[37] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xin- grun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024.\\n\\n[38] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF international conference on computer vision, pages 3836–3847, 2023. 1 In\\n\\n[39] Yiming Zhang, Yicheng Gu, Yanhong Zeng, Zhening Xing, Yuancheng Wang, Zhizheng Wu, and Kai Chen. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. arXiv preprint arXiv:2407.01494, 2024. 1, 2\\n\\n[40] Qingping Zheng, Yuanfan Guo, Jiankang Deng, Jianhua Han, Ying Li, Songcen Xu, and Hang Xu. Any-size-diffusion: To- ward efficient text-driven synthesis for any-size hd images. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 7571–7578, 2024. 1, 2\\n\\n[41] Zhen Zou, Hu Yu, Jie Xiao, and Feng Zhao. Exposure bias reduction for enhancing diffusion transformer feature caching. arXiv preprint arXiv:2503.07120, 2025. 1', 'text_length': 82228, 'message': 'Retrieved 20 pages for 2510v02307v1.pdf'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `generate_summary` with `The paper introduces NoiseShift, a novel, training-free method to improve low-resolution image generation in text-to-image diffusion models. These models often struggle with generating high-quality images at resolutions different from those seen during training, particularly at lower resolutions. NoiseShift addresses this by recalibrating the noise level of the denoiser based on resolution size, without altering the model architecture or sampling schedule. This method mitigates the perceptual mismatch caused by noise schedulers that affect low-resolution images more severely than high-resolution ones. NoiseShift significantly enhances image quality at low resolutions, as demonstrated on models like Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev, with improvements in FID scores on datasets such as LAION-COCO and CelebA. The approach is lightweight, requiring no retraining, and effectively reduces resolution-dependent artifacts, making it a practical solution for improving the adaptability and efficiency of diffusion models in generating low-resolution images.`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m{'status': 'success', 'summary': \"The paper presents NoiseShift, a novel training-free technique designed to enhance low-resolution image generation in text-to-image diffusion models. These models typically face challenges in producing high-quality images at resolutions not encountered during training, especially lower ones. NoiseShift recalibrates the denoiser's noise level based on the resolution size, without modifying the model architecture or sampling schedule, addressing the perceptual mismatch from noise schedulers. This method significantly improves image quality at low resolutions, as evidenced by better FID scores on datasets like LAION-COCO and CelebA, using models such as Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev. The approach is lightweight, requiring no retraining, and effectively reduces resolution-dependent artifacts. This makes NoiseShift a practical solution for enhancing the adaptability and efficiency of diffusion models in generating low-resolution images, with potential industry impact in applications requiring high-quality image generation at varying resolutions.\", 'length': 1081, 'message': 'Generated summary (1081 characters)'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `collapse_summaries` with `[{\"summary\":\"The paper introduces a novel method called Laplacian Eigenvector Gradient Orthogonalization (LEGO) for estimating tangent spaces on data manifolds, addressing the limitations of traditional methods like Local Principal Component Analysis (LPCA) in noisy environments. LEGO utilizes graph Laplacian eigenvectors to leverage the global structure of data, orthogonalizing gradients of low-frequency eigenvectors to enhance the accuracy of tangent space estimates. Theoretical analyses confirm LEGO's robustness against noise, supported by differential geometry and random matrix theory. Empirical results show that LEGO outperforms LPCA in manifold learning, boundary detection, and local intrinsic dimension estimation. This advancement is significant for AI and data science professionals, offering a more reliable method for geometric data analysis in noisy conditions. Consequently, LEGO has the potential to improve machine learning applications where accurate data manifold estimation is crucial, impacting industries reliant on precise data analysis.\"}, {\"summary\":\"The paper presents KAVA, a groundbreaking framework for latent reasoning in large language models (LLMs) that utilizes compressed Key-Value (KV) cache distillation. This approach addresses the high computational cost of traditional chain-of-thought (CoT) reasoning by offering an efficient alternative without sacrificing accuracy. KAVA achieves this by transferring knowledge from a teacher model's compressed KV-cache to a latent reasoning student, aligning stepwise KV trajectories with continuous latent tokens. The framework not only maintains the accuracy of CoT-trained models but also enhances efficiency and scalability. KAVA demonstrates superior performance compared to existing latent reasoning methods, with minimal performance loss when shifting from equation-only to natural-language traces. It effectively scales to larger models, providing a scalable supervision signal that combines CoT accuracy with latent inference efficiency. This advancement is crucial for AI professionals aiming to develop efficient and deployable reasoning models, impacting practical applications and industry by offering a more scalable and efficient reasoning framework.\"}, {\"summary\":\"This blog post by Daniel Voigt Godoy outlines a practical approach to fine-tuning a large language model (LLM) using PyTorch and Hugging Face tools, focusing on Microsoft's Phi-3 Mini 4K Instruct model for translating English into Yoda-speak. Notable advancements include the use of quantized models to reduce memory usage and low-rank adapters (LoRA) to minimize trainable parameters, enhancing efficiency. Performance improvements are achieved through Hugging Face's SFTTrainer for supervised fine-tuning, emphasizing the importance of dataset formatting and tokenizer configuration for conversational AI models. The process involves converting datasets to a conversational format and aligning the tokenizer with the model's training, optimizing memory and configuration settings. The fine-tuned model successfully generates Yoda-like sentences, showcasing the effectiveness of the approach. Practical applications include the potential for broader use in conversational AI, with the model being made accessible via Hugging Face Hub. This demonstrates significant industry impact by facilitating the development and sharing of specialized language models.\"}, {\"summary\":\"The paper presents NoiseShift, a novel training-free technique designed to enhance low-resolution image generation in text-to-image diffusion models. These models typically face challenges in producing high-quality images at resolutions not encountered during training, especially lower ones. NoiseShift recalibrates the denoiser's noise level based on the resolution size, without modifying the model architecture or sampling schedule, addressing the perceptual mismatch from noise schedulers. This method significantly improves image quality at low resolutions, as evidenced by better FID scores on datasets like LAION-COCO and CelebA, using models such as Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev. The approach is lightweight, requiring no retraining, and effectively reduces resolution-dependent artifacts. This makes NoiseShift a practical solution for enhancing the adaptability and efficiency of diffusion models in generating low-resolution images, with potential industry impact in applications requiring high-quality image generation at varying resolutions.\"} ]`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m{'status': 'success', 'collapsed_summaries': [{'summary': \"The paper introduces a novel method called Laplacian Eigenvector Gradient Orthogonalization (LEGO) for estimating tangent spaces on data manifolds, addressing the limitations of traditional methods like Local Principal Component Analysis (LPCA) in noisy environments. LEGO utilizes graph Laplacian eigenvectors to leverage the global structure of data, orthogonalizing gradients of low-frequency eigenvectors to enhance the accuracy of tangent space estimates. Theoretical analyses confirm LEGO's robustness against noise, supported by differential geometry and random matrix theory. Empirical results show that LEGO outperforms LPCA in manifold learning, boundary detection, and local intrinsic dimension estimation. This advancement is significant for AI and data science professionals, offering a more reliable method for geometric data analysis in noisy conditions. Consequently, LEGO has the potential to improve machine learning applications where accurate data manifold estimation is crucial, impacting industries reliant on precise data analysis.\"}, {'summary': \"The paper presents KAVA, a groundbreaking framework for latent reasoning in large language models (LLMs) that utilizes compressed Key-Value (KV) cache distillation. This approach addresses the high computational cost of traditional chain-of-thought (CoT) reasoning by offering an efficient alternative without sacrificing accuracy. KAVA achieves this by transferring knowledge from a teacher model's compressed KV-cache to a latent reasoning student, aligning stepwise KV trajectories with continuous latent tokens. The framework not only maintains the accuracy of CoT-trained models but also enhances efficiency and scalability. KAVA demonstrates superior performance compared to existing latent reasoning methods, with minimal performance loss when shifting from equation-only to natural-language traces. It effectively scales to larger models, providing a scalable supervision signal that combines CoT accuracy with latent inference efficiency. This advancement is crucial for AI professionals aiming to develop efficient and deployable reasoning models, impacting practical applications and industry by offering a more scalable and efficient reasoning framework.\"}, {'summary': \"This blog post by Daniel Voigt Godoy outlines a practical approach to fine-tuning a large language model (LLM) using PyTorch and Hugging Face tools, focusing on Microsoft's Phi-3 Mini 4K Instruct model for translating English into Yoda-speak. Notable advancements include the use of quantized models to reduce memory usage and low-rank adapters (LoRA) to minimize trainable parameters, enhancing efficiency. Performance improvements are achieved through Hugging Face's SFTTrainer for supervised fine-tuning, emphasizing the importance of dataset formatting and tokenizer configuration for conversational AI models. The process involves converting datasets to a conversational format and aligning the tokenizer with the model's training, optimizing memory and configuration settings. The fine-tuned model successfully generates Yoda-like sentences, showcasing the effectiveness of the approach. Practical applications include the potential for broader use in conversational AI, with the model being made accessible via Hugging Face Hub. This demonstrates significant industry impact by facilitating the development and sharing of specialized language models.\"}, {'summary': \"The paper presents NoiseShift, a novel training-free technique designed to enhance low-resolution image generation in text-to-image diffusion models. These models typically face challenges in producing high-quality images at resolutions not encountered during training, especially lower ones. NoiseShift recalibrates the denoiser's noise level based on the resolution size, without modifying the model architecture or sampling schedule, addressing the perceptual mismatch from noise schedulers. This method significantly improves image quality at low resolutions, as evidenced by better FID scores on datasets like LAION-COCO and CelebA, using models such as Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev. The approach is lightweight, requiring no retraining, and effectively reduces resolution-dependent artifacts. This makes NoiseShift a practical solution for enhancing the adaptability and efficiency of diffusion models in generating low-resolution images, with potential industry impact in applications requiring high-quality image generation at varying resolutions.\"}], 'message': 'Summaries already fit in context (1,115 tokens). No collapse needed.'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_document_text` with `2510v02311v1.pdf`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m{'status': 'success', 'filename': '2510v02311v1.pdf', 'pages': 76, 'text': 'INFERRING DYNAMIC PHYSICAL PROPERTIES FROM VIDEO FOUNDATION MODELS\\n\\nGuanqi Zhan1∗, Xianzheng Ma1∗, Weidi Xie1,2, Andrew Zisserman1 1VGG, University of Oxford 2Shanghai Jiao Tong University {guanqi,xianzheng,weidi,az}@robots.ox.ac.uk\\n\\n5 2 0 2 t c O 2 ] V C . s c [ 1 v 1 1 3 2 0 . 0 1 5 2 : v i X r\\n\\na\\n\\nABSTRACT\\n\\nWe study the task of predicting dynamic physical properties from videos. More specifically, we consider physical properties that require temporal information to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid, and dy- namic friction of an object sliding on a surface. To this end, we make the following contributions: (i) We collect a new video dataset for each physical property, con- sisting of synthetic training and testing splits, as well as a real split for real world evaluation. (ii) We explore three ways to infer the physical property from videos: (a) an oracle method where we supply the visual cues that intrinsically reflect the property using classical computer vision techniques; (b) a simple read out mech- anism using a visual prompt and trainable prompt vector for cross-attention on pre-trained video generative and self-supervised models; and (c) prompt strate- gies for Multi-modal Large Language Models (MLLMs). (iii) We show that video foundation models trained in a generative or self-supervised manner achieve a similar performance, though behind that of the oracle, and MLLMs are currently inferior to the other models, though their performance can be improved through suitable prompting.\\n\\n1 INTRODUCTION\\n\\nHumans are remarkably adept at intuitively estimating physical properties from visual observations. Without direct interaction, people can often estimate how bouncy a ball is, how thick a liquid seems, or how slippery a surface might be—simply by watching how objects move. While these estimations are not precise in a scientific sense, they are sufficiently accurate for guiding perception, prediction, and action. Bringing this capability to machines is an important step towards building more general and physically grounded artificial intelligence. In particular, visual systems that can infer dynamic physical properties from raw video could enhance robotic manipulation, embodied agents, and video understanding tasks in ways that go beyond the traditional perception tasks of recognition, detection, and segmentation.\\n\\nRecent progress in video foundation models, including generative models (Xing et al., 2024; Liu et al., 2024b), self-supervised models (Bardes et al., 2023; Assran et al., 2025) and multi-modal large language models (MLLMs) (Hui et al., 2024; Comanici et al., 2025; Hurst et al., 2024), have shown impressive capability in synthesizing realistic dynamics, learning general-purpose video rep- resentations, and tackling semantic understanding tasks, for example, video question answering. However, a question that remains underexplored is: do these models acquire an understanding of dynamic physical properties from videos ?\\n\\nIn this paper, we address this question by focusing on several representative physical properties that are not directly observable in static frames but instead emerge through temporal dynamics: the elasticity of a bouncing object, the viscosity of a flowing liquid, and the dynamic friction between a surface and a sliding object. These properties are especially compelling because their inference requires temporal reasoning and sensitivity to subtle visual cues—such as deformation, deceleration, spreading, or oscillation. By examining how well current video foundation models capture these dynamic attributes, we aim to assess their physical understanding beyond static appearance.\\n\\n*Equal contribution.\\n\\nINFERRING DYNAMIC PHYSICAL PROPERTIES FROM VIDEO FOUNDATION MODELS\\n\\nGuanqi Zhan1∗, Xianzheng Ma1∗, Weidi Xie1,2, Andrew Zisserman1 1VGG, University of Oxford 2Shanghai Jiao Tong University {guanqi,xianzheng,weidi,az}@robots.ox.ac.uk\\n\\n5 2 0 2 t c O 2 ] V C . s c [ 1 v 1 1 3 2 0 . 0 1 5 2 : v i X r\\n\\na\\n\\nABSTRACT\\n\\nWe study the task of predicting dynamic physical properties from videos. More specifically, we consider physical properties that require temporal information to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid, and dy- namic friction of an object sliding on a surface. To this end, we make the following contributions: (i) We collect a new video dataset for each physical property, con- sisting of synthetic training and testing splits, as well as a real split for real world evaluation. (ii) We explore three ways to infer the physical property from videos: (a) an oracle method where we supply the visual cues that intrinsically reflect the property using classical computer vision techniques; (b) a simple read out mech- anism using a visual prompt and trainable prompt vector for cross-attention on pre-trained video generative and self-supervised models; and (c) prompt strate- gies for Multi-modal Large Language Models (MLLMs). (iii) We show that video foundation models trained in a generative or self-supervised manner achieve a similar performance, though behind that of the oracle, and MLLMs are currently inferior to the other models, though their performance can be improved through suitable prompting.\\n\\n1 INTRODUCTION\\n\\nHumans are remarkably adept at intuitively estimating physical properties from visual observations. Without direct interaction, people can often estimate how bouncy a ball is, how thick a liquid seems, or how slippery a surface might be—simply by watching how objects move. While these estimations are not precise in a scientific sense, they are sufficiently accurate for guiding perception, prediction, and action. Bringing this capability to machines is an important step towards building more general and physically grounded artificial intelligence. In particular, visual systems that can infer dynamic physical properties from raw video could enhance robotic manipulation, embodied agents, and video understanding tasks in ways that go beyond the traditional perception tasks of recognition, detection, and segmentation.\\n\\nRecent progress in video foundation models, including generative models (Xing et al., 2024; Liu et al., 2024b), self-supervised models (Bardes et al., 2023; Assran et al., 2025) and multi-modal large language models (MLLMs) (Hui et al., 2024; Comanici et al., 2025; Hurst et al., 2024), have shown impressive capability in synthesizing realistic dynamics, learning general-purpose video rep- resentations, and tackling semantic understanding tasks, for example, video question answering. However, a question that remains underexplored is: do these models acquire an understanding of dynamic physical properties from videos ?\\n\\nIn this paper, we address this question by focusing on several representative physical properties that are not directly observable in static frames but instead emerge through temporal dynamics: the elasticity of a bouncing object, the viscosity of a flowing liquid, and the dynamic friction between a surface and a sliding object. These properties are especially compelling because their inference requires temporal reasoning and sensitivity to subtle visual cues—such as deformation, deceleration, spreading, or oscillation. By examining how well current video foundation models capture these dynamic attributes, we aim to assess their physical understanding beyond static appearance.\\n\\n*Equal contribution.\\n\\nTo support this investigation, we introduce a new dataset, PhysVid, specifically designed to evaluate the dynamic physical properties from video. Existing datasets lack ground-truth annotations for such properties, so we construct PhysVid using a combination of synthetic videos—rendered via a physics simulator—and real-world videos sourced from the internet or captured in-house. Each video is annotated with physical property values, either derived from simulation parameters or estimated manually. The dataset is designed to facilitate the study of out-of-domain generalization, both within the synthetic domain and from synthetic to real-world data. To establish an upper bound on what is inferable from visual input alone, we implement an oracle method for each property. These oracles leverage privileged access to the visual cues that directly reflect the corresponding property.\\n\\nWe evaluate three categories of video foundation models: generative models, self-supervised mod- els, and multi-modal large language models (MLLMs). For the generative and self-supervised mod- els, we propose a simple yet effective readout mechanism that extracts dynamic physical properties from pre-trained, frozen representations. Our method introduces a learnable query vector that at- tends to internal representation tokens via cross-attention, enabling the selective extraction of rele- vant information. This approach is both lightweight and training-efficient. For MLLMs, we explore various prompting strategies to elicit predictions of dynamic physical properties directly from video input. These strategies include few-shot prompting to provide task context, as well as procedu- ral prompting that guides the model through the oracle estimation steps—helping it focus on the intrinsic visual cues that reveal the target properties.\\n\\n2 RELATED WORK\\n\\nPhysics Prediction from Images and Videos. Inferring physical properties from visual obser- vations remains a core challenge in computer vision. Early methods estimate latent physical pa- rameters (e.g., mass, friction, stiffness) via differentiable physics engines or learning-based simula- tors (Wu et al., 2015; Ding et al., 2021; Jatavallabhula et al., 2021; Li et al., 2020; Wang et al., 2020a; 2018), while later works infer salient attributes like viscosity or elasticity from task-specific visual cues (Kawabe et al., 2014; Paulun et al., 2015; Assen et al., 2018; Norman et al., 2007; Kawabe & Nishida, 2016; Paulun et al., 2017; Paulun & Fleming, 2020), yet both rely heavily on simulation supervision, domain priors, or handcrafted heuristics. More recently, unsupervised learning of intu- itive physics has emerged via next-frame prediction from large-scale everyday physical scenes (Vo- leti et al., 2022; Lu et al., 2023; Agrawal et al., 2016; Finn & Levine, 2017; Babaeizadeh et al., 2021; Hafner et al., 2019; Fragkiadaki et al., 2016; Garcia et al., 2025), capturing latent dynamics without explicit physical supervision. However, the resulting representations are usually implicit and lack interpretability in terms of concrete physical quantities. In contrast, we infer physical properties by directly prompting pre-trained video foundation models, enabling explicit estimation without reliance on task-specific heuristics, or end-to-end prediction pipelines from scratch.\\n\\nPhysics Datasets and Benchmarks. An increasing number of physics-related datasets have been collected in recent years to provide ground truth annotations for different physical properties, in- cluding material (Sharma et al., 2023; Gao et al., 2024), shadow (Wang et al., 2020b; 2021), support relations (Silberman et al., 2012),occlusion (Zhan et al., 2022; 2024a), mass and volume (Wu et al., 2016). Another line of work (Chow et al., 2025; Shen et al., 2025; Riochet et al., 2018; Bordes et al., 2025; Tung et al., 2023; Bear et al., 2021) proposes broad benchmarks with video-image-text QA tasks to assess physical understanding in vision-language models, but the questions are typi- cally qualitative and categorical. In contrast, our datasets consist of both synthetic and real-world videos annotated with the quantitative value for the associated physical parameter of the coefficient of friction, elasticity, and viscosity.\\n\\n3 PROBLEM SCENARIO AND THE PhysVid DATASETS\\n\\nIn this paper, we address the problem of estimating physical properties from videos. Specifically, we focus on three properties: elasticity of a bouncing object, viscosity of a flowing liquid, and the dy- namic friction coefficient between a surface and a sliding object. Given a video v ∈ RT×H×W×3, we consider two formulations, the first is absolute value prediction, where the input is a single video and the model is tasked with predicting the numerical value of the physical property, i.e., yabs = Φ(v;θ1). The second is relative value comparison, where the input is a pair of videos\\n\\nTo support this investigation, we introduce a new dataset, PhysVid, specifically designed to evaluate the dynamic physical properties from video. Existing datasets lack ground-truth annotations for such properties, so we construct PhysVid using a combination of synthetic videos—rendered via a physics simulator—and real-world videos sourced from the internet or captured in-house. Each video is annotated with physical property values, either derived from simulation parameters or estimated manually. The dataset is designed to facilitate the study of out-of-domain generalization, both within the synthetic domain and from synthetic to real-world data. To establish an upper bound on what is inferable from visual input alone, we implement an oracle method for each property. These oracles leverage privileged access to the visual cues that directly reflect the corresponding property.\\n\\nWe evaluate three categories of video foundation models: generative models, self-supervised mod- els, and multi-modal large language models (MLLMs). For the generative and self-supervised mod- els, we propose a simple yet effective readout mechanism that extracts dynamic physical properties from pre-trained, frozen representations. Our method introduces a learnable query vector that at- tends to internal representation tokens via cross-attention, enabling the selective extraction of rele- vant information. This approach is both lightweight and training-efficient. For MLLMs, we explore various prompting strategies to elicit predictions of dynamic physical properties directly from video input. These strategies include few-shot prompting to provide task context, as well as procedu- ral prompting that guides the model through the oracle estimation steps—helping it focus on the intrinsic visual cues that reveal the target properties.\\n\\n2 RELATED WORK\\n\\nPhysics Prediction from Images and Videos. Inferring physical properties from visual obser- vations remains a core challenge in computer vision. Early methods estimate latent physical pa- rameters (e.g., mass, friction, stiffness) via differentiable physics engines or learning-based simula- tors (Wu et al., 2015; Ding et al., 2021; Jatavallabhula et al., 2021; Li et al., 2020; Wang et al., 2020a; 2018), while later works infer salient attributes like viscosity or elasticity from task-specific visual cues (Kawabe et al., 2014; Paulun et al., 2015; Assen et al., 2018; Norman et al., 2007; Kawabe & Nishida, 2016; Paulun et al., 2017; Paulun & Fleming, 2020), yet both rely heavily on simulation supervision, domain priors, or handcrafted heuristics. More recently, unsupervised learning of intu- itive physics has emerged via next-frame prediction from large-scale everyday physical scenes (Vo- leti et al., 2022; Lu et al., 2023; Agrawal et al., 2016; Finn & Levine, 2017; Babaeizadeh et al., 2021; Hafner et al., 2019; Fragkiadaki et al., 2016; Garcia et al., 2025), capturing latent dynamics without explicit physical supervision. However, the resulting representations are usually implicit and lack interpretability in terms of concrete physical quantities. In contrast, we infer physical properties by directly prompting pre-trained video foundation models, enabling explicit estimation without reliance on task-specific heuristics, or end-to-end prediction pipelines from scratch.\\n\\nPhysics Datasets and Benchmarks. An increasing number of physics-related datasets have been collected in recent years to provide ground truth annotations for different physical properties, in- cluding material (Sharma et al., 2023; Gao et al., 2024), shadow (Wang et al., 2020b; 2021), support relations (Silberman et al., 2012),occlusion (Zhan et al., 2022; 2024a), mass and volume (Wu et al., 2016). Another line of work (Chow et al., 2025; Shen et al., 2025; Riochet et al., 2018; Bordes et al., 2025; Tung et al., 2023; Bear et al., 2021) proposes broad benchmarks with video-image-text QA tasks to assess physical understanding in vision-language models, but the questions are typi- cally qualitative and categorical. In contrast, our datasets consist of both synthetic and real-world videos annotated with the quantitative value for the associated physical parameter of the coefficient of friction, elasticity, and viscosity.\\n\\n3 PROBLEM SCENARIO AND THE PhysVid DATASETS\\n\\nIn this paper, we address the problem of estimating physical properties from videos. Specifically, we focus on three properties: elasticity of a bouncing object, viscosity of a flowing liquid, and the dy- namic friction coefficient between a surface and a sliding object. Given a video v ∈ RT×H×W×3, we consider two formulations, the first is absolute value prediction, where the input is a single video and the model is tasked with predicting the numerical value of the physical property, i.e., yabs = Φ(v;θ1). The second is relative value comparison, where the input is a pair of videos\\n\\nFigure 1: Examples of the PhysVid dataset. Each row shows a different property, and each column shows three frames from video samples in the synthetic sets (train, test-1, and test-2) and the real test-3 set. The train and test-1 sets are from the same distribution. In test-2 parameters, such as lighting, viewpoint and color, differ from those in test-1.\\n\\ncaptured from the same viewpoint, and the model must determine whether the first video exhibits a higher physical property value than the second, i.e., yrel = Φ(v1,v2;θ2), and yrel is binary.\\n\\nEach scenario is parameterized by a set of variables, including the value of the target physical prop- erty (e.g., elasticity, viscosity, or friction), and a set of nuisance parameters (including camera view- point, object appearance, lighting, etc.). While the model must be sensitive to changes in the physical property, it should be robust (ideally invariant) to variations in nuisance parameters.\\n\\nTo assess generalization, we define two domains of nuisance parameters, denoted as A1 and A2, which differ in their distributions. For instance, A2 may have different camera viewpoints or dif- ferent lighting conditions to A1 (full details of these differences are given in Appendix Section B). We generate a dataset using a physics-based simulator, consisting of one training split and two test splits. The models are only trained on the training split from the simulator for all the evaluations. The training and test-1 splits are sampled from A1, sharing the same nuisance distribution; test-2 is drawn from A2, introducing a distribution shift. The target property values are sampled from a shared range across all splits to ensure consistency. Finally, test-3 consists of real-world videos, used to evaluate generalization beyond simulation.\\n\\n3.1 THE PhysVid DATASETS\\n\\nTo study the dynamic physical properties of elasticity, viscosity, and friction, we construct a dataset for each, containing both synthetic and real-world videos. Synthetic ones are generated with the Genesis simulator (Zhou et al., 2024), and real ones are captured with an iPhone in slow-motion mode or downloaded from the Internet. For each property we have: 10,000 videos for train; 1000 videos for each of test-1 and test-2; and 100 videos for test-3. Sample frames are shown in Figure 1. In the following we describe how each property is realized in the video. Please refer to Appendix Section B for more details of the datasets.\\n\\nElasticity\\n\\nWe study an object’s elasticity by analyzing the motion of a ball dropped onto the ground and its subsequent bounces. In physics, elasticity e is quantified as the ratio of the rebound velocity Vafter impact t0 the impact velocity vbefore impact and also equals ounce/ rop Where hgrop is the dropping height and Apunce 1S the bouncing height. Here and for the following properties, please refer to Appendix Scnr the detailed derivations. These expressions are used for the oracle estimation in Section\\n\\nSynthetic Dataset. All synthetic videos are generated using Genesis (Zhou et al., 2024), with ob- ject’s elasticity as the target property. Nuisance factors include drop height, camera viewpoint, object appearance, and lighting conditions. The object is of the same size in all videos. Note, here and for the following properties, the ground truth property value is obtained directly from the simulator.\\n\\nReal-World Dataset. The real-world videos are collected from YouTube using the search term “ball bouncing experiments”. Each clip is manually trimmed to include the drop-and-bounce sequence of a single ball. The dataset includes a wide range of materials (e.g., rubber balls, tennis balls, basketballs, balloons, efc), resulting in diverse elasticity values. The ground truth elasticity values for the real sequences are estimated by computing pounce harop: the videos are chosen such that the balls bounce in a fronto-parallel plane, which means that ratios of image heights (differences in y-coordinates) are approximately equal to the ratio of heights in 3D. These image differences are obtained by manual annotation.\\n\\nFigure 1: Examples of the PhysVid dataset. Each row shows a different property, and each column shows three frames from video samples in the synthetic sets (train, test-1, and test-2) and the real test-3 set. The train and test-1 sets are from the same distribution. In test-2 parameters, such as lighting, viewpoint and color, differ from those in test-1.\\n\\ncaptured from the same viewpoint, and the model must determine whether the first video exhibits a higher physical property value than the second, i.e., yrel = Φ(v1,v2;θ2), and yrel is binary.\\n\\nEach scenario is parameterized by a set of variables, including the value of the target physical prop- erty (e.g., elasticity, viscosity, or friction), and a set of nuisance parameters (including camera view- point, object appearance, lighting, etc.). While the model must be sensitive to changes in the physical property, it should be robust (ideally invariant) to variations in nuisance parameters.\\n\\nTo assess generalization, we define two domains of nuisance parameters, denoted as A1 and A2, which differ in their distributions. For instance, A2 may have different camera viewpoints or dif- ferent lighting conditions to A1 (full details of these differences are given in Appendix Section B). We generate a dataset using a physics-based simulator, consisting of one training split and two test splits. The models are only trained on the training split from the simulator for all the evaluations. The training and test-1 splits are sampled from A1, sharing the same nuisance distribution; test-2 is drawn from A2, introducing a distribution shift. The target property values are sampled from a shared range across all splits to ensure consistency. Finally, test-3 consists of real-world videos, used to evaluate generalization beyond simulation.\\n\\n3.1 THE PhysVid DATASETS\\n\\nTo study the dynamic physical properties of elasticity, viscosity, and friction, we construct a dataset for each, containing both synthetic and real-world videos. Synthetic ones are generated with the Genesis simulator (Zhou et al., 2024), and real ones are captured with an iPhone in slow-motion mode or downloaded from the Internet. For each property we have: 10,000 videos for train; 1000 videos for each of test-1 and test-2; and 100 videos for test-3. Sample frames are shown in Figure 1. In the following we describe how each property is realized in the video. Please refer to Appendix Section B for more details of the datasets.\\n\\nElasticity\\n\\nWe study an object’s elasticity by analyzing the motion of a ball dropped onto the ground and its subsequent bounces. In physics, elasticity e is quantified as the ratio of the rebound velocity Vafter impact t0 the impact velocity vbefore impact and also equals ounce/ rop Where hgrop is the dropping height and Apunce 1S the bouncing height. Here and for the following properties, please refer to Appendix Scnr the detailed derivations. These expressions are used for the oracle estimation in Section\\n\\nSynthetic Dataset. All synthetic videos are generated using Genesis (Zhou et al., 2024), with ob- ject’s elasticity as the target property. Nuisance factors include drop height, camera viewpoint, object appearance, and lighting conditions. The object is of the same size in all videos. Note, here and for the following properties, the ground truth property value is obtained directly from the simulator.\\n\\nReal-World Dataset. The real-world videos are collected from YouTube using the search term “ball bouncing experiments”. Each clip is manually trimmed to include the drop-and-bounce sequence of a single ball. The dataset includes a wide range of materials (e.g., rubber balls, tennis balls, basketballs, balloons, efc), resulting in diverse elasticity values. The ground truth elasticity values for the real sequences are estimated by computing pounce harop: the videos are chosen such that the balls bounce in a fronto-parallel plane, which means that ratios of image heights (differences in y-coordinates) are approximately equal to the ratio of heights in 3D. These image differences are obtained by manual annotation.\\n\\nViscosity\\n\\nWe study the viscosity by observing a liquid column dropping and spreading on the ground. The viscosity can be reflected by the growth rate of the liquid area on the ground. The viscosity µ is negatively correlated to the liquid area growth rate d(A(t)) , given the controlled liquid density ρ, dt controlled liquid column diameter D, and controlled dropping velocity v of the liquid column when it reaches the ground.\\n\\nSynthetic Dataset. The synthetic videos are generated using Genesis (Zhou et al., 2024), where the target property is the viscosity of liquid. Nuisance factors include camera viewpoint, object appearance, and lighting conditions. The liquid column is of the same size in all videos.\\n\\nReal-World Dataset. Since it is challenging to find real-world videos online that provide ground- truth viscosity values while controlling for other relevant physical parameters—such as ρ, D and v, we collected real videos under controlled conditions. We use a funnel with a fixed nozzle diameter to produce a consistent liquid column. A funnel holder allows us to fix the height from which the liquid is poured, thereby controlling the initial velocity v. Ground-truth viscosity values for each liquid are obtained from standard physics reference tables. The selected liquids span a wide range of viscosities, from 1.2 (e.g., coffee) to 225 (e.g., maple syrup), allowing for a diverse and comprehensive evaluation.\\n\\nFriction\\n\\nWe study friction between an object and a surface by observing how the object slows down as it slides with an initial velocity. The dynamic friction coefficient µk is proportional to the (negative) acceleration of the object a.\\n\\nSynthetic Dataset. The synthetic videos are generated using Genesis (Zhou et al., 2024), where the target property is the dynamic friction coefficient at the contacting surface of the object and the ground. Nuisance factors include initial location and initial velocity of the object, camera viewpoint, object appearance, and lighting conditions. The object is of the same size in all videos.\\n\\nReal-World Dataset. While many online videos depict objects sliding on surfaces, they lack ground- truth annotations for friction coefficients. We therefore collect a real video dataset featuring 5 differ- ent objects and 6 surface materials, spanning a wide range of dynamic friction values. Each object is given an initial velocity by sliding it down from a slope and it then slides on a horizontal plane. To obtain ground-truth friction coefficients, we use a spring dynamometer to measure the friction force F for each object-surface pair (by dragging the object at constant speed), and record the object’s weight G. The dynamic friction coefficient is then computed as: µk = F/G.\\n\\n4 INFERRING PHYSICAL PROPERTIES\\n\\nThis section presents the three different ways for inferring dynamic physical properties: an oracle method via classical computer vision techniques (Section 4.1); a visual prompt mechanism for video generative and self-supervised models (Section 4.2); and prompts for MLLMs (Section 4.3).\\n\\n4.1 ORACLE ESTIMATION\\n\\nElasticity. We aim to estimate elasticity from both synthetic and real-world videos. The key visual cue is the relative height of the ball during its drop and subsequent bounce, observed in 3D. As noted earlier, the ratio in 3D can be approximated from their corresponding image-space measurements. This approximation is exact when the motion occurs in a fronto-parallel plane, and remains reason- ably accurate otherwise—since the ratio of lengths between parallel line segments is invariant under affine transformations (Hartley & Zisserman, 2004). Given that perspective effects are minimal in our videos, the affine approximation provides a reliable estimate for elasticity. To automate this process, we extract the ball’s trajectory y(t) from the video and input the sequence of ratios into a GRU network to regress the elasticity. In detail, we segment the ball in each frame and use their centroids as the y-coordinate. From this trajectory, we identify key points: the initial drop position, the first ground contact, and the peak of the first bounce. The resulting trajectory is normalized to the range [0,1], by subtracting the y-coordinate of the first ground contact and dividing by the initial drop height. This normalization not only ensures invariance to viewpoint and scale, but also sim-\\n\\nViscosity\\n\\nWe study the viscosity by observing a liquid column dropping and spreading on the ground. The viscosity can be reflected by the growth rate of the liquid area on the ground. The viscosity µ is negatively correlated to the liquid area growth rate d(A(t)) , given the controlled liquid density ρ, dt controlled liquid column diameter D, and controlled dropping velocity v of the liquid column when it reaches the ground.\\n\\nSynthetic Dataset. The synthetic videos are generated using Genesis (Zhou et al., 2024), where the target property is the viscosity of liquid. Nuisance factors include camera viewpoint, object appearance, and lighting conditions. The liquid column is of the same size in all videos.\\n\\nReal-World Dataset. Since it is challenging to find real-world videos online that provide ground- truth viscosity values while controlling for other relevant physical parameters—such as ρ, D and v, we collected real videos under controlled conditions. We use a funnel with a fixed nozzle diameter to produce a consistent liquid column. A funnel holder allows us to fix the height from which the liquid is poured, thereby controlling the initial velocity v. Ground-truth viscosity values for each liquid are obtained from standard physics reference tables. The selected liquids span a wide range of viscosities, from 1.2 (e.g., coffee) to 225 (e.g., maple syrup), allowing for a diverse and comprehensive evaluation.\\n\\nFriction\\n\\nWe study friction between an object and a surface by observing how the object slows down as it slides with an initial velocity. The dynamic friction coefficient µk is proportional to the (negative) acceleration of the object a.\\n\\nSynthetic Dataset. The synthetic videos are generated using Genesis (Zhou et al., 2024), where the target property is the dynamic friction coefficient at the contacting surface of the object and the ground. Nuisance factors include initial location and initial velocity of the object, camera viewpoint, object appearance, and lighting conditions. The object is of the same size in all videos.\\n\\nReal-World Dataset. While many online videos depict objects sliding on surfaces, they lack ground- truth annotations for friction coefficients. We therefore collect a real video dataset featuring 5 differ- ent objects and 6 surface materials, spanning a wide range of dynamic friction values. Each object is given an initial velocity by sliding it down from a slope and it then slides on a horizontal plane. To obtain ground-truth friction coefficients, we use a spring dynamometer to measure the friction force F for each object-surface pair (by dragging the object at constant speed), and record the object’s weight G. The dynamic friction coefficient is then computed as: µk = F/G.\\n\\n4 INFERRING PHYSICAL PROPERTIES\\n\\nThis section presents the three different ways for inferring dynamic physical properties: an oracle method via classical computer vision techniques (Section 4.1); a visual prompt mechanism for video generative and self-supervised models (Section 4.2); and prompts for MLLMs (Section 4.3).\\n\\n4.1 ORACLE ESTIMATION\\n\\nElasticity. We aim to estimate elasticity from both synthetic and real-world videos. The key visual cue is the relative height of the ball during its drop and subsequent bounce, observed in 3D. As noted earlier, the ratio in 3D can be approximated from their corresponding image-space measurements. This approximation is exact when the motion occurs in a fronto-parallel plane, and remains reason- ably accurate otherwise—since the ratio of lengths between parallel line segments is invariant under affine transformations (Hartley & Zisserman, 2004). Given that perspective effects are minimal in our videos, the affine approximation provides a reliable estimate for elasticity. To automate this process, we extract the ball’s trajectory y(t) from the video and input the sequence of ratios into a GRU network to regress the elasticity. In detail, we segment the ball in each frame and use their centroids as the y-coordinate. From this trajectory, we identify key points: the initial drop position, the first ground contact, and the peak of the first bounce. The resulting trajectory is normalized to the range [0,1], by subtracting the y-coordinate of the first ground contact and dividing by the initial drop height. This normalization not only ensures invariance to viewpoint and scale, but also sim-\\n\\nFigure 2: Oracle methods for physical properties. The objective in each case is to extract a measurement from the sequence that can directly be used to predict the property. For elasticity, we extract the centroid trajectory from segmentation masks, and then normalize the y-coordinates into 0-1; the ratio of bouncing to dropping height over the sequence indicates the elasticity. For viscosity, we calculate the area size in the image via segmentation masks, and then normalize the area sizes by the area in the frame when the liquid first touches the ground; the slope of the normalized area size sequence reflects the viscosity. For friction, we transform to a bird’s eye view (using a homography transformation based on 4 corner points of the top surface of the sliding object), and fit a parabola x = αt2 + βt + c to the transformed trajectory; the parabola coefficient α predicts the friction coefficient. For each video, we show the segmentation for two frames (left → right).\\n\\nplifies learning for the GRU by standardizing the input distribution. We train a GRU, as it is noisy to directly obtain hdrop and hbounce using heuristics (e.g., determining the maximum and minimum points), and in practice a GRU provides a good estimate. The full pipeline is illustrated in Fig- ure 2 (top row). For the absolute prediction, the normalized trajectory is fed into a GRU network, which directly regresses the elasticity value. For the relative comparison, the binary decision score between two videos v1 and v2 is calculated as:\\n\\nwhere e1 and e2 are the estimated elasticities based on height ratios, and σ(·) denotes the sigmoid function.\\n\\nViscosity. The key visual cue for estimating viscosity is the rate at which the liquid spreads on the ground-plane, measured as an area ratio normalized by the initial area of the liquid column. As with elasticity, we approximate perspective using an affine transformation – here of the ground- plane. Since area ratios are invariant under affine transformations (Hartley & Zisserman, 2004), the liquid’s normalized image-space area growth approximates its true normalized ground-plane expansion (in our setup the liquid spreads only within a limited area around the release point, and the camera is distant; consequently an affine viewing approximation is adequate). Specifically, we extract segmentation masks for each frame and compute the liquid’s area over time. This area sequence is normalized by the area in the first frame where the liquid contacts the surface, ensuring invariance to viewpoint and scale. The process is illustrated in Figure 2 (middle row). For absolute prediction, we calculate the slope k of A(t) and use 1/k to represent the viscosity value; For relative comparison, the binary decision score between two videos v1 and v2 is calculated as in Equation 1, where e1 and e2 are the estimated viscosities based on area growth rate.\\n\\nFriction. The key visual cue for estimating dynamic friction is the acceleration of the sliding ob- ject—i.e., how quickly its velocity decreases due to friction—which can be inferred from its position over time. Since the object moves significantly in the video, we do not use an affine approximation, but instead take account of the projective geometry by mapping the object’s motion to a bird’s-eye view, allowing for consistent trajectory analysis. This is achieved by estimating a homography be- tween the image and bird’s eye view (normal to the plane) from the four corners of the object’s top surface (see Figure 2, bottom row). We fit a parabola x = αt2 + βt + c to the transformed top surface trajectory to estimate the acceleration a from the coefficient α, and the coefficient of friction µk = 2α/g. For absolute prediction, we use the estimated µk to represent the friction coefficient value; For relative comparison, the binary decision score between two videos v1 and v2 is calculated as in Equation 1, where e1 and e2 are the estimated friction coefficients based on the transformed object trajectory.\\n\\n(1)\\n\\n4.2 VIDEO GENERATIVE AND SELF-SUPERVISED MODELS\\n\\nVideo Feature Extraction\\n\\nFigure 2: Oracle methods for physical properties. The objective in each case is to extract a measurement from the sequence that can directly be used to predict the property. For elasticity, we extract the centroid trajectory from segmentation masks, and then normalize the y-coordinates into 0-1; the ratio of bouncing to dropping height over the sequence indicates the elasticity. For viscosity, we calculate the area size in the image via segmentation masks, and then normalize the area sizes by the area in the frame when the liquid first touches the ground; the slope of the normalized area size sequence reflects the viscosity. For friction, we transform to a bird’s eye view (using a homography transformation based on 4 corner points of the top surface of the sliding object), and fit a parabola x = αt2 + βt + c to the transformed trajectory; the parabola coefficient α predicts the friction coefficient. For each video, we show the segmentation for two frames (left → right).\\n\\nplifies learning for the GRU by standardizing the input distribution. We train a GRU, as it is noisy to directly obtain hdrop and hbounce using heuristics (e.g., determining the maximum and minimum points), and in practice a GRU provides a good estimate. The full pipeline is illustrated in Fig- ure 2 (top row). For the absolute prediction, the normalized trajectory is fed into a GRU network, which directly regresses the elasticity value. For the relative comparison, the binary decision score between two videos v1 and v2 is calculated as:\\n\\nwhere e1 and e2 are the estimated elasticities based on height ratios, and σ(·) denotes the sigmoid function.\\n\\nViscosity. The key visual cue for estimating viscosity is the rate at which the liquid spreads on the ground-plane, measured as an area ratio normalized by the initial area of the liquid column. As with elasticity, we approximate perspective using an affine transformation – here of the ground- plane. Since area ratios are invariant under affine transformations (Hartley & Zisserman, 2004), the liquid’s normalized image-space area growth approximates its true normalized ground-plane expansion (in our setup the liquid spreads only within a limited area around the release point, and the camera is distant; consequently an affine viewing approximation is adequate). Specifically, we extract segmentation masks for each frame and compute the liquid’s area over time. This area sequence is normalized by the area in the first frame where the liquid contacts the surface, ensuring invariance to viewpoint and scale. The process is illustrated in Figure 2 (middle row). For absolute prediction, we calculate the slope k of A(t) and use 1/k to represent the viscosity value; For relative comparison, the binary decision score between two videos v1 and v2 is calculated as in Equation 1, where e1 and e2 are the estimated viscosities based on area growth rate.\\n\\nFriction. The key visual cue for estimating dynamic friction is the acceleration of the sliding ob- ject—i.e., how quickly its velocity decreases due to friction—which can be inferred from its position over time. Since the object moves significantly in the video, we do not use an affine approximation, but instead take account of the projective geometry by mapping the object’s motion to a bird’s-eye view, allowing for consistent trajectory analysis. This is achieved by estimating a homography be- tween the image and bird’s eye view (normal to the plane) from the four corners of the object’s top surface (see Figure 2, bottom row). We fit a parabola x = αt2 + βt + c to the transformed top surface trajectory to estimate the acceleration a from the coefficient α, and the coefficient of friction µk = 2α/g. For absolute prediction, we use the estimated µk to represent the friction coefficient value; For relative comparison, the binary decision score between two videos v1 and v2 is calculated as in Equation 1, where e1 and e2 are the estimated friction coefficients based on the transformed object trajectory.\\n\\n(1)\\n\\n4.2 VIDEO GENERATIVE AND SELF-SUPERVISED MODELS\\n\\nVideo Feature Extraction\\n\\nFigure 3: Architectures for dynamic physical property prediction. Left: video generative model as backbone; Middle: video self-supervised model as backbone; Right: multimodal large language model (MLLM). For the pre-trained video diffusion model (U-Net, left) and the pre-trained self- supervised model (ViT, middle), the representations are kept frozen, and a ‘visual prompt’ learns to infer the physical properties. For the MLLMs, the physical properties are inferred using a language prompt (right).\\n\\nGiven a video v ∈ RT×H×W×3, we extract features with a pre-trained video backbone, that can either be generative or self-supervised, resulting into spatiotemporal feature representations, i.e., r = ψ(v) ∈ Rt×h×w×c, which can be detailed as follows.\\n\\nGenerative Model as Backbone. We adopt a pre-trained video diffusion model (Figure left), namely DynamiCrafter (Xing et al to compute the visual features. Specifically, given an input video, we add noise to the latent representations after the pre-trained VAE encoder, and re- place the text prompt with a learnable embedding. We extract multi-scale features from all U-Net layers at diffusion tme 50, Wlll was shown to be effective for capturing 3D physics in prior work ( Tang et al . To aggregate the features, we introduce a learnable query vector ¢, different dimensions of the multi-scale features (see Appendix Setn - r et and then attends to the diffusion tokens (;) via cross-attention: txhxw p= i=1 softmax(q - r;) - r;. The resulting vectors p from different layers are then mapped by another MLP network to a common dimension and average pooled to generate the final video feature representation P. To predict the physical properties, we train the text token of the generative model, together with the ‘visual prompt’ architecture that includes the query ¢ and the MLPs\\n\\nSelf-Supervised Model as Backbone. Here, we adopt a pre-trained self-supervised model (Figure 3, middle), namely V-JEPA-2 (Assran et al., 2025), as the visual backbone. The input video is passed through the model, and we extract feature tokens from all layers of the ViT encoder. Similar to the generative setting, we introduce a learnable query vector q to extract the video feature representation P from the ViT tokens via attentive pooling. Although the feature dimension at each ViT layer is the same, we still use a MLP network to map q to generate the query vector of each layer (keeping it similar to the generative setting in terms of MLP network architecture), and use another MLP network to map the output vectors p to a same dimension as the generative setting before average pooling them to get P. Please see Appendix Section A.2 for more details.\\n\\nPhysical Property Prediction\\n\\nGiven the computed feature P from video foundation models, we train a MLP network to predict the physical properties using the synthetic video dataset training split. The network for each property is trained separately.\\n\\nAbsolute Value Prediction. Given the resulting video feature (P), we pass it through a MLP network γ to predict the absolute value χ of the physical property: χ = γ(P). For elasticity and friction, the absolute value prediction is supervised with L1 loss with the ground truth value; For viscosity, as the ground truth values may have very different scales, i.e., from 1e−5 to 1e−2, the absolute value prediction is trained with Log L1 loss, which calculates L1 loss between the log of the predicted value and the log of the ground truth value.\\n\\nRelative Value Prediction. Given the resulting features for a pair of videos, P1 and P2, we con- catenate them and formulate a binary classification problem, indicating which video has a larger physical property value via a MLP network γ: ξ = γ([P1,P2]). The binary prediction for all three tasks is trained with binary cross entropy loss with the binary ground truth.\\n\\nBridging the Sim2real Gap. Since our models are trained on synthetic datasets, they may not generalize well to real-world test videos due to the domain gap. To mitigate this sim-to-real gap, for both synthetic training and real test, we draw a red circle on each video frame, enclosing the full trajectory of the target object or liquid, as illustrated in Figure 4 (middle). The red circle is obtained\\n\\nFigure 3: Architectures for dynamic physical property prediction. Left: video generative model as backbone; Middle: video self-supervised model as backbone; Right: multimodal large language model (MLLM). For the pre-trained video diffusion model (U-Net, left) and the pre-trained self- supervised model (ViT, middle), the representations are kept frozen, and a ‘visual prompt’ learns to infer the physical properties. For the MLLMs, the physical properties are inferred using a language prompt (right).\\n\\nGiven a video v ∈ RT×H×W×3, we extract features with a pre-trained video backbone, that can either be generative or self-supervised, resulting into spatiotemporal feature representations, i.e., r = ψ(v) ∈ Rt×h×w×c, which can be detailed as follows.\\n\\nGenerative Model as Backbone. We adopt a pre-trained video diffusion model (Figure left), namely DynamiCrafter (Xing et al to compute the visual features. Specifically, given an input video, we add noise to the latent representations after the pre-trained VAE encoder, and re- place the text prompt with a learnable embedding. We extract multi-scale features from all U-Net layers at diffusion tme 50, Wlll was shown to be effective for capturing 3D physics in prior work ( Tang et al . To aggregate the features, we introduce a learnable query vector ¢, different dimensions of the multi-scale features (see Appendix Setn - r et and then attends to the diffusion tokens (;) via cross-attention: txhxw p= i=1 softmax(q - r;) - r;. The resulting vectors p from different layers are then mapped by another MLP network to a common dimension and average pooled to generate the final video feature representation P. To predict the physical properties, we train the text token of the generative model, together with the ‘visual prompt’ architecture that includes the query ¢ and the MLPs\\n\\nSelf-Supervised Model as Backbone. Here, we adopt a pre-trained self-supervised model (Figure 3, middle), namely V-JEPA-2 (Assran et al., 2025), as the visual backbone. The input video is passed through the model, and we extract feature tokens from all layers of the ViT encoder. Similar to the generative setting, we introduce a learnable query vector q to extract the video feature representation P from the ViT tokens via attentive pooling. Although the feature dimension at each ViT layer is the same, we still use a MLP network to map q to generate the query vector of each layer (keeping it similar to the generative setting in terms of MLP network architecture), and use another MLP network to map the output vectors p to a same dimension as the generative setting before average pooling them to get P. Please see Appendix Section A.2 for more details.\\n\\nPhysical Property Prediction\\n\\nGiven the computed feature P from video foundation models, we train a MLP network to predict the physical properties using the synthetic video dataset training split. The network for each property is trained separately.\\n\\nAbsolute Value Prediction. Given the resulting video feature (P), we pass it through a MLP network γ to predict the absolute value χ of the physical property: χ = γ(P). For elasticity and friction, the absolute value prediction is supervised with L1 loss with the ground truth value; For viscosity, as the ground truth values may have very different scales, i.e., from 1e−5 to 1e−2, the absolute value prediction is trained with Log L1 loss, which calculates L1 loss between the log of the predicted value and the log of the ground truth value.\\n\\nRelative Value Prediction. Given the resulting features for a pair of videos, P1 and P2, we con- catenate them and formulate a binary classification problem, indicating which video has a larger physical property value via a MLP network γ: ξ = γ([P1,P2]). The binary prediction for all three tasks is trained with binary cross entropy loss with the binary ground truth.\\n\\nBridging the Sim2real Gap. Since our models are trained on synthetic datasets, they may not generalize well to real-world test videos due to the domain gap. To mitigate this sim-to-real gap, for both synthetic training and real test, we draw a red circle on each video frame, enclosing the full trajectory of the target object or liquid, as illustrated in Figure 4 (middle). The red circle is obtained\\n\\nautomatically as a bounding ellipse enclosing the merged masks of the target object or liquid across all frames. This visual cue directs the model’s attention to the relevant region (Shtedritski et al., 2023), effectively signaling which object to focus on for physical reasoning. The red circle serves as a lightweight yet effective form of weak annotation that helps the model localize and interpret the dynamics of interest. Please refer to Appendix Section G for the quantitative results demonstrating the effectiveness of drawing such red circles to mitigate the sim-to-real gap.\\n\\n4.3 MULTIMODAL LARGE LANGUAGE MODELS\\n\\nThis section studies off-the-shelf multimodal large language models (MLLMs) for understanding dynamic physical properties from video. We explore various prompting strategies on state-of-the- art MLLMs, including Qwen2.5-VL-Max (Hui et al., 2024), GPT-4o (Hurst et al., 2024), and Gem- ini 2.5 Pro (Comanici et al., 2025), as illustrated in Figure 3 (right). Examples of the prompting strategies are provided in Appendix Section E.\\n\\nPreliminary. The MLLM receives video frames as visual input. The text prompt includes (1) a brief description of the target property—for example: “we are studying the viscosity of the liquid, where water is 1.0 and honey is 5000.0.” This is followed by (2) a query, such as: “what is the viscosity value of the liquid in the video?” (absolute) or “which video shows a liquid with higher viscosity? please output a decision score between 0 and 1, indicating the likelihood that the first video exhibits a higher property value.” (relative). All the following prompt strategies provide (1) and (2) by default, and we note the differences and extensions.\\n\\nBaseline Prompt. For relative tasks, we specify that the first n frames belong to the first video and the last n to the second.\\n\\nBlack Frames in Between. For the relative setting, we insert black frames between the two video segments to clearly separate them. In the prompt, we refer to the videos as the frames before and after the black frames, rather than as the first and last n frames.\\n\\nFew-Shot Examples. For both relative and absolute settings, we provide several examples, includ- ing the video input and desired ground truth. For fair comparison with visual prompting, we use examples in the synthetic training split.\\n\\nFrame Index Provided. For both relative and absolute settings, we input the text of the index of each frame along with the frames. In this way the MLLMs may have a better understanding about the temporal relations between the input video frames.\\n\\nOracle Estimation Teaching. For both relative and absolute settings, we provide the key cue to concentrate on from the PhysVid Datasets section description to teach the MLLM how to estimate the properties step by step.\\n\\n5 EXPERIMENTS\\n\\nImplementation Details. During oracle estimation, we train the GRU network with a learning rate of 1e − 3 and the batch size is 128. For the generative and self-supervised video models, the backbones are frozen, the trainable parameters are optimised with a learning rate of 1e − 5 and the batch size 16. For MLLMs, we perform promp selection, and use the best strategy that we find for each of the absolute and relative settings for the experiments. Few-shot examples and oracle estimation teaching work best for the absolute and relative settings, respectively, as they directly provide the model with more context information about the properties. Please refer to Appendix Section D for the comparison results and analysis. All models are trained on H100/A6000/A40 GPUs. Please refer to Appendix Section A for more implementation details.\\n\\nEvaluation Metrics. For relative value comparison, we report the ROC AUC score; for absolute value prediction, we use the Pearson Correlation Coefficient between the prediction and ground truth as this automatically calibrates the predictions to the scale of the ground truth. Please refer to Appendix Section A.4 for more details and motivations on the evaluation metrics.\\n\\nautomatically as a bounding ellipse enclosing the merged masks of the target object or liquid across all frames. This visual cue directs the model’s attention to the relevant region (Shtedritski et al., 2023), effectively signaling which object to focus on for physical reasoning. The red circle serves as a lightweight yet effective form of weak annotation that helps the model localize and interpret the dynamics of interest. Please refer to Appendix Section G for the quantitative results demonstrating the effectiveness of drawing such red circles to mitigate the sim-to-real gap.\\n\\n4.3 MULTIMODAL LARGE LANGUAGE MODELS\\n\\nThis section studies off-the-shelf multimodal large language models (MLLMs) for understanding dynamic physical properties from video. We explore various prompting strategies on state-of-the- art MLLMs, including Qwen2.5-VL-Max (Hui et al., 2024), GPT-4o (Hurst et al., 2024), and Gem- ini 2.5 Pro (Comanici et al., 2025), as illustrated in Figure 3 (right). Examples of the prompting strategies are provided in Appendix Section E.\\n\\nPreliminary. The MLLM receives video frames as visual input. The text prompt includes (1) a brief description of the target property—for example: “we are studying the viscosity of the liquid, where water is 1.0 and honey is 5000.0.” This is followed by (2) a query, such as: “what is the viscosity value of the liquid in the video?” (absolute) or “which video shows a liquid with higher viscosity? please output a decision score between 0 and 1, indicating the likelihood that the first video exhibits a higher property value.” (relative). All the following prompt strategies provide (1) and (2) by default, and we note the differences and extensions.\\n\\nBaseline Prompt. For relative tasks, we specify that the first n frames belong to the first video and the last n to the second.\\n\\nBlack Frames in Between. For the relative setting, we insert black frames between the two video segments to clearly separate them. In the prompt, we refer to the videos as the frames before and after the black frames, rather than as the first and last n frames.\\n\\nFew-Shot Examples. For both relative and absolute settings, we provide several examples, includ- ing the video input and desired ground truth. For fair comparison with visual prompting, we use examples in the synthetic training split.\\n\\nFrame Index Provided. For both relative and absolute settings, we input the text of the index of each frame along with the frames. In this way the MLLMs may have a better understanding about the temporal relations between the input video frames.\\n\\nOracle Estimation Teaching. For both relative and absolute settings, we provide the key cue to concentrate on from the PhysVid Datasets section description to teach the MLLM how to estimate the properties step by step.\\n\\n5 EXPERIMENTS\\n\\nImplementation Details. During oracle estimation, we train the GRU network with a learning rate of 1e − 3 and the batch size is 128. For the generative and self-supervised video models, the backbones are frozen, the trainable parameters are optimised with a learning rate of 1e − 5 and the batch size 16. For MLLMs, we perform promp selection, and use the best strategy that we find for each of the absolute and relative settings for the experiments. Few-shot examples and oracle estimation teaching work best for the absolute and relative settings, respectively, as they directly provide the model with more context information about the properties. Please refer to Appendix Section D for the comparison results and analysis. All models are trained on H100/A6000/A40 GPUs. Please refer to Appendix Section A for more implementation details.\\n\\nEvaluation Metrics. For relative value comparison, we report the ROC AUC score; for absolute value prediction, we use the Pearson Correlation Coefficient between the prediction and ground truth as this automatically calibrates the predictions to the scale of the ground truth. Please refer to Appendix Section A.4 for more details and motivations on the evaluation metrics.\\n\\nFigure 4: Qualitative results. Top Left: An example for elasticity absolute value prediction; Bottom Left: An example for friction relative value comparison. For each example, the original input video is shown on the left. A static red circle is overlaid in the center to highlight the full trajectory of the object on every frame, shown in the middle. Model predictions are shown on the right, including results from the Video Generative Model (VGM), Video Self-Supervised Model (VSM), and a MLLM (Gemini in this case). For the relative formulation, the ground truth value of ‘1’ indicates that the first (top) video has larger dynamic friction coefficient than the second video. In this example, the initial velocity of the lego brick in the two videos is similar (note the same displacement from frame 0 to 2), but the velocity reduces to 0 at frame 30 in the first video, while the object is still moving in frame 30 to 60 in the second video. Right: Scatter plots of prediction vs ground truth for the elasticity property from the V-JEPA-2 model.\\n\\n5.1 RESULTS FOR RELATIVE VALUE COMPARISON\\n\\nTable 1 (left) shows relative value comparison results across physical properties and model types. The oracle estimator performs nearly perfectly on test-1 and test-2, and strongly on test-3, indicating that the task is largely solvable using visual cues, geometry, and physics. Both generative and self-supervised video models achieve strong results on synthetic splits (test-1 and test-2). Notably, they can also generalize well to the real-world split (test-3) for viscosity and elastic- ity, which rely on simple height ratios and expansion. However, friction proves more challenging. Models trained on synthetic data struggle to generalize, likely due to the fact that reliance on visual references (e.g., ground plane grids) is absent in real videos, and due to friction’s inherent complex- ity involving higher-order motion and projective geometry of the viewpoint. To further confirm, we introduce an additional real-world training split for friction videos with disjoint objects and surfaces from the test set (see Appendix Section B.2 for more details). Fine-tuning the visual prompting architecture on this data improves performance on the real test split, as shown by the * values in Table 1. Multimodal large language models (MLLMs), though not working very well with Baseline Prompt (see Appendix Section D), when prompted properly, also perform well, especially on real videos, which are more in-distribution for them – while on synthetic splits, their performance drops significantly. This is likely due to the fact that the models tend to leverage semantic cues rather than visual motion.\\n\\n5.2 RESULTS FOR ABSOLUTE VALUE PREDICTION\\n\\nTable 1 (right) shows results for absolute value prediction across physical properties and methods. This task is more challenging than relative comparison, as models must regress quantitative physical values rather than compare video pairs from the same viewpoint. Similar to the relative setting, the oracle estimator achieves near-perfect performance on test-1 and test-2, and strong per- formance on test-3, confirming that the task is largely solvable through visual cues, multi-view geometry, and physical laws. We highlight several key observations: (i) comparable performance across backbones. Despite being trained for generative tasks, video generative models perform on par with self-supervised models when predicting dynamic physical properties. (ii) friction remains challenging. Similar to the relative setting, both generative and self-supervised models struggle with friction estimation. Performance again improves with domain adaptation. (iii) MLLMs better on real test split than synthetic. MLLMs continue to perform better on the real test split than syn- thetic test splits, benefiting from their familiarity with real-world visual semantics. (iv) greater gap from oracle. The performance gap between video foundation models and the oracle is more pro- nounced here than in the relative setting, indicating that accurate physical value regression remains a significant challenge for current video models.\\n\\nFigure 4: Qualitative results. Top Left: An example for elasticity absolute value prediction; Bottom Left: An example for friction relative value comparison. For each example, the original input video is shown on the left. A static red circle is overlaid in the center to highlight the full trajectory of the object on every frame, shown in the middle. Model predictions are shown on the right, including results from the Video Generative Model (VGM), Video Self-Supervised Model (VSM), and a MLLM (Gemini in this case). For the relative formulation, the ground truth value of ‘1’ indicates that the first (top) video has larger dynamic friction coefficient than the second video. In this example, the initial velocity of the lego brick in the two videos is similar (note the same displacement from frame 0 to 2), but the velocity reduces to 0 at frame 30 in the first video, while the object is still moving in frame 30 to 60 in the second video. Right: Scatter plots of prediction vs ground truth for the elasticity property from the V-JEPA-2 model.\\n\\n5.1 RESULTS FOR RELATIVE VALUE COMPARISON\\n\\nTable 1 (left) shows relative value comparison results across physical properties and model types. The oracle estimator performs nearly perfectly on test-1 and test-2, and strongly on test-3, indicating that the task is largely solvable using visual cues, geometry, and physics. Both generative and self-supervised video models achieve strong results on synthetic splits (test-1 and test-2). Notably, they can also generalize well to the real-world split (test-3) for viscosity and elastic- ity, which rely on simple height ratios and expansion. However, friction proves more challenging. Models trained on synthetic data struggle to generalize, likely due to the fact that reliance on visual references (e.g., ground plane grids) is absent in real videos, and due to friction’s inherent complex- ity involving higher-order motion and projective geometry of the viewpoint. To further confirm, we introduce an additional real-world training split for friction videos with disjoint objects and surfaces from the test set (see Appendix Section B.2 for more details). Fine-tuning the visual prompting architecture on this data improves performance on the real test split, as shown by the * values in Table 1. Multimodal large language models (MLLMs), though not working very well with Baseline Prompt (see Appendix Section D), when prompted properly, also perform well, especially on real videos, which are more in-distribution for them – while on synthetic splits, their performance drops significantly. This is likely due to the fact that the models tend to leverage semantic cues rather than visual motion.\\n\\n5.2 RESULTS FOR ABSOLUTE VALUE PREDICTION\\n\\nTable 1 (right) shows results for absolute value prediction across physical properties and methods. This task is more challenging than relative comparison, as models must regress quantitative physical values rather than compare video pairs from the same viewpoint. Similar to the relative setting, the oracle estimator achieves near-perfect performance on test-1 and test-2, and strong per- formance on test-3, confirming that the task is largely solvable through visual cues, multi-view geometry, and physical laws. We highlight several key observations: (i) comparable performance across backbones. Despite being trained for generative tasks, video generative models perform on par with self-supervised models when predicting dynamic physical properties. (ii) friction remains challenging. Similar to the relative setting, both generative and self-supervised models struggle with friction estimation. Performance again improves with domain adaptation. (iii) MLLMs better on real test split than synthetic. MLLMs continue to perform better on the real test split than syn- thetic test splits, benefiting from their familiarity with real-world visual semantics. (iv) greater gap from oracle. The performance gap between video foundation models and the oracle is more pro- nounced here than in the relative setting, indicating that accurate physical value regression remains a significant challenge for current video models.\\n\\nTable 1: Results for relative value comparison and absolute value prediction. Left: ROC AUC scores for relative comparisons (range [0,1]). Right: Pearson correlation coefficients for absolute predictions (range [−1,1]). * indicates results after domain adaptation using a disjoint real training set. test-1 is the synthetic in-distribution test split; test-2 is the synthetic out-of-distribution test split; test-3 is the real-world test split.\\n\\nProperty Method Relative – ROC AUC Test-1 Test-2 Test-3 Absolute – Pearson Corr. Test-1 Test-2 Test-3 Oracle 1.00 1.00 1.00 0.99 0.98 0.87 Video Generative Model 1.00 0.98 0.84 0.92 0.82 0.07 Elasticity Video Self-Supervised Model 0.89 0.96 0.77 0.96 0.93 0.47 Qwen2.5VL-max 0.59 0.50 0.54 -0.05 0.11 0.16 GPT-4o 0.51 0.66 0.62 0.19 0.11 0.30 Gemini-2.5-pro 0.64 0.80 0.47 0.04 0.15 0.24 Oracle 0.99 1.00 1.00 0.99 0.98 0.80 Video Generative Model 1.00 1.00 1.00 0.99 0.95 0.76 Viscosity Video Self-Supervised Model 1.00 1.00 0.99 1.00 0.97 0.79 Qwen2.5VL-max 0.64 0.61 0.86 0.16 0.06 0.02 GPT-4o 0.63 0.59 0.99 0.18 0.08 0.55 Gemini-2.5-pro 0.48 0.69 0.95 -0.06 -0.05 0.60 Oracle 1.00 1.00 0.87 0.99 1.00 0.83 Video Generative Model 0.98 0.89 0.47 0.95 0.78 0.21 + Domain Adaptation – – 0.74* – – 0.82* Friction Video Self-Supervised Model 1.00 0.97 0.58 0.71 0.58 0.28 + Domain Adaptation – – 0.63* – – 0.71* Qwen2.5VL-max 0.50 0.62 0.80 0.03 0.14 0.06 GPT-4o 0.34 0.42 0.67 -0.10 0.03 0.38 Gemini-2.5-pro 0.54 0.59 0.97 -0.03 -0.05 0.12\\n\\n5.3 QUALITATIVE RESULTS\\n\\nFigure 4 (left) shows qualitative examples comparing model predictions across different tasks. In the first row, we illustrate an example from the elasticity absolute value prediction task. The video generative model, self-supervised model, and MLLMs predict values of 0.83, 0.96, and 1.0, respec- tively—all reasonably close to the ground-truth value of 0.89. In the second row, we present a friction relative value comparison task. The input consists of two videos, where the first exhibits a higher dynamic friction coefficient than the second. Both the video generative model and the MLLM correctly assign high likelihoods to this relationship (0.82 and 1.0, respectively), aligning with the ground truth. In contrast, the self-supervised model incorrectly predicts the reverse and does so with high confidence. Figure 4 (right) shows examples of the scatter plots for the absolute value predic- tion. More specifically, we show the scatter plots of video self-supervised model on the three test splits. It can be observed that the performance degrades from test-1 to test-3, as test-1 is of the same distribution as the synthetic training split, while test-2 is out-of-distribution synthetic test and test-3 is for real evaluation. We provide more scatter plots in Appendix Section F.\\n\\n6 CONCLUSION\\n\\nWe investigate the task of inferring dynamic physical properties—elasticity, viscosity, and fric- tion—from videos. To support this, we introduce a benchmark dataset with ground-truth annota- tions and evaluate a range of video foundation models under both absolute prediction and relative comparison settings. We adopt a simple architecture to extract physical cues from off-the-shelf gen- erative and self-supervised video models, and explore prompting strategies to elicit predictions from MLLMs. Experiments show that generative and self-supervised models have similar performance. MLLMs perform worse overall but improve with more informative prompting, especially on real- world data. However, all models fall short of the oracle, particularly in absolute value prediction. These results highlight the need to enhance physical reasoning in video models—a key direction for future research.\\n\\nTable 1: Results for relative value comparison and absolute value prediction. Left: ROC AUC scores for relative comparisons (range [0,1]). Right: Pearson correlation coefficients for absolute predictions (range [−1,1]). * indicates results after domain adaptation using a disjoint real training set. test-1 is the synthetic in-distribution test split; test-2 is the synthetic out-of-distribution test split; test-3 is the real-world test split.\\n\\nProperty Method Relative – ROC AUC Test-1 Test-2 Test-3 Absolute – Pearson Corr. Test-1 Test-2 Test-3 Oracle 1.00 1.00 1.00 0.99 0.98 0.87 Video Generative Model 1.00 0.98 0.84 0.92 0.82 0.07 Elasticity Video Self-Supervised Model 0.89 0.96 0.77 0.96 0.93 0.47 Qwen2.5VL-max 0.59 0.50 0.54 -0.05 0.11 0.16 GPT-4o 0.51 0.66 0.62 0.19 0.11 0.30 Gemini-2.5-pro 0.64 0.80 0.47 0.04 0.15 0.24 Oracle 0.99 1.00 1.00 0.99 0.98 0.80 Video Generative Model 1.00 1.00 1.00 0.99 0.95 0.76 Viscosity Video Self-Supervised Model 1.00 1.00 0.99 1.00 0.97 0.79 Qwen2.5VL-max 0.64 0.61 0.86 0.16 0.06 0.02 GPT-4o 0.63 0.59 0.99 0.18 0.08 0.55 Gemini-2.5-pro 0.48 0.69 0.95 -0.06 -0.05 0.60 Oracle 1.00 1.00 0.87 0.99 1.00 0.83 Video Generative Model 0.98 0.89 0.47 0.95 0.78 0.21 + Domain Adaptation – – 0.74* – – 0.82* Friction Video Self-Supervised Model 1.00 0.97 0.58 0.71 0.58 0.28 + Domain Adaptation – – 0.63* – – 0.71* Qwen2.5VL-max 0.50 0.62 0.80 0.03 0.14 0.06 GPT-4o 0.34 0.42 0.67 -0.10 0.03 0.38 Gemini-2.5-pro 0.54 0.59 0.97 -0.03 -0.05 0.12\\n\\n5.3 QUALITATIVE RESULTS\\n\\nFigure 4 (left) shows qualitative examples comparing model predictions across different tasks. In the first row, we illustrate an example from the elasticity absolute value prediction task. The video generative model, self-supervised model, and MLLMs predict values of 0.83, 0.96, and 1.0, respec- tively—all reasonably close to the ground-truth value of 0.89. In the second row, we present a friction relative value comparison task. The input consists of two videos, where the first exhibits a higher dynamic friction coefficient than the second. Both the video generative model and the MLLM correctly assign high likelihoods to this relationship (0.82 and 1.0, respectively), aligning with the ground truth. In contrast, the self-supervised model incorrectly predicts the reverse and does so with high confidence. Figure 4 (right) shows examples of the scatter plots for the absolute value predic- tion. More specifically, we show the scatter plots of video self-supervised model on the three test splits. It can be observed that the performance degrades from test-1 to test-3, as test-1 is of the same distribution as the synthetic training split, while test-2 is out-of-distribution synthetic test and test-3 is for real evaluation. We provide more scatter plots in Appendix Section F.\\n\\n6 CONCLUSION\\n\\nWe investigate the task of inferring dynamic physical properties—elasticity, viscosity, and fric- tion—from videos. To support this, we introduce a benchmark dataset with ground-truth annota- tions and evaluate a range of video foundation models under both absolute prediction and relative comparison settings. We adopt a simple architecture to extract physical cues from off-the-shelf gen- erative and self-supervised video models, and explore prompting strategies to elicit predictions from MLLMs. Experiments show that generative and self-supervised models have similar performance. MLLMs perform worse overall but improve with more informative prompting, especially on real- world data. However, all models fall short of the oracle, particularly in absolute value prediction. These results highlight the need to enhance physical reasoning in video models—a key direction for future research.\\n\\nAcknowledgements. This research is supported by EPSRC Programme Grant VisualAI EP/ T028572/1, a Royal Society Research Professorship RP\\\\R1\\\\191132 and a China Oxford Schol- arship. We thank Minghao Chen, Shuai Chen, Jindong Gu, Jo˜ao Henriques, Zeren Jiang, Shuai Mao, Boyu Pang, Ashish Thandavan, Jianyuan Wang, Junyu Xie, Wen Xiong and Chuanxia Zheng for their help and support for the project.\\n\\nREFERENCES\\n\\nPulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: Experiential learning of intuitive physics. In Adcances on Neural Information Processing Systems (NeurIPS), 2016.\\n\\nJan Assen, Pascal Barla, and Roland Fleming. Visual features in the perception of liquids. Current Biology, 2018.\\n\\nMido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Am- mar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025.\\n\\nMohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn, and Dumitru Erhan. Fitvid: Overfitting in pixel-level video prediction. arXiv preprint arXiv:2106.13195, 2021.\\n\\nAdrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. V-jepa: Latent video prediction for visual representation learning. OpenReview, 2023.\\n\\nDaniel M Bear, Elias Wang, Damian Mrowca, Felix J Binder, Hsiao-Yu Fish Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines. arXiv preprint arXiv:2106.08261, 2021.\\n\\nFlorian Bordes, Quentin Garrido, Justine T Kao, Adina Williams, Michael Rabbat, and Emmanuel Dupoux. Intphys 2: Benchmarking intuitive physics understanding in complex synthetic environ- ments. arXiv preprint arXiv:2506.09849, 2025.\\n\\nWei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Bench- marking and enhancing vision-language models for physical world understanding. International Conference on Learning Representation (ICLR), 2025.\\n\\nGheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capa- bilities. arXiv preprint arXiv:2507.06261, 2025.\\n\\nMingyu Ding, Zhenfang Chen, Tao Du, Ping Luo, Josh Tenenbaum, and Chuang Gan. Dynamic visual reasoning by learning differentiable physics models from video and language. Advances In Neural Information Processing Systems (NeurIPS), 2021.\\n\\nChelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), 2017.\\n\\nKaterina Fragkiadaki, Pulkit Agrawal, Sergey Levine, and Jitendra Malik. Learning visual predictive models of physics for playing billiards. In International Conference on Learning Representations (ICLR), 2016.\\n\\nJensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded vision-language models for robotic manipulation. In Interna- tional Conference on Robotics and Automation (ICRA), 2024.\\n\\nAlejandro Casta˜neda Garcia, Jan Warchocki, Jan van Gemert, Daan Brinks, and Nergis Tomen. Learning physics from video: Unsupervised physical parameter estimation for continuous dy- namical systems. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), 2025.\\n\\nAcknowledgements. This research is supported by EPSRC Programme Grant VisualAI EP/ T028572/1, a Royal Society Research Professorship RP\\\\R1\\\\191132 and a China Oxford Schol- arship. We thank Minghao Chen, Shuai Chen, Jindong Gu, Jo˜ao Henriques, Zeren Jiang, Shuai Mao, Boyu Pang, Ashish Thandavan, Jianyuan Wang, Junyu Xie, Wen Xiong and Chuanxia Zheng for their help and support for the project.\\n\\nREFERENCES\\n\\nPulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: Experiential learning of intuitive physics. In Adcances on Neural Information Processing Systems (NeurIPS), 2016.\\n\\nJan Assen, Pascal Barla, and Roland Fleming. Visual features in the perception of liquids. Current Biology, 2018.\\n\\nMido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Am- mar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025.\\n\\nMohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn, and Dumitru Erhan. Fitvid: Overfitting in pixel-level video prediction. arXiv preprint arXiv:2106.13195, 2021.\\n\\nAdrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. V-jepa: Latent video prediction for visual representation learning. OpenReview, 2023.\\n\\nDaniel M Bear, Elias Wang, Damian Mrowca, Felix J Binder, Hsiao-Yu Fish Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines. arXiv preprint arXiv:2106.08261, 2021.\\n\\nFlorian Bordes, Quentin Garrido, Justine T Kao, Adina Williams, Michael Rabbat, and Emmanuel Dupoux. Intphys 2: Benchmarking intuitive physics understanding in complex synthetic environ- ments. arXiv preprint arXiv:2506.09849, 2025.\\n\\nWei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Bench- marking and enhancing vision-language models for physical world understanding. International Conference on Learning Representation (ICLR), 2025.\\n\\nGheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capa- bilities. arXiv preprint arXiv:2507.06261, 2025.\\n\\nMingyu Ding, Zhenfang Chen, Tao Du, Ping Luo, Josh Tenenbaum, and Chuang Gan. Dynamic visual reasoning by learning differentiable physics models from video and language. Advances In Neural Information Processing Systems (NeurIPS), 2021.\\n\\nChelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), 2017.\\n\\nKaterina Fragkiadaki, Pulkit Agrawal, Sergey Levine, and Jitendra Malik. Learning visual predictive models of physics for playing billiards. In International Conference on Learning Representations (ICLR), 2016.\\n\\nJensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded vision-language models for robotic manipulation. In Interna- tional Conference on Robotics and Automation (ICRA), 2024.\\n\\nAlejandro Casta˜neda Garcia, Jan Warchocki, Jan van Gemert, Daan Brinks, and Nergis Tomen. Learning physics from video: Unsupervised physical parameter estimation for continuous dy- namical systems. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), 2025.\\n\\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning (ICML), 2019.\\n\\nRichard I. Hartley and Andrew Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, ISBN: 0521540518, second edition, 2004.\\n\\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024.\\n\\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os- trow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.\\n\\nKrishna Murthy Jatavallabhula, Miles Macklin, Florian Golemo, Vikram Voleti, Linda Petrini, Mar- tin Weiss, Breandan Considine, Jerome Parent-Levesque, Kevin Xie, Kenny Erleben, Liam Paull, Florian Shkurti, Derek Nowrouzezahrai, and Sanja Fidler. gradsim: Differentiable simulation for system identification and visuomotor control. In International Conference on Learning Repre- sentations (ICLR), 2021.\\n\\nTakahiro Kawabe and Shin’ya Nishida. Seeing jelly: Judging elasticity of a transparent object. In Proceedings of the ACM Symposium on Applied Perception, 2016.\\n\\nTakahiro Kawabe, Kazushi Maruya, Roland Fleming, and Shin’ya Nishida. Seeing liquids from visual motion. Vision Research, 2014.\\n\\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll´ar, and Ross Girshick. Segment anything. Proceedings of the International Conference on Computer Vision (ICCV), 2023.\\n\\nYunzhu Li, Toru Lin, Kexin Yi, Daniel Bear, Daniel L.K. Yamins, Jiajun Wu, Joshua B. Tenenbaum, and Antonio Torralba. Visual grounding of learned physical models. In International Conference on Machine Learning (ICML), 2020.\\n\\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision (ECCV), 2024a.\\n\\nYixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024b.\\n\\nHaoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt: An empirical study on video diffusion with transformers. arXiv preprint arXiv:2305.13311, 2023.\\n\\nJ Norman, Elizabeth Wiesemann, Hideko Norman, M Taylor, and Warren Craft. The visual discrim- ination of bending. Perception, 2007.\\n\\nVivian Paulun, Takahiro Kawabe, Shin’ya Nishida, and Roland Fleming. Seeing liquids from static snapshots. Vision research, 2015.\\n\\nVivian Paulun, Filipp Schmidt, Jan Assen, and Roland Fleming. Shape, motion, and optical cues to stiffness of elastic objects. Journal of Vision, 2017.\\n\\nVivian C. Paulun and Roland W. Fleming. Visually inferring elasticity from the motion trajectory of bouncing cubes. Journal of Vision, 2020.\\n\\nNikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R¨adle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Va- sudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Doll´ar, and Christoph Fe- ichtenhofer. Sam 2: Segment anything in images and videos. In International Conference on Learning Representations (ICLR), 2025.\\n\\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning (ICML), 2019.\\n\\nRichard I. Hartley and Andrew Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, ISBN: 0521540518, second edition, 2004.\\n\\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024.\\n\\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os- trow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.\\n\\nKrishna Murthy Jatavallabhula, Miles Macklin, Florian Golemo, Vikram Voleti, Linda Petrini, Mar- tin Weiss, Breandan Considine, Jerome Parent-Levesque, Kevin Xie, Kenny Erleben, Liam Paull, Florian Shkurti, Derek Nowrouzezahrai, and Sanja Fidler. gradsim: Differentiable simulation for system identification and visuomotor control. In International Conference on Learning Repre- sentations (ICLR), 2021.\\n\\nTakahiro Kawabe and Shin’ya Nishida. Seeing jelly: Judging elasticity of a transparent object. In Proceedings of the ACM Symposium on Applied Perception, 2016.\\n\\nTakahiro Kawabe, Kazushi Maruya, Roland Fleming, and Shin’ya Nishida. Seeing liquids from visual motion. Vision Research, 2014.\\n\\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll´ar, and Ross Girshick. Segment anything. Proceedings of the International Conference on Computer Vision (ICCV), 2023.\\n\\nYunzhu Li, Toru Lin, Kexin Yi, Daniel Bear, Daniel L.K. Yamins, Jiajun Wu, Joshua B. Tenenbaum, and Antonio Torralba. Visual grounding of learned physical models. In International Conference on Machine Learning (ICML), 2020.\\n\\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision (ECCV), 2024a.\\n\\nYixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024b.\\n\\nHaoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt: An empirical study on video diffusion with transformers. arXiv preprint arXiv:2305.13311, 2023.\\n\\nJ Norman, Elizabeth Wiesemann, Hideko Norman, M Taylor, and Warren Craft. The visual discrim- ination of bending. Perception, 2007.\\n\\nVivian Paulun, Takahiro Kawabe, Shin’ya Nishida, and Roland Fleming. Seeing liquids from static snapshots. Vision research, 2015.\\n\\nVivian Paulun, Filipp Schmidt, Jan Assen, and Roland Fleming. Shape, motion, and optical cues to stiffness of elastic objects. Journal of Vision, 2017.\\n\\nVivian C. Paulun and Roland W. Fleming. Visually inferring elasticity from the motion trajectory of bouncing cubes. Journal of Vision, 2020.\\n\\nNikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R¨adle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Va- sudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Doll´ar, and Christoph Fe- ichtenhofer. Sam 2: Segment anything in images and videos. In International Conference on Learning Representations (ICLR), 2025.\\n\\nTianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, Yuda Xiong, Hao Zhang, Feng Li, Peijun Tang, Kent Yu, and Lei Zhang. Grounding dino 1.5: Advance the ”edge” of open-set object detection. arXiv preprint arXiv:2405.10300, 2024a.\\n\\nTianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024b.\\n\\nRonan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, V´eronique Izard, and Emmanuel Dupoux. Intphys: A framework and benchmark for visual intuitive physics reasoning. arXiv preprint arXiv:1803.07616, 2018.\\n\\nPrafull Sharma, Julien Philip, Micha¨el Gharbi, Bill Freeman, Fredo Durand, and Valentin Deschain- tre. Materialistic: Selecting similar materials in images. ACM Transactions on Graphics (TOG), 2023.\\n\\nHui Shen, Taiqiang Wu, Qi Han, Yunta Hsieh, Jizhou Wang, Yuyue Zhang, Yuxin Cheng, Zijian Hao, Yuansheng Ni, Xin Wang, et al. Phyx: Does your model have the” wits” for physical reasoning? arXiv preprint arXiv:2505.15929, 2025.\\n\\nAleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about a red circle? visual prompt engineering for vlms. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\\n\\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and sup- port inference from rgbd images. In European Conference on Computer Vision (ECCV), 2012.\\n\\nLuming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emer- gent correspondence from image diffusion. Advances in Neural Information Processing Systems (NeurIPS), 2023.\\n\\nHsiao-Yu Tung, Mingyu Ding, Zhenfang Chen, Daniel Bear, Chuang Gan, Josh Tenenbaum, Dan Yamins, Judith Fan, and Kevin Smith. Physion++: Evaluating physical scene understanding that requires online inference of different physical properties. Advances in Neural Information Processing Systems (NeurIPS), 2023.\\n\\nVikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Masked conditional video diffusion for prediction, generation, and interpolation. arXiv preprint arXiv:2205.09853, 2022.\\n\\nBin Wang, Paul Kry, Yuanmin Deng, Uri Ascher, Hui Huang, and Baoquan Chen. Neural mate- rial: Learning elastic constitutive material and damping models from sparse data. arXiv preprint arXiv:1808.04931, 2018.\\n\\nKun Wang, Mridul Aanjaneya, and Kostas Bekris. A first principles approach for data-efficient system identification of spring-rod systems via differentiable physics engines. In Learning for Dynamics and Control, 2020a.\\n\\nTianyu Wang, Xiaowei Hu, Qiong Wang, Pheng-Ann Heng, and Chi-Wing Fu. Instance shadow detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020b.\\n\\nTianyu Wang, Xiaowei Hu, Chi-Wing Fu, and Pheng-Ann Heng. Single-stage instance shadow de- tection with bidirectional relation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nWikipedia contributors. Coefficient of restitution — wikipedia, the free encyclopedia, 2025a. URL https://en.wikipedia.org/wiki/Coefficient_of_restitution.\\n\\nWikipedia contributors. Viscosity — wikipedia, the free encyclopedia, 2025b. URL https:// en.wikipedia.org/wiki/Viscosity.\\n\\nTianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, Yuda Xiong, Hao Zhang, Feng Li, Peijun Tang, Kent Yu, and Lei Zhang. Grounding dino 1.5: Advance the ”edge” of open-set object detection. arXiv preprint arXiv:2405.10300, 2024a.\\n\\nTianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024b.\\n\\nRonan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, V´eronique Izard, and Emmanuel Dupoux. Intphys: A framework and benchmark for visual intuitive physics reasoning. arXiv preprint arXiv:1803.07616, 2018.\\n\\nPrafull Sharma, Julien Philip, Micha¨el Gharbi, Bill Freeman, Fredo Durand, and Valentin Deschain- tre. Materialistic: Selecting similar materials in images. ACM Transactions on Graphics (TOG), 2023.\\n\\nHui Shen, Taiqiang Wu, Qi Han, Yunta Hsieh, Jizhou Wang, Yuyue Zhang, Yuxin Cheng, Zijian Hao, Yuansheng Ni, Xin Wang, et al. Phyx: Does your model have the” wits” for physical reasoning? arXiv preprint arXiv:2505.15929, 2025.\\n\\nAleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about a red circle? visual prompt engineering for vlms. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\\n\\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and sup- port inference from rgbd images. In European Conference on Computer Vision (ECCV), 2012.\\n\\nLuming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emer- gent correspondence from image diffusion. Advances in Neural Information Processing Systems (NeurIPS), 2023.\\n\\nHsiao-Yu Tung, Mingyu Ding, Zhenfang Chen, Daniel Bear, Chuang Gan, Josh Tenenbaum, Dan Yamins, Judith Fan, and Kevin Smith. Physion++: Evaluating physical scene understanding that requires online inference of different physical properties. Advances in Neural Information Processing Systems (NeurIPS), 2023.\\n\\nVikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Masked conditional video diffusion for prediction, generation, and interpolation. arXiv preprint arXiv:2205.09853, 2022.\\n\\nBin Wang, Paul Kry, Yuanmin Deng, Uri Ascher, Hui Huang, and Baoquan Chen. Neural mate- rial: Learning elastic constitutive material and damping models from sparse data. arXiv preprint arXiv:1808.04931, 2018.\\n\\nKun Wang, Mridul Aanjaneya, and Kostas Bekris. A first principles approach for data-efficient system identification of spring-rod systems via differentiable physics engines. In Learning for Dynamics and Control, 2020a.\\n\\nTianyu Wang, Xiaowei Hu, Qiong Wang, Pheng-Ann Heng, and Chi-Wing Fu. Instance shadow detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020b.\\n\\nTianyu Wang, Xiaowei Hu, Chi-Wing Fu, and Pheng-Ann Heng. Single-stage instance shadow de- tection with bidirectional relation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nWikipedia contributors. Coefficient of restitution — wikipedia, the free encyclopedia, 2025a. URL https://en.wikipedia.org/wiki/Coefficient_of_restitution.\\n\\nWikipedia contributors. Viscosity — wikipedia, the free encyclopedia, 2025b. URL https:// en.wikipedia.org/wiki/Viscosity.\\n\\nWikipedia contributors. Wetting — wikipedia, the free encyclopedia, 2025c. URL https://en. wikipedia.org/wiki/Wetting.\\n\\nJiajun Wu, Ilker Yildirim, Joseph J Lim, Bill Freeman, and Josh Tenenbaum. Galileo: Perceiving physical object properties by integrating a physics engine with deep learning. Advances in neural information processing systems (NeurIPS), 2015.\\n\\nJiajun Wu, Joseph J Lim, Hongyi Zhang, Joshua B Tenenbaum, and William T Freeman. Physics 101: Learning physical object properties from unlabeled videos. In British Machine Vision Con- ference (BMVC), 2016.\\n\\nJinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision (ECCV), 2024.\\n\\nGuanqi Zhan, Weidi Xie, and Andrew Zisserman. A tri-layer plugin to improve occluded detection. British Machine Vision Conference (BMVC), 2022.\\n\\nGuanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zisserman. Amodal ground truth and completion in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024a.\\n\\nGuanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zisserman. A general protocol to probe large vision models for 3d physical understanding. Advances in Neural Information Processing Systems (NeurIPS), 2024b.\\n\\nXian Zhou, Yiling Qiao, Zhenjia Xu, Tsun-Hsuan Wang, Zhehuan Chen, Juntian Zheng, Ziyan Xiong, Yian Wang, Mingrui Zhang, Pingchuan Ma, Yufei Wang, Zhiyang Dou, Byungchul Kim, Yunsheng Tian, Yipu Chen, Xiaowen Qiu, Chunru Lin, Tairan He, Zilin Si, Yunchu Zhang, Zhan- lue Yang, Tiantian Liu, Tianyu Li, Kashu Yamazaki, Hongxin Zhang, Huy Ha, Yu Zhang, Michael Liu, Shaokun Zheng, Zipeng Fu, Qi Wu, Yiran Geng, Feng Chen, Milky, Yuanming Hu, Guanya Shi, Lingjie Liu, Taku Komura, Zackory Erickson, David Held, Minchen Li, Linxi ”Jim” Fan, Yuke Zhu, Wojciech Matusik, Dan Gutfreund, Shuran Song, Daniela Rus, Ming Lin, Bo Zhu, Katerina Fragkiadaki, and Chuang Gan. Genesis: A universal and generative physics engine for robotics and beyond, 2024. URL https://github.com/Genesis-Embodied-AI/ Genesis.\\n\\nWikipedia contributors. Wetting — wikipedia, the free encyclopedia, 2025c. URL https://en. wikipedia.org/wiki/Wetting.\\n\\nJiajun Wu, Ilker Yildirim, Joseph J Lim, Bill Freeman, and Josh Tenenbaum. Galileo: Perceiving physical object properties by integrating a physics engine with deep learning. Advances in neural information processing systems (NeurIPS), 2015.\\n\\nJiajun Wu, Joseph J Lim, Hongyi Zhang, Joshua B Tenenbaum, and William T Freeman. Physics 101: Learning physical object properties from unlabeled videos. In British Machine Vision Con- ference (BMVC), 2016.\\n\\nJinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision (ECCV), 2024.\\n\\nGuanqi Zhan, Weidi Xie, and Andrew Zisserman. A tri-layer plugin to improve occluded detection. British Machine Vision Conference (BMVC), 2022.\\n\\nGuanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zisserman. Amodal ground truth and completion in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024a.\\n\\nGuanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zisserman. A ge', 'text_length': 100000, 'message': 'Retrieved 76 pages for 2510v02311v1.pdf'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `generate_summary` with `The paper explores the task of predicting dynamic physical properties from videos, focusing on properties like elasticity, viscosity, and dynamic friction that require temporal information. It introduces PhysVid, a new dataset comprising synthetic and real-world videos annotated with these properties. The study evaluates three approaches: an oracle method using classical computer vision techniques, a visual prompt mechanism for generative and self-supervised video models, and prompting strategies for multi-modal large language models (MLLMs). Results show that generative and self-supervised models perform similarly, though below the oracle's accuracy, while MLLMs lag behind but improve with better prompting. The paper highlights the potential of video foundation models in understanding dynamic physical properties and suggests further research to enhance their physical reasoning capabilities.`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m{'status': 'success', 'summary': \"The paper introduces PhysVid, a novel dataset of synthetic and real-world videos annotated with dynamic physical properties such as elasticity, viscosity, and dynamic friction, requiring temporal information for accurate prediction. It evaluates three approaches: an oracle method using classical computer vision techniques, visual prompts for generative and self-supervised video models, and prompting strategies for multi-modal large language models (MLLMs). While generative and self-supervised models show similar performance, they fall short of the oracle's accuracy; MLLMs perform worse but show potential for improvement with enhanced prompting. The study underscores the capability of video foundation models to understand dynamic physical properties and calls for further research to boost their physical reasoning abilities. This work represents a significant advancement in AI's ability to interpret complex physical interactions from video data, with potential applications in fields requiring detailed physical property analysis, such as robotics, material science, and autonomous systems.\", 'length': 1102, 'message': 'Generated summary (1102 characters)'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `collapse_summaries` with `[{\"summary\":\"The paper introduces a novel method called Laplacian Eigenvector Gradient Orthogonalization (LEGO) for estimating tangent spaces on data manifolds, addressing the limitations of traditional methods like Local Principal Component Analysis (LPCA) in noisy environments. LEGO utilizes graph Laplacian eigenvectors to leverage the global structure of data, orthogonalizing gradients of low-frequency eigenvectors to enhance the accuracy of tangent space estimates. Theoretical analyses confirm LEGO's robustness against noise, supported by differential geometry and random matrix theory. Empirical results show that LEGO outperforms LPCA in manifold learning, boundary detection, and local intrinsic dimension estimation. This advancement is significant for AI and data science professionals, offering a more reliable method for geometric data analysis in noisy conditions. Consequently, LEGO has the potential to improve machine learning applications where accurate data manifold estimation is crucial, impacting industries reliant on precise data analysis.\"}, {\"summary\":\"The paper presents KAVA, a groundbreaking framework for latent reasoning in large language models (LLMs) that utilizes compressed Key-Value (KV) cache distillation. This approach addresses the high computational cost of traditional chain-of-thought (CoT) reasoning by offering an efficient alternative without sacrificing accuracy. KAVA achieves this by transferring knowledge from a teacher model's compressed KV-cache to a latent reasoning student, aligning stepwise KV trajectories with continuous latent tokens. The framework not only maintains the accuracy of CoT-trained models but also enhances efficiency and scalability. KAVA demonstrates superior performance compared to existing latent reasoning methods, with minimal performance loss when shifting from equation-only to natural-language traces. It effectively scales to larger models, providing a scalable supervision signal that combines CoT accuracy with latent inference efficiency. This advancement is crucial for AI professionals aiming to develop efficient and deployable reasoning models, impacting practical applications and industry by offering a more scalable and efficient reasoning framework.\"}, {\"summary\":\"This blog post by Daniel Voigt Godoy outlines a practical approach to fine-tuning a large language model (LLM) using PyTorch and Hugging Face tools, focusing on Microsoft's Phi-3 Mini 4K Instruct model for translating English into Yoda-speak. Notable advancements include the use of quantized models to reduce memory usage and low-rank adapters (LoRA) to minimize trainable parameters, enhancing efficiency. Performance improvements are achieved through Hugging Face's SFTTrainer for supervised fine-tuning, emphasizing the importance of dataset formatting and tokenizer configuration for conversational AI models. The process involves converting datasets to a conversational format and aligning the tokenizer with the model's training, optimizing memory and configuration settings. The fine-tuned model successfully generates Yoda-like sentences, showcasing the effectiveness of the approach. Practical applications include the potential for broader use in conversational AI, with the model being made accessible via Hugging Face Hub. This demonstrates significant industry impact by facilitating the development and sharing of specialized language models.\"}, {\"summary\":\"The paper presents NoiseShift, a novel training-free technique designed to enhance low-resolution image generation in text-to-image diffusion models. These models typically face challenges in producing high-quality images at resolutions not encountered during training, especially lower ones. NoiseShift recalibrates the denoiser's noise level based on the resolution size, without modifying the model architecture or sampling schedule, addressing the perceptual mismatch from noise schedulers. This method significantly improves image quality at low resolutions, as evidenced by better FID scores on datasets like LAION-COCO and CelebA, using models such as Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev. The approach is lightweight, requiring no retraining, and effectively reduces resolution-dependent artifacts. This makes NoiseShift a practical solution for enhancing the adaptability and efficiency of diffusion models in generating low-resolution images, with potential industry impact in applications requiring high-quality image generation at varying resolutions.\"}, {\"summary\":\"The paper introduces PhysVid, a novel dataset of synthetic and real-world videos annotated with dynamic physical properties such as elasticity, viscosity, and dynamic friction, requiring temporal information for accurate prediction. It evaluates three approaches: an oracle method using classical computer vision techniques, visual prompts for generative and self-supervised video models, and prompting strategies for multi-modal large language models (MLLMs). While generative and self-supervised models show similar performance, they fall short of the oracle's accuracy; MLLMs perform worse but show potential for improvement with enhanced prompting. The study underscores the capability of video foundation models to understand dynamic physical properties and calls for further research to boost their physical reasoning abilities. This work represents a significant advancement in AI's ability to interpret complex physical interactions from video data, with potential applications in fields requiring detailed physical property analysis, such as robotics, material science, and autonomous systems.\"} ]`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m{'status': 'success', 'collapsed_summaries': [{'summary': \"The paper introduces a novel method called Laplacian Eigenvector Gradient Orthogonalization (LEGO) for estimating tangent spaces on data manifolds, addressing the limitations of traditional methods like Local Principal Component Analysis (LPCA) in noisy environments. LEGO utilizes graph Laplacian eigenvectors to leverage the global structure of data, orthogonalizing gradients of low-frequency eigenvectors to enhance the accuracy of tangent space estimates. Theoretical analyses confirm LEGO's robustness against noise, supported by differential geometry and random matrix theory. Empirical results show that LEGO outperforms LPCA in manifold learning, boundary detection, and local intrinsic dimension estimation. This advancement is significant for AI and data science professionals, offering a more reliable method for geometric data analysis in noisy conditions. Consequently, LEGO has the potential to improve machine learning applications where accurate data manifold estimation is crucial, impacting industries reliant on precise data analysis.\"}, {'summary': \"The paper presents KAVA, a groundbreaking framework for latent reasoning in large language models (LLMs) that utilizes compressed Key-Value (KV) cache distillation. This approach addresses the high computational cost of traditional chain-of-thought (CoT) reasoning by offering an efficient alternative without sacrificing accuracy. KAVA achieves this by transferring knowledge from a teacher model's compressed KV-cache to a latent reasoning student, aligning stepwise KV trajectories with continuous latent tokens. The framework not only maintains the accuracy of CoT-trained models but also enhances efficiency and scalability. KAVA demonstrates superior performance compared to existing latent reasoning methods, with minimal performance loss when shifting from equation-only to natural-language traces. It effectively scales to larger models, providing a scalable supervision signal that combines CoT accuracy with latent inference efficiency. This advancement is crucial for AI professionals aiming to develop efficient and deployable reasoning models, impacting practical applications and industry by offering a more scalable and efficient reasoning framework.\"}, {'summary': \"This blog post by Daniel Voigt Godoy outlines a practical approach to fine-tuning a large language model (LLM) using PyTorch and Hugging Face tools, focusing on Microsoft's Phi-3 Mini 4K Instruct model for translating English into Yoda-speak. Notable advancements include the use of quantized models to reduce memory usage and low-rank adapters (LoRA) to minimize trainable parameters, enhancing efficiency. Performance improvements are achieved through Hugging Face's SFTTrainer for supervised fine-tuning, emphasizing the importance of dataset formatting and tokenizer configuration for conversational AI models. The process involves converting datasets to a conversational format and aligning the tokenizer with the model's training, optimizing memory and configuration settings. The fine-tuned model successfully generates Yoda-like sentences, showcasing the effectiveness of the approach. Practical applications include the potential for broader use in conversational AI, with the model being made accessible via Hugging Face Hub. This demonstrates significant industry impact by facilitating the development and sharing of specialized language models.\"}, {'summary': \"The paper presents NoiseShift, a novel training-free technique designed to enhance low-resolution image generation in text-to-image diffusion models. These models typically face challenges in producing high-quality images at resolutions not encountered during training, especially lower ones. NoiseShift recalibrates the denoiser's noise level based on the resolution size, without modifying the model architecture or sampling schedule, addressing the perceptual mismatch from noise schedulers. This method significantly improves image quality at low resolutions, as evidenced by better FID scores on datasets like LAION-COCO and CelebA, using models such as Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev. The approach is lightweight, requiring no retraining, and effectively reduces resolution-dependent artifacts. This makes NoiseShift a practical solution for enhancing the adaptability and efficiency of diffusion models in generating low-resolution images, with potential industry impact in applications requiring high-quality image generation at varying resolutions.\"}, {'summary': \"The paper introduces PhysVid, a novel dataset of synthetic and real-world videos annotated with dynamic physical properties such as elasticity, viscosity, and dynamic friction, requiring temporal information for accurate prediction. It evaluates three approaches: an oracle method using classical computer vision techniques, visual prompts for generative and self-supervised video models, and prompting strategies for multi-modal large language models (MLLMs). While generative and self-supervised models show similar performance, they fall short of the oracle's accuracy; MLLMs perform worse but show potential for improvement with enhanced prompting. The study underscores the capability of video foundation models to understand dynamic physical properties and calls for further research to boost their physical reasoning abilities. This work represents a significant advancement in AI's ability to interpret complex physical interactions from video data, with potential applications in fields requiring detailed physical property analysis, such as robotics, material science, and autonomous systems.\"}], 'message': 'Summaries already fit in context (1,391 tokens). No collapse needed.'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `generate_brief` with `[{\"summary\":\"The paper introduces a novel method called Laplacian Eigenvector Gradient Orthogonalization (LEGO) for estimating tangent spaces on data manifolds, addressing the limitations of traditional methods like Local Principal Component Analysis (LPCA) in noisy environments. LEGO utilizes graph Laplacian eigenvectors to leverage the global structure of data, orthogonalizing gradients of low-frequency eigenvectors to enhance the accuracy of tangent space estimates. Theoretical analyses confirm LEGO's robustness against noise, supported by differential geometry and random matrix theory. Empirical results show that LEGO outperforms LPCA in manifold learning, boundary detection, and local intrinsic dimension estimation. This advancement is significant for AI and data science professionals, offering a more reliable method for geometric data analysis in noisy conditions. Consequently, LEGO has the potential to improve machine learning applications where accurate data manifold estimation is crucial, impacting industries reliant on precise data analysis.\"}, {\"summary\":\"The paper presents KAVA, a groundbreaking framework for latent reasoning in large language models (LLMs) that utilizes compressed Key-Value (KV) cache distillation. This approach addresses the high computational cost of traditional chain-of-thought (CoT) reasoning by offering an efficient alternative without sacrificing accuracy. KAVA achieves this by transferring knowledge from a teacher model's compressed KV-cache to a latent reasoning student, aligning stepwise KV trajectories with continuous latent tokens. The framework not only maintains the accuracy of CoT-trained models but also enhances efficiency and scalability. KAVA demonstrates superior performance compared to existing latent reasoning methods, with minimal performance loss when shifting from equation-only to natural-language traces. It effectively scales to larger models, providing a scalable supervision signal that combines CoT accuracy with latent inference efficiency. This advancement is crucial for AI professionals aiming to develop efficient and deployable reasoning models, impacting practical applications and industry by offering a more scalable and efficient reasoning framework.\"}, {\"summary\":\"This blog post by Daniel Voigt Godoy outlines a practical approach to fine-tuning a large language model (LLM) using PyTorch and Hugging Face tools, focusing on Microsoft's Phi-3 Mini 4K Instruct model for translating English into Yoda-speak. Notable advancements include the use of quantized models to reduce memory usage and low-rank adapters (LoRA) to minimize trainable parameters, enhancing efficiency. Performance improvements are achieved through Hugging Face's SFTTrainer for supervised fine-tuning, emphasizing the importance of dataset formatting and tokenizer configuration for conversational AI models. The process involves converting datasets to a conversational format and aligning the tokenizer with the model's training, optimizing memory and configuration settings. The fine-tuned model successfully generates Yoda-like sentences, showcasing the effectiveness of the approach. Practical applications include the potential for broader use in conversational AI, with the model being made accessible via Hugging Face Hub. This demonstrates significant industry impact by facilitating the development and sharing of specialized language models.\"}, {\"summary\":\"The paper presents NoiseShift, a novel training-free technique designed to enhance low-resolution image generation in text-to-image diffusion models. These models typically face challenges in producing high-quality images at resolutions not encountered during training, especially lower ones. NoiseShift recalibrates the denoiser's noise level based on the resolution size, without modifying the model architecture or sampling schedule, addressing the perceptual mismatch from noise schedulers. This method significantly improves image quality at low resolutions, as evidenced by better FID scores on datasets like LAION-COCO and CelebA, using models such as Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev. The approach is lightweight, requiring no retraining, and effectively reduces resolution-dependent artifacts. This makes NoiseShift a practical solution for enhancing the adaptability and efficiency of diffusion models in generating low-resolution images, with potential industry impact in applications requiring high-quality image generation at varying resolutions.\"}, {\"summary\":\"The paper introduces PhysVid, a novel dataset of synthetic and real-world videos annotated with dynamic physical properties such as elasticity, viscosity, and dynamic friction, requiring temporal information for accurate prediction. It evaluates three approaches: an oracle method using classical computer vision techniques, visual prompts for generative and self-supervised video models, and prompting strategies for multi-modal large language models (MLLMs). While generative and self-supervised models show similar performance, they fall short of the oracle's accuracy; MLLMs perform worse but show potential for improvement with enhanced prompting. The study underscores the capability of video foundation models to understand dynamic physical properties and calls for further research to boost their physical reasoning abilities. This work represents a significant advancement in AI's ability to interpret complex physical interactions from video data, with potential applications in fields requiring detailed physical property analysis, such as robotics, material science, and autonomous systems.\"} ]`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m{'status': 'success', 'brief': \"**Executive Summary: AI Industry Developments and Trends**\\n\\n**Introduction**\\n\\nAs the AI industry continues to evolve at a rapid pace, several significant developments have emerged, each with profound implications for various sectors. This executive summary highlights the most impactful advancements, their practical applications, and key performance milestones. It also synthesizes overarching trends that are shaping the future of AI, providing C-suite executives with a strategic overview of the current landscape.\\n\\n**Significant Industry Developments**\\n\\n1. **Laplacian Eigenvector Gradient Orthogonalization (LEGO):** This novel method addresses the limitations of traditional techniques in estimating tangent spaces on data manifolds, particularly in noisy environments. By leveraging graph Laplacian eigenvectors, LEGO enhances the accuracy of geometric data analysis, which is crucial for machine learning applications. This development is particularly relevant for industries that rely on precise data analysis, such as finance, healthcare, and autonomous systems.\\n\\n2. **KAVA Framework for Latent Reasoning in LLMs:** KAVA introduces a more efficient approach to latent reasoning in large language models by utilizing compressed Key-Value cache distillation. This framework maintains the accuracy of chain-of-thought reasoning while enhancing scalability and efficiency. The implications of KAVA are significant for AI professionals developing deployable reasoning models, impacting sectors like customer service automation, legal tech, and education.\\n\\n3. **Fine-Tuning LLMs with PyTorch and Hugging Face:** The practical approach outlined by Daniel Voigt Godoy demonstrates the use of quantized models and low-rank adapters to optimize memory usage and reduce trainable parameters. This advancement facilitates the development of specialized language models, with potential applications in conversational AI and customer interaction platforms.\\n\\n4. **NoiseShift for Low-Resolution Image Generation:** NoiseShift is a training-free technique that improves image quality in text-to-image diffusion models at low resolutions. By recalibrating the denoiser's noise level, this method enhances the adaptability and efficiency of diffusion models, with potential applications in media, entertainment, and e-commerce sectors where high-quality image generation is essential.\\n\\n5. **PhysVid Dataset for Dynamic Physical Properties:** The introduction of PhysVid, a dataset annotated with dynamic physical properties, represents a significant advancement in AI's ability to interpret complex physical interactions from video data. This development has potential applications in robotics, material science, and autonomous systems, where understanding dynamic physical properties is crucial.\\n\\n**Practical Applications**\\n\\nThe practical applications of these developments are vast and varied. LEGO's robust method for geometric data analysis can enhance predictive analytics in finance and healthcare. KAVA's efficient reasoning framework can improve the performance of AI-driven customer service platforms, providing faster and more accurate responses. The fine-tuning of LLMs using PyTorch and Hugging Face tools enables the creation of more engaging conversational AI systems, enhancing user experience in customer interaction. NoiseShift's ability to generate high-quality images at low resolutions can revolutionize content creation in media and advertising. Finally, PhysVid's insights into dynamic physical properties can advance the development of more sophisticated robotics and autonomous systems.\\n\\n**Key Performance Milestones**\\n\\nSeveral key performance milestones have been achieved across these developments. LEGO has demonstrated superior performance in manifold learning and boundary detection, outperforming traditional methods. KAVA has shown that it can maintain accuracy while significantly reducing computational costs, making it a scalable solution for large language models. The fine-tuning approach for LLMs has successfully optimized memory usage, allowing for the efficient deployment of specialized models. NoiseShift has achieved better FID scores, indicating improved image quality in diffusion models. PhysVid has provided a benchmark for evaluating AI's ability to understand dynamic physical properties, setting the stage for further advancements in this area.\\n\\n**Synthesis of Trends**\\n\\nThe overarching trend across these developments is the increasing emphasis on efficiency and scalability in AI models. As AI applications become more integrated into various industries, the demand for models that can perform complex tasks with minimal computational resources is growing. Additionally, there is a clear focus on enhancing the accuracy and reliability of AI systems, particularly in noisy or complex environments. This trend is driving innovation in methods like LEGO and KAVA, which prioritize robustness and efficiency. Furthermore, the development of datasets like PhysVid highlights the industry's commitment to advancing AI's understanding of real-world interactions, paving the way for more sophisticated applications in robotics and autonomous systems.\\n\\n**Conclusion**\\n\\nThe AI industry is witnessing significant advancements that are transforming the capabilities of AI models and their applications across various sectors. These developments underscore the importance of efficiency, scalability, and accuracy in AI systems, setting the stage for continued innovation and growth. As these trends continue to evolve, they will undoubtedly shape the future of AI, offering new opportunities and challenges for businesses and industries worldwide.\", 'word_count': 745, 'message': 'Generated executive brief (745 words)'}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m**Executive Summary: AI Industry Developments and Trends**\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "As the AI industry continues to evolve at a rapid pace, several significant developments have emerged, each with profound implications for various sectors. This executive summary highlights the most impactful advancements, their practical applications, and key performance milestones. It also synthesizes overarching trends that are shaping the future of AI, providing C-suite executives with a strategic overview of the current landscape.\n",
      "\n",
      "**Significant Industry Developments**\n",
      "\n",
      "1. **Laplacian Eigenvector Gradient Orthogonalization (LEGO):** This novel method addresses the limitations of traditional techniques in estimating tangent spaces on data manifolds, particularly in noisy environments. By leveraging graph Laplacian eigenvectors, LEGO enhances the accuracy of geometric data analysis, which is crucial for machine learning applications. This development is particularly relevant for industries that rely on precise data analysis, such as finance, healthcare, and autonomous systems.\n",
      "\n",
      "2. **KAVA Framework for Latent Reasoning in LLMs:** KAVA introduces a more efficient approach to latent reasoning in large language models by utilizing compressed Key-Value cache distillation. This framework maintains the accuracy of chain-of-thought reasoning while enhancing scalability and efficiency. The implications of KAVA are significant for AI professionals developing deployable reasoning models, impacting sectors like customer service automation, legal tech, and education.\n",
      "\n",
      "3. **Fine-Tuning LLMs with PyTorch and Hugging Face:** The practical approach outlined by Daniel Voigt Godoy demonstrates the use of quantized models and low-rank adapters to optimize memory usage and reduce trainable parameters. This advancement facilitates the development of specialized language models, with potential applications in conversational AI and customer interaction platforms.\n",
      "\n",
      "4. **NoiseShift for Low-Resolution Image Generation:** NoiseShift is a training-free technique that improves image quality in text-to-image diffusion models at low resolutions. By recalibrating the denoiser's noise level, this method enhances the adaptability and efficiency of diffusion models, with potential applications in media, entertainment, and e-commerce sectors where high-quality image generation is essential.\n",
      "\n",
      "5. **PhysVid Dataset for Dynamic Physical Properties:** The introduction of PhysVid, a dataset annotated with dynamic physical properties, represents a significant advancement in AI's ability to interpret complex physical interactions from video data. This development has potential applications in robotics, material science, and autonomous systems, where understanding dynamic physical properties is crucial.\n",
      "\n",
      "**Practical Applications**\n",
      "\n",
      "The practical applications of these developments are vast and varied. LEGO's robust method for geometric data analysis can enhance predictive analytics in finance and healthcare. KAVA's efficient reasoning framework can improve the performance of AI-driven customer service platforms, providing faster and more accurate responses. The fine-tuning of LLMs using PyTorch and Hugging Face tools enables the creation of more engaging conversational AI systems, enhancing user experience in customer interaction. NoiseShift's ability to generate high-quality images at low resolutions can revolutionize content creation in media and advertising. Finally, PhysVid's insights into dynamic physical properties can advance the development of more sophisticated robotics and autonomous systems.\n",
      "\n",
      "**Key Performance Milestones**\n",
      "\n",
      "Several key performance milestones have been achieved across these developments. LEGO has demonstrated superior performance in manifold learning and boundary detection, outperforming traditional methods. KAVA has shown that it can maintain accuracy while significantly reducing computational costs, making it a scalable solution for large language models. The fine-tuning approach for LLMs has successfully optimized memory usage, allowing for the efficient deployment of specialized models. NoiseShift has achieved better FID scores, indicating improved image quality in diffusion models. PhysVid has provided a benchmark for evaluating AI's ability to understand dynamic physical properties, setting the stage for further advancements in this area.\n",
      "\n",
      "**Synthesis of Trends**\n",
      "\n",
      "The overarching trend across these developments is the increasing emphasis on efficiency and scalability in AI models. As AI applications become more integrated into various industries, the demand for models that can perform complex tasks with minimal computational resources is growing. Additionally, there is a clear focus on enhancing the accuracy and reliability of AI systems, particularly in noisy or complex environments. This trend is driving innovation in methods like LEGO and KAVA, which prioritize robustness and efficiency. Furthermore, the development of datasets like PhysVid highlights the industry's commitment to advancing AI's understanding of real-world interactions, paving the way for more sophisticated applications in robotics and autonomous systems.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "The AI industry is witnessing significant advancements that are transforming the capabilities of AI models and their applications across various sectors. These developments underscore the importance of efficiency, scalability, and accuracy in AI systems, setting the stage for continued innovation and growth. As these trends continue to evolve, they will undoubtedly shape the future of AI, offering new opportunities and challenges for businesses and industries worldwide.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "============================================================\n",
      "✅ SUMMARIZER COMPLETE\n",
      "============================================================\n",
      "\n",
      "**Executive Summary: AI Industry Developments and Trends**\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "As the AI industry continues to evolve at a rapid pace, several significant developments have emerged, each with profound implications for various sectors. This executive summary highlights the most impactful advancements, their practical applications, and key performance milestones. It also synthesizes overarching trends that are shaping the future of AI, providing C-suite executives with a strategic overview of the current landscape.\n",
      "\n",
      "**Significant Industry Developments**\n",
      "\n",
      "1. **Laplacian Eigenvector Gradient Orthogonalization (LEGO):** This novel method addresses the limitations of traditional techniques in estimating tangent spaces on data manifolds, particularly in noisy environments. By leveraging graph Laplacian eigenvectors, LEGO enhances the accuracy of geometric data analysis, which is crucial for machine learning applications. This development is particularly relevant for industries that rely on precise data analysis, such as finance, healthcare, and autonomous systems.\n",
      "\n",
      "2. **KAVA Framework for Latent Reasoning in LLMs:** KAVA introduces a more efficient approach to latent reasoning in large language models by utilizing compressed Key-Value cache distillation. This framework maintains the accuracy of chain-of-thought reasoning while enhancing scalability and efficiency. The implications of KAVA are significant for AI professionals developing deployable reasoning models, impacting sectors like customer service automation, legal tech, and education.\n",
      "\n",
      "3. **Fine-Tuning LLMs with PyTorch and Hugging Face:** The practical approach outlined by Daniel Voigt Godoy demonstrates the use of quantized models and low-rank adapters to optimize memory usage and reduce trainable parameters. This advancement facilitates the development of specialized language models, with potential applications in conversational AI and customer interaction platforms.\n",
      "\n",
      "4. **NoiseShift for Low-Resolution Image Generation:** NoiseShift is a training-free technique that improves image quality in text-to-image diffusion models at low resolutions. By recalibrating the denoiser's noise level, this method enhances the adaptability and efficiency of diffusion models, with potential applications in media, entertainment, and e-commerce sectors where high-quality image generation is essential.\n",
      "\n",
      "5. **PhysVid Dataset for Dynamic Physical Properties:** The introduction of PhysVid, a dataset annotated with dynamic physical properties, represents a significant advancement in AI's ability to interpret complex physical interactions from video data. This development has potential applications in robotics, material science, and autonomous systems, where understanding dynamic physical properties is crucial.\n",
      "\n",
      "**Practical Applications**\n",
      "\n",
      "The practical applications of these developments are vast and varied. LEGO's robust method for geometric data analysis can enhance predictive analytics in finance and healthcare. KAVA's efficient reasoning framework can improve the performance of AI-driven customer service platforms, providing faster and more accurate responses. The fine-tuning of LLMs using PyTorch and Hugging Face tools enables the creation of more engaging conversational AI systems, enhancing user experience in customer interaction. NoiseShift's ability to generate high-quality images at low resolutions can revolutionize content creation in media and advertising. Finally, PhysVid's insights into dynamic physical properties can advance the development of more sophisticated robotics and autonomous systems.\n",
      "\n",
      "**Key Performance Milestones**\n",
      "\n",
      "Several key performance milestones have been achieved across these developments. LEGO has demonstrated superior performance in manifold learning and boundary detection, outperforming traditional methods. KAVA has shown that it can maintain accuracy while significantly reducing computational costs, making it a scalable solution for large language models. The fine-tuning approach for LLMs has successfully optimized memory usage, allowing for the efficient deployment of specialized models. NoiseShift has achieved better FID scores, indicating improved image quality in diffusion models. PhysVid has provided a benchmark for evaluating AI's ability to understand dynamic physical properties, setting the stage for further advancements in this area.\n",
      "\n",
      "**Synthesis of Trends**\n",
      "\n",
      "The overarching trend across these developments is the increasing emphasis on efficiency and scalability in AI models. As AI applications become more integrated into various industries, the demand for models that can perform complex tasks with minimal computational resources is growing. Additionally, there is a clear focus on enhancing the accuracy and reliability of AI systems, particularly in noisy or complex environments. This trend is driving innovation in methods like LEGO and KAVA, which prioritize robustness and efficiency. Furthermore, the development of datasets like PhysVid highlights the industry's commitment to advancing AI's understanding of real-world interactions, paving the way for more sophisticated applications in robotics and autonomous systems.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "The AI industry is witnessing significant advancements that are transforming the capabilities of AI models and their applications across various sectors. These developments underscore the importance of efficiency, scalability, and accuracy in AI systems, setting the stage for continued innovation and growth. As these trends continue to evolve, they will undoubtedly shape the future of AI, offering new opportunities and challenges for businesses and industries worldwide.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CUSTOMIZE YOUR PROMPTS HERE\n",
    "# ============================================================\n",
    "\n",
    "SUMMARY_PROMPT = \"\"\"You are an expert at summarizing AI research papers and industry developments.\n",
    "\n",
    "Please write a concise, informative summary of the following content, focusing specifically on:\n",
    "- Novel advancements or breakthroughs in AI/ML\n",
    "- State-of-the-art techniques or methodologies\n",
    "- Performance improvements or benchmark results\n",
    "- Practical applications and industry impact\n",
    "- Significance to the AI research community\n",
    "\n",
    "Keep the summary focused and relevant to AI industry professionals. Maximum 12 sentences.\"\"\"\n",
    "\n",
    "EXECUTIVE_BRIEF_PROMPT = \"\"\"You are an expert AI industry analyst creating executive summaries for C-suite executives and industry leaders.\n",
    "\n",
    "You are given detailed summaries of recent AI research papers and industry developments. Your task is to create a concise executive summary of approximately 700 words that:\n",
    "\n",
    "1. **Identifies the most significant industry developments** - Focus on breakthroughs that will impact businesses, products, or the competitive landscape\n",
    "2. **Highlights practical applications** - Emphasize real-world uses and business implications\n",
    "3. **Notes key performance milestones** - Include impressive benchmark results or technical achievements\n",
    "4. **Synthesizes trends** - Look for patterns or themes across multiple developments\n",
    "5. **Maintains accessibility** - Write for business leaders who may not have deep technical expertise\n",
    "\n",
    "Structure your summary with:\n",
    "- A brief opening paragraph highlighting the week's most significant theme or development\n",
    "- 3-4 paragraphs covering the most important individual developments, organized by impact or theme\n",
    "- A concluding paragraph on what these developments mean for the AI industry going forward\n",
    "\n",
    "Target length: approximately 700 words. Be selective - only include the most industry-relevant developments.\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Execute Summarizer Agent\n",
    "# ============================================================\n",
    "\n",
    "print(\"📝 Starting Summarizer Agent\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📋 Task: Generate AI newsletter from MongoDB\")\n",
    "print(f\"🗄️  Database: {MONGODB_DATABASE}\")\n",
    "print(f\"📁 Collection: {MONGODB_COLLECTION}\")\n",
    "\n",
    "# Get document count before starting\n",
    "doc_info = retrieve_documents_from_mongodb()\n",
    "if doc_info[\"status\"] == \"success\":\n",
    "    print(f\"📄 Documents to process: {doc_info['unique_files']} unique files ({doc_info['total_documents']} total chunks)\")\n",
    "else:\n",
    "    print(f\"⚠️  Could not retrieve document count\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summarizer_response = summarizer_executor.invoke({\n",
    "    \"input\": f\"\"\"Generate this week's AI newsletter from MongoDB documents.\n",
    "\n",
    "For each document summary, use these instructions:\n",
    "{SUMMARY_PROMPT}\n",
    "\n",
    "For the executive brief, use these instructions:\n",
    "{EXECUTIVE_BRIEF_PROMPT}\n",
    "\n",
    "Process all documents and generate the complete newsletter.\"\"\",\n",
    "    \"mongodb_database\": MONGODB_DATABASE,\n",
    "    \"mongodb_collection\": MONGODB_COLLECTION\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ SUMMARIZER COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{summarizer_response['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7cb11",
   "metadata": {},
   "source": [
    "## What You've Learned\n",
    "\n",
    "**Document Processing Pipeline**: You've learned how to process PDF documents and HTML files with high-resolution partitioning, maintain page boundaries with page-based chunking, and store structured content in MongoDB for downstream applications.\n",
    "\n",
    "**Unstructured API Capabilities**: You've experienced intelligent document processing with hi_res strategy, advanced table detection and structure preservation, flexible chunking strategies for optimal text organization, and seamless integration with MongoDB for document storage.\n",
    "\n",
    "**AI-Powered Newsletter Generation**: You've built a complete system for retrieving processed documents from MongoDB, generating detailed summaries with customizable prompts, creating executive briefs that highlight key developments, and iterating on prompts to perfect your newsletter content.\n",
    "\n",
    "### Ready to Scale?\n",
    "\n",
    "Deploy automated newsletter systems for industry intelligence, build document summarization tools for research teams, or create AI-powered content aggregation systems. Add more document sources using additional S3 buckets, implement scheduled pipeline runs for fresh content, or scale up for production document volumes with automated processing.\n",
    "\n",
    "### Try Unstructured Today\n",
    "\n",
    "Ready to build your own AI-powered document processing system? [Sign up for a free trial](https://unstructured.io/?modal=try-for-free) and start transforming your documents into intelligent, searchable knowledge.\n",
    "\n",
    "**Need help getting started?** Contact our team to schedule a demo and see how Unstructured can solve your specific document processing challenges."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
