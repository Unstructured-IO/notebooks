{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building RAG for Docs with Firecrawl <> Unstructured Platform API <> MongoDB\n",
        "\n",
        "Have you ever found yourself frustrated while searching through technical documentation? Hours spent going through pages, trying to find that one specific detail you need to solve your problem? You're not alone.\n",
        "\n",
        "What if you could have a personal documentation assistant that knows all your docs inside and out, can stay up-to-date automatically, and answer your specific questions precisely?\n",
        "\n",
        "In this tutorial, we're building exactly that: a documentation copilot that combines web crawling, intelligent document processing, and vector search to create a system that actually understands your documentation and can answer questions about it naturally.\n",
        "\n",
        "By connecting [Firecrawl](https://www.firecrawl.dev/)'s powerful web crawler, [Unstructured](https://unstructured.io/)'s document processing workflows, and [MongoDB](https://www.mongodb.com/)'s vector search, we'll create a tool that:\n",
        "\n",
        "* Ingests documentation from any source (public or private)\n",
        "* Understands context across document boundaries\n",
        "* Answers questions using your documentation as its source of truth\n",
        "* Saves you hours of searching through fragmented documentation\n",
        "\n",
        "Let's dive in and build a tool that makes documentation work for you instead of the other way around!"
      ],
      "metadata": {
        "id": "hP51tUn8Bbye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Dependencies"
      ],
      "metadata": {
        "id": "tCRvEi_EBaeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Requirements\n",
        "\n",
        "For this project, you'll need the following libraries:\n",
        "\n",
        "- `firecrawl-py` ‚Äì to use Firecrawl's Python SDK for scraping websites\n",
        "- `unstructured` ‚Äì to preprocess your scraped data using the Unstructured client\n",
        "- `openai` ‚Äì to generate embeddings and run chat completions with OpenAI models\n",
        "- `pymongo` ‚Äì to interact with MongoDB and perform vector search\n"
      ],
      "metadata": {
        "id": "1BYOWoxaQZKY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjTvOqQwiZPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c60cbb5-fc6f-47d2-f8a5-f9043d41fb57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m599.1/599.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU pymongo openai firecrawl-py \"unstructured-client>=0.31.3\" boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook walks through the complete process of building an intelligent documentation assistant. We'll cover:\n",
        "\n",
        "* Data Collection: Using Firecrawl to automatically gather documentation from websites\n",
        "* Document Processing: Using Unstructured to transform raw HTML into LLM-ready chunks\n",
        "* Storage & Retrieval: Setting up MongoDB for efficient vector search\n",
        "* Question Answering: Building a RAG system that answers questions based on your docs\n",
        "\n",
        "By the end, you'll have a system that can answer questions about any documentation you feed it. Let's start by collecting the data!\n"
      ],
      "metadata": {
        "id": "WAGcbHOOzBHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Firecrawl to crawl and fetch documentation pages\n",
        "\n",
        "First, we need to gather the documentation content. For public docs, we'll use web crawling. Firecrawl is a specialized tool designed specifically for this purpose.\n",
        "\n",
        "To use Firecrawl's capabilities, you'll need an API key from https://www.firecrawl.dev/.\n",
        "\n",
        "For storage, we'll use an S3 bucket to temporarily hold our crawled data. This gives us flexibility and makes it easy for other systems to access the content later."
      ],
      "metadata": {
        "id": "N-6Rce4TBwDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# your firecrawl api key to call firecrawl\n",
        "os.environ['FIRECRAWL_API_KEY'] = userdata.get('FIRECRAWL_API_KEY')\n",
        "\n",
        "# AWS envs to host data onto a bucket\n",
        "os.environ['S3_AWS_KEY'] = userdata.get('S3_AWS_KEY')\n",
        "os.environ['S3_AWS_SECRET'] = userdata.get('S3_AWS_SECRET')\n"
      ],
      "metadata": {
        "id": "laAYfVi0SEii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the following helper functions to invoke Firecrawl with a given `url`.  The crawler will retrieve up to `limit` number of URLs, exploring up to `maxDepth` levels deep from each page.\n",
        "\n",
        "The functions below handle:\n",
        "- Configuring and authenticating with the Firecrawl API\n",
        "- Crawl a given url\n",
        "- Polling for job completion\n",
        "- Saving the crawled HTML files locally\n",
        "- Uploading the results to a specified S3 bucket\n",
        "\n",
        "Make sure you already set your `FIRECRAWL_API_KEY` and AWS credentials (`S3_AWS_KEY`, `S3_AWS_SECRET`) as environment variables.\n"
      ],
      "metadata": {
        "id": "cd60ql3353zA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import tempfile\n",
        "import boto3\n",
        "import hashlib\n",
        "from typing import Dict, Any\n",
        "from firecrawl import FirecrawlApp\n",
        "\n",
        "def prepare_firecrawl_config(api_key=None) -> Dict[str, str]:\n",
        "    '''\n",
        "    Fetch all env variables for firecrawl\n",
        "    '''\n",
        "    api_key = api_key or os.environ[\"FIRECRAWL_API_KEY\"]\n",
        "    if not api_key:\n",
        "        return {\"error\": \"Firecrawl API key is required.\"}\n",
        "    return {\"api_key\": api_key}\n",
        "\n",
        "\n",
        "def invoke_firecrawl_crawlhtml(url: str, s3_uri: str, limit: int = 100, maxDepth: int = 2, api_key: str = None,\n",
        "                                timeout: int = 3600) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Start a web crawl job using Firecrawl to retrieve HTML content. We poll every 30s to check if the job has been completed.\n",
        "    When done, the result data is stored as html files and uploaded into a directory in S3.\n",
        "    Args:\n",
        "      url: Url to crawl\n",
        "      s3_uri: Uri to store the html files (the results get stored under {s3_uri}/{id_generated by the process})\n",
        "      limit: Number of urls to crawl totally\n",
        "      maxDepth: Number of urls to crawl from a particular page\n",
        "      timeout: When this Job should timeout if it doesn't finish by then\n",
        "\n",
        "    Returns:\n",
        "      Dict with Job ID, Status of Job, s3_uri of job content, number of files uploaded, upload stats\n",
        "    \"\"\"\n",
        "    config = prepare_firecrawl_config(api_key)\n",
        "    if \"error\" in config:\n",
        "        return {\"error\": config[\"error\"]}\n",
        "\n",
        "    try:\n",
        "        validated_s3_uri = ensure_valid_s3_uri(s3_uri)\n",
        "    except ValueError as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "    firecrawl = FirecrawlApp(api_key=config[\"api_key\"])\n",
        "    job = firecrawl.async_crawl_url(url, params={\"limit\": limit, \"maxDepth\": maxDepth, \"scrapeOptions\": {\"formats\": [\"html\"]}})\n",
        "    job_id = job.get(\"id\")\n",
        "\n",
        "    start = time.time()\n",
        "    while True:\n",
        "        status = firecrawl.check_crawl_status(job_id)\n",
        "        if status.get(\"status\") == \"completed\":\n",
        "            break\n",
        "        if time.time() - start > timeout:\n",
        "            return {\"id\": job_id, \"status\": \"timeout\", \"error\": \"Job timed out.\"}\n",
        "        print(f\"Job status is {status.get('status')} Sleeping 30s before polling again...\")\n",
        "        time.sleep(30)\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        job_dir = os.path.join(tmpdir, job_id)\n",
        "        os.makedirs(job_dir, exist_ok=True)\n",
        "        file_count = process_crawlhtml_results(status, job_dir)\n",
        "        upload_stats = upload_directory_to_s3(job_dir, validated_s3_uri + job_id + \"/\")\n",
        "\n",
        "    return {\n",
        "        \"id\": job_id,\n",
        "        \"status\": \"completed\",\n",
        "        \"s3_uri\": validated_s3_uri + job_id + \"/\",\n",
        "        \"file_count\": file_count,\n",
        "        **upload_stats\n",
        "    }\n",
        "\n",
        "def process_crawlhtml_results(result: Dict[str, Any], output_dir: str) -> int:\n",
        "    \"\"\"\n",
        "    Takes the crawled results and saves them as .html files in the output directory\n",
        "    Args:\n",
        "      result: Result from the Crawl Job\n",
        "      output_dir: Directory to store the .html files\n",
        "\n",
        "    Returns:\n",
        "      Number of files saved\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    for i, page in enumerate(result.get(\"data\", [])):\n",
        "        html = page.get(\"html\")\n",
        "        if not html:\n",
        "            continue\n",
        "        url = page.get(\"metadata\", {}).get(\"url\", f\"page-{i}\")\n",
        "        filename = clean_url_to_filename(url)\n",
        "        with open(os.path.join(output_dir, filename), \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(html)\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "def upload_directory_to_s3(local_dir: str, s3_uri: str) -> Dict[str, Any]:\n",
        "    \"\"\"Uploads a directory to the S3 path provided\n",
        "\n",
        "    Args:\n",
        "        local_dir: Local directory to upload\n",
        "        s3_uri: S3 URI to upload to\n",
        "\n",
        "    Returns:\n",
        "        Dict with upload stats\n",
        "    \"\"\"\n",
        "    bucket, prefix = s3_uri[5:].split('/', 1)\n",
        "    s3_client = boto3.client(\n",
        "        's3',\n",
        "        aws_access_key_id=os.environ['S3_AWS_KEY'],\n",
        "        aws_secret_access_key=os.environ['S3_AWS_SECRET']\n",
        "    )\n",
        "    stats = {\"uploaded_files\": 0, \"failed_files\": 0, \"total_bytes\": 0}\n",
        "    for root, _, files in os.walk(local_dir):\n",
        "        for file in files:\n",
        "            local_path = os.path.join(root, file)\n",
        "            relative_path = os.path.relpath(local_path, local_dir)\n",
        "            s3_key = os.path.join(prefix, relative_path).replace(\"\\\\\", \"/\")\n",
        "            try:\n",
        "                s3_client.upload_file(local_path, bucket, s3_key)\n",
        "                stats[\"uploaded_files\"] += 1\n",
        "                stats[\"total_bytes\"] += os.path.getsize(local_path)\n",
        "                print(f\"Uploaded {local_path} to s3://{bucket}/{s3_key}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading {local_path}: {str(e)}\")\n",
        "                stats[\"failed_files\"] += 1\n",
        "    return stats\n",
        "\n",
        "\n",
        "def ensure_valid_s3_uri(s3_uri: str) -> str:\n",
        "    '''\n",
        "    Validate and Normalize input S3 Uri\n",
        "\n",
        "    Args:\n",
        "      s3_uri: input s3_uri\n",
        "\n",
        "    Returns:\n",
        "      ValueError if not a valid s3_uri else a normalizewd s3_uri\n",
        "    '''\n",
        "    if not s3_uri.startswith(\"s3://\"):\n",
        "        raise ValueError(\"S3 URI must start with 's3://'\")\n",
        "    return s3_uri if s3_uri.endswith(\"/\") else s3_uri + \"/\"\n",
        "\n",
        "def clean_url_to_filename(url: str) -> str:\n",
        "    '''\n",
        "    Convert a URL to a valid filename.\n",
        "\n",
        "    Args:\n",
        "      s3_uri: input url\n",
        "\n",
        "    Returns:\n",
        "      Url converted into a validfilename\n",
        "    '''\n",
        "    filename = url.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
        "    filename = filename.replace(\"/\", \"_\").replace(\"?\", \"_\").replace(\"&\", \"_\").replace(\":\", \"_\")\n",
        "    if len(filename) > 200:\n",
        "        domain = filename.split('_')[0]\n",
        "        filename_hash = hashlib.md5(url.encode()).hexdigest()\n",
        "        return f\"{domain}_{filename_hash}.html\"\n",
        "    return f\"{filename}.html\""
      ],
      "metadata": {
        "id": "eW3pJkBudtP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's Start Crawling!\n",
        "For this example, we'll crawl the documentation from the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) - a standard for providing context to AI models. This is just an example - you could use any documentation site that's relevant to your work.\n",
        "\n",
        "We'll limit the documentation crawling to 10 urls with a maxDepth of 2 urls from each site."
      ],
      "metadata": {
        "id": "Ss0NJtfeDmB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remote_s3_uri = \"s3://<bucketname>/<directoryname>\" # Provide your own S3 Uri\n",
        "crawl_result = invoke_firecrawl_crawlhtml(\n",
        "    url=\"https://modelcontextprotocol.io/\",\n",
        "    s3_uri=remote_s3_uri,\n",
        "    limit=10, # num urls to crawl\n",
        "    maxDepth=2, # max urls to crawl within a given url (As per the documentation, the max number of slashes the pathname of a scraped URL may contain.)\n",
        ")\n"
      ],
      "metadata": {
        "id": "3ev2fw_iDnFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f723ba0-f3e5-4c03-8b3b-b2e107e9c91c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job status is scraping Sleeping 30s before polling again...\n",
            "Uploaded /tmp/tmp4lk7babp/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_development_roadmap.html to s3://ajay-uns-devrel-content/mcp-content1/crawl_results/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_development_roadmap.html\n",
            "Uploaded /tmp/tmp4lk7babp/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_clients.html to s3://ajay-uns-devrel-content/mcp-content1/crawl_results/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_clients.html\n",
            "Uploaded /tmp/tmp4lk7babp/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_quickstart_server.html to s3://ajay-uns-devrel-content/mcp-content1/crawl_results/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_quickstart_server.html\n",
            "Uploaded /tmp/tmp4lk7babp/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_tutorials_building-mcp-with-llms.html to s3://ajay-uns-devrel-content/mcp-content1/crawl_results/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_tutorials_building-mcp-with-llms.html\n",
            "Uploaded /tmp/tmp4lk7babp/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_examples.html to s3://ajay-uns-devrel-content/mcp-content1/crawl_results/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_examples.html\n",
            "Uploaded /tmp/tmp4lk7babp/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_development_updates.html to s3://ajay-uns-devrel-content/mcp-content1/crawl_results/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_development_updates.html\n",
            "Uploaded /tmp/tmp4lk7babp/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_development_contributing.html to s3://ajay-uns-devrel-content/mcp-content1/crawl_results/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_development_contributing.html\n",
            "Uploaded /tmp/tmp4lk7babp/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_quickstart_user.html to s3://ajay-uns-devrel-content/mcp-content1/crawl_results/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_quickstart_user.html\n",
            "Uploaded /tmp/tmp4lk7babp/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_quickstart_client.html to s3://ajay-uns-devrel-content/mcp-content1/crawl_results/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_quickstart_client.html\n",
            "Uploaded /tmp/tmp4lk7babp/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_introduction.html to s3://ajay-uns-devrel-content/mcp-content1/crawl_results/193ce3ec-a289-4ba9-abda-63790faa7aad/modelcontextprotocol.io_introduction.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Now we have raw documentation content stored in S3. Now we have to transform the raw HTML docs into a format we can use for RAG. This is where Unstructured comes in."
      ],
      "metadata": {
        "id": "onYpR6CvG-Ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Unstructured to Transform Documentation into RAG-Ready Data\n",
        "\n",
        "Raw HTML is messy - it's filled with navigation bars, footers, styling code, and other elements that aren't relevant to the actual documentation content. This \"noise\" makes it hard for LLMs to focus on what matters.\n",
        "The Unstructured Platform specializes in turning messy documents into clean, structured data that's perfect for AI applications. It can:\n",
        "\n",
        "* Extract the meaningful content from HTML documents\n",
        "* Identify headers, paragraphs, code blocks, and other elements\n",
        "* Split content into chunks that preserve semantic meaning\n",
        "* Generate vector embeddings that capture the conceptual meaning\n",
        "\n",
        "To use Unstructured Platform, you'll need an API key:\n",
        "\n",
        "1. Go to [https://platform.unstructured.io](https://platform.unstructured.io) and use your email address, Google account, or GitHub account to sign up (if you don't already have an account) and sign in. The Unstructured user interface (UI) will appear.\n",
        "\n",
        "2. Get your Unstructured API key:\n",
        "   - In the Unstructured UI, click **API Keys** on the sidebar.\n",
        "   - Click **Generate API Key**.\n",
        "   - Follow the on-screen instructions to finish generating the key.\n",
        "   - Click the **Copy** icon next to your new key to copy it to your clipboard.\n"
      ],
      "metadata": {
        "id": "3T-PXEYVDn9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['UNSTRUCTURED_API_KEY'] = userdata.get('UNSTRUCTURED_API_KEY')"
      ],
      "metadata": {
        "id": "bvxG881ChKND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper code to pretty print info\n",
        "\n",
        "def pretty_print_model(response_model):\n",
        "    print(response_model.model_dump_json(indent=4))"
      ],
      "metadata": {
        "id": "tb0Co7cqSBv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Unstructured Workflows\n",
        "\n",
        "Unstructured uses a workflow concept to process documents. A workflow consists of:\n",
        "\n",
        "* Source Connectors: Where Unstructured gets the documents from (S3 in our case)\n",
        "* Processing Nodes: How Unstructured transforms the documents\n",
        "* Destination Connectors: Where Unstructured sends the processed results (MongoDB in our case)\n",
        "\n",
        "Let's set these up one by one:\n"
      ],
      "metadata": {
        "id": "YxdzSFk4jn9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured_client import UnstructuredClient\n",
        "from unstructured_client.models.operations import CreateSourceRequest\n",
        "from unstructured_client.models.operations import CreateDestinationRequest\n",
        "\n",
        "from unstructured_client.models.shared import (\n",
        "    CreateSourceConnector,\n",
        "    SourceConnectorType,\n",
        "    S3SourceConnectorConfigInput\n",
        ")\n",
        "from unstructured_client.models.shared import (\n",
        "    CreateDestinationConnector,\n",
        "    DestinationConnectorType,\n",
        "    MongoDBConnectorConfigInput\n",
        ")\n",
        "\n",
        "unstructured_client = UnstructuredClient(api_key_auth=os.environ['UNSTRUCTURED_API_KEY'])"
      ],
      "metadata": {
        "id": "KchmC7ig9wyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up S3 Source Connector\n"
      ],
      "metadata": {
        "id": "Mnlo4DtFDwhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Source Connector for this notebook is AWS S3. We're setting this up with AWS secret access key authentication. For this you require AWS Key, AWS Secret and the S3_Uri containing the files. Using the ones from the previous steps\n",
        "\n",
        "We also support 20 other source connectors (check out [here](https://docs.unstructured.io/api-reference/workflow/sources/overview))"
      ],
      "metadata": {
        "id": "Ld3PKHInjd6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_connector_response = unstructured_client.sources.create_source(\n",
        "    request=CreateSourceRequest(\n",
        "        create_source_connector=CreateSourceConnector(\n",
        "            name=\"fc_mongo_source\",\n",
        "            type=SourceConnectorType.S3,\n",
        "            config=S3SourceConnectorConfigInput(\n",
        "                key=os.environ['S3_AWS_KEY'],\n",
        "                secret=os.environ['S3_AWS_SECRET'],\n",
        "                remote_url=crawl_result['s3_uri'], # The s3 uri with the crawled results\n",
        "                recursive=True #True/False - True enables processing every file in the directory\n",
        "            )\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "pretty_print_model(source_connector_response.source_connector_information)"
      ],
      "metadata": {
        "id": "Z7Q0wzniDp0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f4ae8e1-5b36-4915-d05f-292975959f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"config\": {\n",
            "        \"anonymous\": false,\n",
            "        \"recursive\": true,\n",
            "        \"remote_url\": \"s3://ajay-uns-devrel-content/mcp-content1/crawl_results/193ce3ec-a289-4ba9-abda-63790faa7aad/\",\n",
            "        \"key\": \"**********\",\n",
            "        \"secret\": \"**********\"\n",
            "    },\n",
            "    \"created_at\": \"2025-04-01T13:27:43.310017Z\",\n",
            "    \"id\": \"5894cdff-9a3e-49da-ab49-b1d5a19df98b\",\n",
            "    \"name\": \"fc_mongo_source\",\n",
            "    \"type\": \"s3\",\n",
            "    \"updated_at\": \"2025-04-01T13:27:43.937805Z\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up MongoDB Destination Connector\n",
        "\n",
        "After processing, we'll store our enriched documentation in MongoDB. MongoDB Atlas is perfect for this use case because it supports vector search, which will let us quickly find relevant pieces of documentation.\n",
        "\n",
        "Unstructured requires the following information to set up a MongoDB Destination Connector:\n",
        "\n",
        "- `database name`\n",
        "- `collection name`\n",
        "- `uri`\n",
        "\n",
        "Follow [this tutorial](https://docs.unstructured.io/api-reference/workflow/destinations/mongodb) to create your MongoDB cluster and get the connection details.\n",
        "\n",
        "> **Note:**\n",
        "> Make sure your IP address can access MongoDB.  \n",
        "\n",
        "Also, ensure you've created a **search index** for the `embeddings` field to enable vector search. Use the following configuration to create the index. Make sure to set the correct dimensionality of 1536 for OpenAI embeddings. This is necessary for similarity search later:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"mappings\": {\n",
        "    \"dynamic\": true,\n",
        "    \"fields\": {\n",
        "      \"embeddings\": {\n",
        "        \"dimensions\": 1536,\n",
        "        \"similarity\": \"cosine\",\n",
        "        \"type\": \"knnVector\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "id": "viobVf7DD-Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['MONGO_DATABASE'] = userdata.get('MONGO_DATABASE')\n",
        "os.environ['MONGO_COLLECTION'] = userdata.get('MONGO_COLLECTION')\n",
        "os.environ['MONGO_URI'] = userdata.get('MONGO_URI')"
      ],
      "metadata": {
        "id": "uDT35PY1yN8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dest_connector_response = unstructured_client.destinations.create_destination(\n",
        "    request=CreateDestinationRequest(\n",
        "        create_destination_connector=CreateDestinationConnector(\n",
        "            name=\"fc_mongo_dest\",\n",
        "            type=DestinationConnectorType.MONGODB,\n",
        "            config=MongoDBConnectorConfigInput(\n",
        "                database=os.environ['MONGO_DATABASE'],\n",
        "                collection=os.environ['MONGO_COLLECTION'],\n",
        "                uri=os.environ['MONGO_URI']\n",
        "            )\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "pretty_print_model(dest_connector_response.destination_connector_information)"
      ],
      "metadata": {
        "id": "eSfDrVTaEAd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ff99e0-16d5-4e10-d221-090f414c89eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"config\": {\n",
            "        \"collection\": \"demo2\",\n",
            "        \"database\": \"demo2\",\n",
            "        \"uri\": \"**********\"\n",
            "    },\n",
            "    \"created_at\": \"2025-04-01T13:27:45.544845Z\",\n",
            "    \"id\": \"692e8385-6d45-4c62-90b9-10d8a8005b49\",\n",
            "    \"name\": \"fc_mongo_dest\",\n",
            "    \"type\": \"mongodb\",\n",
            "    \"updated_at\": \"2025-04-01T13:27:45.787342Z\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Document Processing Workflow\n",
        "\n",
        "Now we'll define how our documents should be processed. We'll use three key data processing nodes/steps:\n",
        "\n",
        "* **Partitioner** - Partitioning extracts content from raw unstructured files and outputs that content as structured document elements. We use a VLM Partitioner here, which essentially uses a Vision Language Model (`claude-3.7 sonnent` to extract elements from each page.\n",
        "\n",
        "* **Chunker** - chunking arranges the resulting document elements into manageable ‚Äúchunks‚Äù to retrieve only parts of documents that contain only the information that is relevant to a user‚Äôs query.\n",
        "\n",
        "* **Embedder** - creates arrays of numbers known as vectors, representing the text that is extracted by Unstructured. These vectors are stored or embedded next to the text itself. These vector embeddings are generated by an embedding model that is provided by an embedding provider.\n",
        "\n",
        "\n",
        "For more information on the arguments used, check out the [Concepts tab](https://docs.unstructured.io/ui/document-elements).\n"
      ],
      "metadata": {
        "id": "y6LlVf9PEBEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured_client.models.shared import (\n",
        "    WorkflowNode,\n",
        "    WorkflowNodeType,\n",
        "    WorkflowType,\n",
        "    Schedule\n",
        ")\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "vlm_partitioner_workflow_node = WorkflowNode(\n",
        "    name=\"Partitioner\",\n",
        "    subtype=\"vlm\",\n",
        "    type=WorkflowNodeType.PARTITION,\n",
        "    settings={\n",
        "       \"provider\": \"anthropic\",\n",
        "       \"model\": \"claude-3-7-sonnet-20250219\", # Vision Language Model used for partitioning\n",
        "       \"output_format\": \"text/html\",\n",
        "       \"user_prompt\": None, # provide custom instructions or prompts to guide the partitioning process (String)\n",
        "       \"format_html\": True, # formats documents in html, particularly helpful when you want to preserve the layout\n",
        "       \"unique_element_ids\": True, # each extracted element has its own id\n",
        "   }\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Chunk the partitioned content.\n",
        "chunk_node = WorkflowNode(\n",
        "    name=\"Chunker\",\n",
        "    subtype=\"chunk_by_title\",\n",
        "    type=WorkflowNodeType.CHUNK,\n",
        "    settings={\n",
        "        \"multipage_sections\": False, # Elements that occur on different pages are separated into distinct chunks.\n",
        "        \"combine_text_under_n_chars\": 0, # It combines elements from a section into a chunk until the section reaches a specified character length.\n",
        "        \"include_orig_elements\": True, # maintains detailed metadata that might otherwise be lost during consolidation.\n",
        "        \"new_after_n_chars\": 1900, #  a \"soft\" maximum size for a chunk, closing a chunk after reaching this character length.\n",
        "        \"max_characters\": 2048, # If a single element exceeds this size, it will be divided into two or more chunks using text-splitting.\n",
        "        \"overlap\": 160, # create overlapping text between chunks\n",
        "        \"overlap_all\": False, # Applies overlap between all consecutive chunks. If False, overlap is to chunks formed from oversized elements\n",
        "    }\n",
        ")\n",
        "\n",
        "# Generate vector embeddings.\n",
        "embed_node = WorkflowNode(\n",
        "    name=\"Embedder\",\n",
        "    subtype=\"azure_openai\",\n",
        "    type=WorkflowNodeType.EMBED, # Embedding type node creates embedding\n",
        "    settings={\n",
        "        \"model_name\": \"text-embedding-3-small\" # max dimension size with MongoDB is 2048, so lets use a model with an embedding size of 1536\n",
        "    }\n",
        ")\n",
        "\n",
        "unique_workflow_suffix = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "\n",
        "response = unstructured_client.workflows.create_workflow(\n",
        "    request={\n",
        "        \"create_workflow\": {\n",
        "            \"name\": f\"fc-s3-mongo-custom-workflow-{unique_workflow_suffix}\",\n",
        "            \"source_id\": source_connector_response.source_connector_information.id,\n",
        "            \"destination_id\": dest_connector_response.destination_connector_information.id,\n",
        "            \"workflow_type\": WorkflowType.CUSTOM,\n",
        "            \"workflow_nodes\": [\n",
        "                vlm_partitioner_workflow_node,\n",
        "                chunk_node,\n",
        "                embed_node\n",
        "            ],\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow_id = response.workflow_information.id\n",
        "pretty_print_model(response.workflow_information)"
      ],
      "metadata": {
        "id": "BbnzQxX4EEKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc1651e7-b50e-4166-a544-e4e086f2b50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"created_at\": \"2025-04-01T13:28:02.998345Z\",\n",
            "    \"destinations\": [\n",
            "        \"692e8385-6d45-4c62-90b9-10d8a8005b49\"\n",
            "    ],\n",
            "    \"id\": \"664dfc70-7828-45f5-ba82-c7f558c72b77\",\n",
            "    \"name\": \"fc-s3-mongo-custom-workflow-2025-04-01-13-28-02\",\n",
            "    \"sources\": [\n",
            "        \"5894cdff-9a3e-49da-ab49-b1d5a19df98b\"\n",
            "    ],\n",
            "    \"status\": \"active\",\n",
            "    \"workflow_nodes\": [\n",
            "        {\n",
            "            \"name\": \"Partitioner\",\n",
            "            \"subtype\": \"vlm\",\n",
            "            \"type\": \"partition\",\n",
            "            \"id\": \"f1fc40a5-f1af-4a5c-af3d-2a4f5536f56b\",\n",
            "            \"settings\": {\n",
            "                \"provider\": \"anthropic\",\n",
            "                \"provider_api_key\": null,\n",
            "                \"model\": \"claude-3-7-sonnet-20250219\",\n",
            "                \"output_format\": \"text/html\",\n",
            "                \"user_prompt\": null,\n",
            "                \"format_html\": true,\n",
            "                \"unique_element_ids\": true,\n",
            "                \"is_dynamic\": false,\n",
            "                \"allow_fast\": true\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Chunker\",\n",
            "            \"subtype\": \"chunk_by_title\",\n",
            "            \"type\": \"chunk\",\n",
            "            \"id\": \"dd950c97-5a5c-4d92-bbee-76c7b9861586\",\n",
            "            \"settings\": {\n",
            "                \"unstructured_api_url\": null,\n",
            "                \"unstructured_api_key\": null,\n",
            "                \"multipage_sections\": false,\n",
            "                \"combine_text_under_n_chars\": 0,\n",
            "                \"include_orig_elements\": true,\n",
            "                \"new_after_n_chars\": 1900,\n",
            "                \"max_characters\": 2048,\n",
            "                \"overlap\": 160,\n",
            "                \"overlap_all\": false,\n",
            "                \"contextual_chunking_strategy\": null\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Embedder\",\n",
            "            \"subtype\": \"azure_openai\",\n",
            "            \"type\": \"embed\",\n",
            "            \"id\": \"366612c4-9157-4775-9976-88deaf135ddd\",\n",
            "            \"settings\": {\n",
            "                \"model_name\": \"text-embedding-3-small\"\n",
            "            }\n",
            "        }\n",
            "    ],\n",
            "    \"schedule\": {\n",
            "        \"crontab_entries\": []\n",
            "    },\n",
            "    \"updated_at\": \"2025-04-01T13:28:03.007634Z\",\n",
            "    \"workflow_type\": \"custom\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the workflow\n",
        "\n",
        "Now that we've defined how we want to process our documentation, let's start the workflow and wait for it to complete:"
      ],
      "metadata": {
        "id": "791C7qsOEE-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job = unstructured_client.workflows.run_workflow(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id,\n",
        "    }\n",
        ")\n",
        "\n",
        "pretty_print_model(job.job_information)"
      ],
      "metadata": {
        "id": "FN6RJg8eEWAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb552b43-ebf1-4932-9924-0a00f0e4f108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"created_at\": \"2025-04-01T13:28:05.031504Z\",\n",
            "    \"id\": \"956240a1-8050-4919-88a9-ca870040d059\",\n",
            "    \"status\": \"SCHEDULED\",\n",
            "    \"workflow_id\": \"664dfc70-7828-45f5-ba82-c7f558c72b77\",\n",
            "    \"workflow_name\": \"fc-s3-mongo-custom-workflow-2025-04-01-13-28-02\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = unstructured_client.jobs.list_jobs(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id\n",
        "    }\n",
        ")\n",
        "\n",
        "last_job = response.response_list_jobs[0]\n",
        "job_id = last_job.id\n",
        "print(f\"job_id: {job_id}\")"
      ],
      "metadata": {
        "id": "1NCr0rIZc2mi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3143b989-dc45-40b0-c6ab-9486a223334a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "job_id: 956240a1-8050-4919-88a9-ca870040d059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've created and started a job, we can poll Unstructured's `get_job` endpoint and check for its status every 30s till completion"
      ],
      "metadata": {
        "id": "lbhDlPRGvnEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def poll_job_status(job_id, poll_time=30):\n",
        "    while True:\n",
        "        response = unstructured_client.jobs.get_job(\n",
        "            request={\n",
        "                \"job_id\": job_id\n",
        "            }\n",
        "        )\n",
        "\n",
        "        job = response.job_information\n",
        "\n",
        "        if job.status == \"SCHEDULED\":\n",
        "            print(f\"Job is scheduled, polling again in {poll_time} seconds...\")\n",
        "            time.sleep(poll_time)\n",
        "        elif job.status == \"IN_PROGRESS\":\n",
        "            print(f\"Job is in progress, polling again in {poll_time} seconds...\")\n",
        "            time.sleep(poll_time)\n",
        "        else:\n",
        "            print(\"Job is completed\")\n",
        "            break\n",
        "\n",
        "    return job\n",
        "\n",
        "job = poll_job_status(job_id)\n",
        "pretty_print_model(job)"
      ],
      "metadata": {
        "id": "0oU729Nvc4DP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0720e2-89b8-4357-a989-c59c68963fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job is scheduled, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is completed\n",
            "{\n",
            "    \"created_at\": \"2025-04-01T13:28:05.031504\",\n",
            "    \"id\": \"956240a1-8050-4919-88a9-ca870040d059\",\n",
            "    \"status\": \"COMPLETED\",\n",
            "    \"workflow_id\": \"664dfc70-7828-45f5-ba82-c7f558c72b77\",\n",
            "    \"workflow_name\": \"fc-s3-mongo-custom-workflow-2025-04-01-13-28-02\",\n",
            "    \"runtime\": \"PT0S\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fantastic! At this point, our documentation has been:\n",
        "\n",
        "* Crawled from the web using Firecrawl\n",
        "* Processed and structured using Unstructured\n",
        "* Stored as vector-embedded chunks in MongoDB\n",
        "\n",
        "Now comes the exciting part - actually using this processed documentation to answer questions!\n",
        "\n",
        "Let's build RAG!"
      ],
      "metadata": {
        "id": "m7Ry0L07-YlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG ü§ñ\n",
        "\n",
        "\n",
        "Now we'll create a simple RAG (Retrieval-Augmented Generation) system that can answer questions about our documentation. Here's how it works:\n",
        "\n",
        "\n",
        "RAG combines two key steps:\n",
        "1. **Retrieval** ‚Äì We use MongoDB Atlas Vector Search to find the most relevant documents based on a user's query.\n",
        "2. **Generation** ‚Äì We feed those documents into an OpenAI model to generate a grounded, context-aware answer.\n",
        "\n",
        "This approach helps LLMs go beyond their training data by anchoring responses in real, up-to-date content ‚Äî including internal or domain-specific docs you control."
      ],
      "metadata": {
        "id": "QoXrEGupEiib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "from openai import OpenAI\n"
      ],
      "metadata": {
        "id": "P31rvoZKElLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use OpenAI models for embedding and generation, you'll need add your `OPENAI_API_KEY` to secrets."
      ],
      "metadata": {
        "id": "0886lw7fwpiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "OPENAI_CLIENT = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "GENERATION_MODEL = \"gpt-4o-2024-11-20\""
      ],
      "metadata": {
        "id": "iqaS6IvodeYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's connect to our MongoDB collection:"
      ],
      "metadata": {
        "id": "jXoe9x2f_w5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pymongo import MongoClient\n",
        "from typing import List\n",
        "\n",
        "# MongoDB setup\n",
        "def get_collection(collection_name: str, database_name: str):\n",
        "    \"\"\"\n",
        "    Establish connection to MongoDB Atlas and return the specified collection.\n",
        "    Args:\n",
        "        collection_name (str): Name of the collection to retrieve\n",
        "        database_name (str): Name of the database\n",
        "    Returns:\n",
        "        Collection object from MongoDB\n",
        "    \"\"\"\n",
        "\n",
        "    mongo_uri = os.environ[\"MONGO_URI\"]\n",
        "    client = MongoClient(mongo_uri)\n",
        "    database = client[database_name]\n",
        "    collection = database[collection_name]\n",
        "\n",
        "    print(f\"Connected to MongoDB Collection: {database_name}\")\n",
        "    return collection\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EKHPeMugdf05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have these set in your environment\n",
        "COLLECTION = get_collection(\n",
        "    os.environ[\"MONGO_COLLECTION\"],\n",
        "    os.environ[\"MONGO_DATABASE\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW4ed4xKpK4x",
        "outputId": "207af96f-9217-48b8-bbf0-d433837244c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to MongoDB Collection: demo2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We're building a lightweight RAG (Retrieval-Augmented Generation) setup using OpenAI and MongoDB Atlas Vector Search.\n",
        "\n",
        "Here‚Äôs how it works:\n",
        "\n",
        "1. **`get_embedding()`** takes any text and generates a vector using OpenAI‚Äôs embedding model.\n",
        "\n",
        "2. **`simple_retriever()`** uses that vector to query MongoDB using `$vectorSearch`, returning the top-N most relevant documents based on similarity.\n",
        "\n",
        "3. **`build_debugging_prompt()`** takes the retrieved docs and the user‚Äôs question, and constructs a prompt that grounds the LLM‚Äôs response in real context.\n",
        "\n",
        "4. **`vanilla_rag()`** ties it all together ‚Äî it fetches the relevant docs, builds the prompt, and calls OpenAI to generate a context-aware answer.\n"
      ],
      "metadata": {
        "id": "I6k_JJr1EUIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding function\n",
        "def get_embedding(text: str):\n",
        "    \"\"\"\n",
        "    Generate embedding for given text using OpenAI's embedding model.\n",
        "    \"\"\"\n",
        "    return OPENAI_CLIENT.embeddings.create(\n",
        "        input=text, model=EMBEDDING_MODEL\n",
        "    ).data[0].embedding\n",
        "\n",
        "# Simple retrieval using $vector search (requires MongoDB Atlas Vector Search)\n",
        "def simple_retriever(query: str, n=10):\n",
        "    \"\"\"Retrieve top-N most relevant documents from MongoDB vector index\"\"\"\n",
        "    query_embedding = get_embedding(query)\n",
        "\n",
        "    results = COLLECTION.aggregate([\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"queryVector\": query_embedding,\n",
        "                \"path\": \"embeddings\",        # This must match the field in your documents\n",
        "                \"numCandidates\": 50,        # Increase if needed for better recall\n",
        "                \"limit\": n,\n",
        "                \"index\": \"vector_index\"      # Use the name you gave your search index\n",
        "            }\n",
        "        }\n",
        "    ])\n",
        "\n",
        "    docs = [doc[\"text\"] for doc in results]\n",
        "    return \"\\n\".join(\n",
        "        [f\"\\n\\n===== Document {i+1} =====\\n{doc}\" for i, doc in enumerate(docs)]\n",
        "    )\n",
        "\n",
        "\n",
        "# Prompt builder\n",
        "def build_debugging_prompt(user_query: str, retrieved_docs: str) -> str:\n",
        "    \"\"\"Create a prompt for the LLM grounded in retrieved context\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "---\n",
        "\n",
        "### Retrieved Documentation:\n",
        "{retrieved_docs}\n",
        "\n",
        "Analyse the retrieved docs, and then take the user's question, retrieved documents and suggest an answer based on the context provided. You can use your understanding of the retrieved documents to build on top and answer the user's question.\n",
        "\n",
        "Respond in clear **Markdown**. Use code blocks where relevant. Make sure the code is syntactically accurate and the content very relevant.\n",
        "\n",
        "---\n",
        "\n",
        "### User Question:\n",
        "{user_query}\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "# Main RAG function\n",
        "def vanilla_rag(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a structured, grounded answer using retrieved documents and user query.\n",
        "    \"\"\"\n",
        "    retrieved_docs = simple_retriever(question)\n",
        "    final_prompt = build_debugging_prompt(question, retrieved_docs)\n",
        "\n",
        "    response = OPENAI_CLIENT.chat.completions.create(\n",
        "        model=GENERATION_MODEL,\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"user\", \"content\": final_prompt}]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content, retrieved_docs\n"
      ],
      "metadata": {
        "id": "bJAIxgTHdrLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting everything together, you can start asking questions that are tailored to your needs\n"
      ],
      "metadata": {
        "id": "Av2RgeS2EbTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain MCP to me\"\n",
        "explanation, docs = vanilla_rag(query)\n",
        "\n",
        "display(Markdown(explanation))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pk7lyHy1ds5C",
        "outputId": "32d6af03-1425-44b4-a0fc-c3c64618c4be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### What is MCP?\n\nThe **Model Context Protocol (MCP)** is an **open protocol** designed to standardize how applications provide context to **Large Language Models (LLMs)**. Think of MCP as a **USB-C port for AI applications**‚Äîit provides a universal way to connect LLMs to various data sources, tools, and workflows. This makes it easier to build AI-powered applications that can interact with external systems, databases, and tools in a consistent and flexible manner.\n\n---\n\n### Why MCP?\n\nMCP addresses the challenges of integrating LLMs with external data and tools by providing:\n\n1. **Standardized Integration**: MCP creates a common framework for connecting LLMs to data sources, tools, and workflows, reducing the complexity of custom integrations.\n2. **Flexibility**: It allows developers to switch between LLM providers and vendors without significant changes to their applications.\n3. **Security**: MCP emphasizes best practices for securing data within your infrastructure.\n4. **Interoperability**: By joining the MCP ecosystem, applications can leverage a growing list of pre-built integrations and tools.\n\n---\n\n### How MCP Works\n\nAt its core, MCP follows a **client-server architecture**:\n\n- **MCP Hosts**: Applications like IDEs, AI tools, or platforms (e.g., Claude Desktop) that want to access data or tools via MCP.\n- **MCP Clients**: Protocol clients that establish 1:1 connections with MCP servers.\n- **MCP Servers**: Lightweight programs that expose specific capabilities (e.g., accessing a database, running tools) through the MCP standard.\n- **Local Data Sources**: Files, databases, or services on your computer that MCP servers can securely access.\n- **Remote Services**: External systems (e.g., APIs) that MCP servers can connect to.\n\nThis architecture allows LLMs to interact with both local and remote resources in a secure and standardized way.\n\n---\n\n### Key Features of MCP\n\n1. **Resources**: MCP servers can expose data (e.g., database schemas, files) as resources that LLMs can query or interact with.\n2. **Tools**: MCP enables LLMs to perform actions (e.g., running SQL queries, searching the web) through tools provided by servers.\n3. **Prompts**: Developers can create reusable prompt templates and workflows for LLMs.\n4. **Sampling**: Servers can request completions or responses from LLMs, enabling dynamic interactions.\n5. **Communication Mechanism**: MCP uses **Server-Sent Events (SSE)** for real-time communication between clients and servers.\n\n---\n\n### Example Use Cases\n\n1. **Database Integration**: An MCP server can connect to a PostgreSQL database, expose table schemas as resources, and provide tools for running SQL queries.\n2. **File System Access**: An MCP server can allow LLMs to securely access and analyze files on your local machine.\n3. **Agent Workflows**: MCP supports building hierarchical agent systems, enabling complex workflows where agents interact with each other and external systems.\n\n---\n\n### Benefits of Using MCP\n\n- **Interoperability**: Join a growing ecosystem of AI applications that work seamlessly together.\n- **Customizability**: Build custom servers and clients tailored to your specific needs.\n- **Ease of Use**: Pre-built SDKs (e.g., Python, TypeScript) and tools make it easy to implement MCP in your applications.\n- **Scalability**: MCP supports both local-first workflows and remote connections, making it suitable for a wide range of use cases.\n\n---\n\n### Example MCP Servers\n\nHere are some examples of MCP servers and their capabilities:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    },\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n    },\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n- **Memory Server**: Provides in-memory storage for testing and development.\n- **File System Server**: Allows secure access to specific files on your local machine.\n- **GitHub Server**: Interacts with GitHub repositories using a personal access token.\n\n---\n\n### Getting Started with MCP\n\n1. **Choose Your Role**:\n   - **Server Developer**: Build your own MCP server to expose data or tools.\n   - **Client Developer**: Build a client that integrates with existing MCP servers.\n   - **End User**: Use pre-built servers with tools like Claude Desktop.\n\n2. **Use SDKs**:\n   - MCP provides SDKs in **Python** and **TypeScript** to simplify development.\n\n3. **Explore Examples**:\n   - Check out the [MCP Servers Repository](https://github.com) for reference implementations.\n\n4. **Test and Debug**:\n   - Use tools like the **MCP Inspector** or **Apify MCP Tester** to test your servers and clients.\n\n---\n\n### Conclusion\n\nMCP is a powerful protocol that simplifies the integration of LLMs with external systems and tools. By standardizing how context is provided to LLMs, MCP enables developers to build more robust, flexible, and interoperable AI applications. Whether you're building custom servers, creating clients, or using pre-built integrations, MCP provides the tools and framework to unlock the full potential of LLMs."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even ones that are highly specific"
      ],
      "metadata": {
        "id": "MUXU77jDhauf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Can you explain the components of MCP in more detail to me like I'm an engineer with no idea of tech\"\n",
        "explanation, docs = vanilla_rag(query)\n",
        "\n",
        "display(Markdown(explanation))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GD445Zjvc19D",
        "outputId": "164ae528-fc18-4592-9656-e858986e1afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Certainly! Let me break down the **Model Context Protocol (MCP)** into simple, digestible components, using relatable analogies and examples. Think of MCP as a system that helps **AI models (like ChatGPT or Claude)** connect to different tools, data, and services in a standardized way. It‚Äôs like a universal adapter for AI applications.\n\n---\n\n### **Core Components of MCP**\n\n1. **MCP Hosts**\n   - **What it is:** These are the programs or applications that want to use AI models to access data or perform tasks.\n   - **Analogy:** Imagine a smartphone that wants to connect to different devices like headphones, printers, or TVs. The smartphone is the \"host.\"\n   - **Example:** Claude Desktop App or an IDE (like VS Code) that uses MCP to fetch data or run tools.\n\n2. **MCP Servers**\n   - **What it is:** These are lightweight programs that provide specific capabilities or data to the host. They act as the \"providers\" of information or tools.\n   - **Analogy:** Think of a vending machine that provides snacks. Each vending machine (server) offers a specific type of snack (capability).\n   - **Example:** An MCP server could connect to a database, expose file systems, or provide access to APIs like GitHub.\n\n3. **MCP Clients**\n   - **What it is:** These are the middlemen that connect the host to the server. They maintain a 1:1 connection with the servers and help the host communicate with them.\n   - **Analogy:** Imagine a translator who helps two people speaking different languages understand each other. The client is the translator.\n   - **Example:** A client could be a tool like the **Apify MCP Tester**, which connects to servers and tests their functionality.\n\n4. **Local Data Sources**\n   - **What it is:** These are files, databases, or services on your computer that MCP servers can securely access.\n   - **Analogy:** Think of your computer as a library, and the MCP server is a librarian who fetches the books (data) you need.\n   - **Example:** An MCP server might access your local file system to retrieve documents or images.\n\n5. **Remote Services**\n   - **What it is:** These are external systems or APIs available over the internet that MCP servers can connect to.\n   - **Analogy:** Imagine ordering food from a restaurant using a delivery app. The restaurant is the remote service, and the app is the MCP server connecting to it.\n   - **Example:** An MCP server could connect to a weather API to fetch real-time weather data.\n\n---\n\n### **How MCP Works Together**\n\nLet‚Äôs put it all together with an example:\n\n- **Scenario:** You‚Äôre using an AI assistant in your IDE (like VS Code) to analyze a database and generate a report.\n  1. The **MCP Host** (your IDE) wants to access the database.\n  2. The **MCP Client** connects the host to an **MCP Server** that knows how to interact with the database.\n  3. The **MCP Server** fetches the data from the **Local Data Source** (your database) and sends it back to the host.\n  4. The AI assistant in the IDE uses this data to generate the report.\n\n---\n\n### **Key Features of MCP**\n\n- **Standardization:** MCP provides a common way for AI models to interact with tools and data, like how USB-C works for connecting devices.\n- **Flexibility:** You can switch between different AI models or tools without changing the underlying infrastructure.\n- **Security:** MCP ensures that data stays within your infrastructure and is accessed securely.\n\n---\n\n### **Why MCP is Useful**\n\n- **For Developers:** It simplifies the process of integrating AI models with tools and data. You don‚Äôt need to reinvent the wheel every time you want to connect an AI model to a new system.\n- **For Businesses:** It allows seamless workflows and better utilization of AI capabilities, making it easier to build powerful applications.\n\n---\n\n### **Visualizing MCP**\n\nHere‚Äôs a simple diagram to help you visualize the components:\n\n```\n[Host (e.g., IDE)] <---> [Client] <---> [Server] <---> [Data Source/Remote Service]\n```\n\n---\n\n### **Example Use Case**\n\nLet‚Äôs say you want to build an MCP server that connects to a PostgreSQL database. Here‚Äôs how you‚Äôd describe it:\n\n1. **What it does:** The server connects to the database, exposes table schemas as resources, and allows running read-only SQL queries.\n2. **How it helps:** It enables an AI assistant to analyze the database and provide insights without directly accessing the database itself.\n\n---\n\n### **Next Steps**\n\nIf you want to dive deeper, you can:\n- Explore the **MCP SDKs** (e.g., Python or TypeScript) to start building your own servers or clients.\n- Use tools like the **MCP Inspector** to test and debug your servers.\n- Check out the **MCP Servers Repository** for examples and inspiration.\n\nLet me know if you‚Äôd like more details or examples! üòä"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You've built a powerful documentation assistant using **Firecrawl**, **Unstructured**, and **MongoDB Atlas** that can:\n",
        "\n",
        "- Programmatically crawl and extract content from the web\n",
        "- Preprocess and structure unstructured data for LLM consumption\n",
        "- Ingest and query that data using a vector-enabled database\n",
        "- Generate grounded, context-rich responses using OpenAI's gpt-4o\n",
        "\n",
        "**Good RAG starts with well-prepared data, and the [Unstructured Platform](https://unstructured.io/developers) simplifies this critical first step.** By enabling efficient ingestion, partitioning, and metadata enrichment of unstructured data, it ensures that your RAG pipeline is built on a solid foundation ‚Äî unlocking its full potential.\n",
        "\n",
        "Now go forth and conquer your documentation!"
      ],
      "metadata": {
        "id": "aJFhyPwYEoRU"
      }
    }
  ]
}