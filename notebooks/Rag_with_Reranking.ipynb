{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6YEKxii0WgP"
      },
      "source": [
        "# RAG for Patent Question Answering with Reranker\n",
        "\n",
        "Most retrieval-augmented generation (RAG) pipelines follow a common recipe: take a userâ€™s question, retrieve relevant documents, and feed them to a language model to generate a response. This works reasonably well â€” until it doesn't.\n",
        "\n",
        "When dealing with complex domains like patents, the limitations of naive retrieval become glaring:\n",
        "- The language is dense and technical.\n",
        "- Similarity-based retrievers often surface verbose but irrelevant sections.\n",
        "- Critical information may be buried across long documents.\n",
        "\n",
        "In this notebook, weâ€™ll build a more **robust and domain-aware RAG system** specifically designed to answer technical and legal questions over patents. To improve retrieval quality, weâ€™ll incorporate a **reranker** â€” a model that sits between retrieval and generation, reshuffling candidate passages to surface the most answer-relevant chunks.\n",
        "\n",
        "This system will:\n",
        "- Load and structure unstructured patent filings using the [Unstructured API](https://unstructured.io/).\n",
        "- Ingest data into a [Pinecone](https://www.pinecone.io/product/) vector database for fast semantic retrieval.\n",
        "- Re-rank retrieved candidates using **Cohereâ€™s `rerank-english-v3.0`**.\n",
        "- Answer user questions using **GPT-4o** grounded in the reranked context.\n",
        "\n",
        "Weâ€™ll go step by step â€” starting with document ingestion and ending with an end-to-end QA pipeline that performs well even on nuanced queries.\n",
        "\n",
        "Letâ€™s dive in.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Liy5kmr7DKp7"
      },
      "source": [
        "#Preparing the Data\n",
        "To prepare our patent data for retrieval and reranking, we need to first break down the raw PDFs into structured chunks. This step is foundational for any RAG pipeline, and itâ€™s where [Unstructured](https://unstructured.io) comes in.\n",
        "\n",
        "The Unstructured API lets us:\n",
        "- Extract clean, structured content from any document.\n",
        "- Generates metadata, chunk text, and prep it for downstream applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IDg_hZ-Dqzo"
      },
      "source": [
        "## Setting Up the Unstructured Client\n",
        "\n",
        "Before we can begin parsing raw patent documents, we need to set up access to the [Unstructured API](https://unstructured.io). The Unstructured API allows us to programmatically process documents, extract structured elements, and prepare them for chunking and embedding, all from within this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1yEFnvkQGcSV",
        "outputId": "f6f11e5c-98aa-4498-8fc8-6bd91e018416"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"unstructured-client\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es7EaldpEtj7"
      },
      "source": [
        "If you havenâ€™t already:\n",
        "1. Login to [platform.unstructured.io](https://platform.unstructured.io)\n",
        "2. In the sidebar, go to **API Keys**.\n",
        "3. Click **New Key**, give it a name like `\"patent-qna-notebook\"`, and copy the key.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUUQ6TnbE-R-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from unstructured_client import UnstructuredClient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vH2wwCAF2mF"
      },
      "source": [
        "Fetching the keys from Colab Secrets!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUG_gKfH0d9R"
      },
      "outputs": [],
      "source": [
        "os.environ['UNSTRUCTURED_API_KEY'] = userdata.get(\"UNSTRUCTURED_API_KEY\")\n",
        "client = UnstructuredClient(api_key_auth=os.getenv(\"UNSTRUCTURED_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAgNx9Ej0cjz"
      },
      "outputs": [],
      "source": [
        "# utility for inspecting responses in a readable way\n",
        "def pretty_print_model(response_model):\n",
        "    print(response_model.model_dump_json(indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXvczp6t0h_S"
      },
      "source": [
        "## Setting up the S3 Source Connector\n",
        "\n",
        "\n",
        "For this demo, we will be using AWS Key and Secret for Authentication.\n",
        "Make sure to add fetch the corresponding values and for `S3_AWS_KEY` and `S3_AWS_SECRET` and add to the Secrets in Colab.\n",
        "\n",
        "\n",
        "Similarly, fetch the the S3 URI to the bucket or folder, formatted as `s3://my-bucket/` (if the files are in the bucket's root) or `s3://my-bucket/my-folder/` and add it to `S3_REMOTE_URL` in the Secrets.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For other authentication options and more details refer to [this](https://docs.unstructured.io/api-reference/workflow/sources/s3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gebtI0S20fum"
      },
      "outputs": [],
      "source": [
        "os.environ['AWS_ACCESS'] = userdata.get('AWS_ACCESS')\n",
        "os.environ['AWS_SECRET'] = userdata.get('AWS_SECRET')\n",
        "os.environ['S3_REMOTE_URL'] = userdata.get('S3_REMOTE_URL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IobJkeHa0wKJ"
      },
      "outputs": [],
      "source": [
        "from unstructured_client.models.operations import CreateSourceRequest\n",
        "from unstructured_client.models.shared import CreateSourceConnector\n",
        "\n",
        "source_response = client.sources.create_source(\n",
        "    request=CreateSourceRequest(\n",
        "        create_source_connector=CreateSourceConnector(\n",
        "            name=f\"Reranker Tutorial Source Connector_\",\n",
        "            type=\"s3\",\n",
        "            config={\n",
        "              \"key\": os.environ.get('AWS_ACCESS'),\n",
        "              \"secret\": os.environ.get('AWS_SECRET'),\n",
        "              \"remote_url\": os.environ.get('S3_REMOTE_URL'),\n",
        "              \"recursive\": True\n",
        "            }\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os1_javB0mQT",
        "outputId": "7f7543b0-f7d2-470d-cc46-f8082667b6c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"config\": {\n",
            "        \"anonymous\": false,\n",
            "        \"recursive\": true,\n",
            "        \"remote_url\": \"s3://ajay-uns-devrel-content/mm-agentic-rag/\",\n",
            "        \"key\": \"**********\",\n",
            "        \"secret\": \"**********\"\n",
            "    },\n",
            "    \"created_at\": \"2025-08-06T14:57:07.277627Z\",\n",
            "    \"id\": \"e63b3e59-58e7-4e0b-90b3-85a7a6f5ad69\",\n",
            "    \"name\": \"Reranker Tutorial Source Connector_\",\n",
            "    \"type\": \"s3\",\n",
            "    \"updated_at\": \"2025-08-06T14:57:07.416960Z\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "pretty_print_model(source_response.source_connector_information)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQgI4qJr0oFC"
      },
      "source": [
        "## Setting up the Pinecone Destination Connector\n",
        "\n",
        "Now that weâ€™ve defined our document source (from S3), the next step is to configure where the processed chunks should go. For that, weâ€™re using **Pinecone** â€” a fast, scalable vector database that's perfect for similarity search.\n",
        "\n",
        "In our case, weâ€™ll send embedded chunks of patent text to Pinecone, where they can later be searched via semantic queries.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŒ² Why Pinecone?\n",
        "\n",
        "Pinecone is optimized for storing and querying high-dimensional vector embeddings. It provides:\n",
        "- Scalable infrastructure for similarity search.\n",
        "- Fast approximate nearest neighbor lookup.\n",
        "- Simple API access for indexing and querying.\n",
        "\n",
        "In this setup, Unstructured handles:\n",
        "- Preprocessing the data (partitioning, chunking, embedding).\n",
        "- Pushing the output vectors directly into our Pinecone index.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "To connect Unstructured with Pinecone, youâ€™ll need:\n",
        "\n",
        "- **API Key**: Found under the API Keys tab in the Pinecone dashboard.\n",
        "- **Index Name**: Create one manually from the dashboard, and ensure itâ€™s in the \"Serverless\" environment.\n",
        "- (Optional: Namespace) â€” used to logically group your documents inside the index.\n",
        "\n",
        "If you havenâ€™t already:\n",
        "1. Go to [https://app.pinecone.io](https://app.pinecone.io) and sign in.\n",
        "2. Create a **Serverless Index**.\n",
        "3. Note the **index name** and **API key** from the dashboard.\n",
        "\n",
        "Store both values securely in Colab secrets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Hyxry7D8Ivq"
      },
      "outputs": [],
      "source": [
        "os.environ['PINECONE_INDEX'] = userdata.get('PINECONE_INDEX')\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZn6XbJw1DhQ",
        "outputId": "2b0a2235-64b8-42e1-acdb-8d65cc733248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"config\": {\n",
            "        \"api_key\": \"**********\",\n",
            "        \"batch_size\": 50,\n",
            "        \"index_name\": \"uns-demo-2\",\n",
            "        \"namespace\": \"Default\"\n",
            "    },\n",
            "    \"created_at\": \"2025-08-06T14:57:09.636042Z\",\n",
            "    \"id\": \"3122da51-b23b-415c-a544-e329ba964c66\",\n",
            "    \"name\": \"Reranker Tutorial Destination Connector_\",\n",
            "    \"type\": \"pinecone\",\n",
            "    \"updated_at\": \"2025-08-06T14:57:09.739495Z\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from unstructured_client.models.operations import CreateDestinationRequest\n",
        "from unstructured_client.models.shared import CreateDestinationConnector\n",
        "\n",
        "destination_response = client.destinations.create_destination(\n",
        "    request=CreateDestinationRequest(\n",
        "        create_destination_connector=CreateDestinationConnector(\n",
        "            name=f\"Reranker Tutorial Destination Connector_\",\n",
        "            type=\"pinecone\",\n",
        "            config={\n",
        "                \"index_name\": os.environ.get(\"PINECONE_INDEX\"),\n",
        "                \"api_key\": os.environ.get(\"PINECONE_API_KEY\"),\n",
        "                \"batch_size\": 50,\n",
        "                \"namespace\": \"Default\" # Default Option\n",
        "            }\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "pretty_print_model(destination_response.destination_connector_information)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIy3Y1gEGpOj"
      },
      "source": [
        "Next, weâ€™ll wire everything together into a full document processing workflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onYT6ODu0uSp"
      },
      "source": [
        "## Creating a Document Processing Workflow\n",
        "\n",
        "Now that we have access to our data, the next step is setting up how it should be processed.\n",
        "\n",
        "We'll define a simple but powerful document pipeline using three key types of processing nodes:\n",
        "\n",
        "- **Partitioner**  \n",
        "  This step takes raw, unstructured files and extracts structured content from them.  \n",
        "  We'll use a **Vision-Language Model (VLM) Partitioner**, which leverages a model capable of understanding both text and layout information from documents â€” pulling out elements from each page with higher fidelity.\n",
        "\n",
        "- **Chunker**  \n",
        "  After partitioning, the extracted elements are grouped into manageable \"chunks.\"  \n",
        "  Chunking ensures that during retrieval, we can focus only on the most relevant sections of a document â€” not the whole thing.\n",
        "\n",
        "- **Embedder**  \n",
        "  Finally, we'll generate vector embeddings for each chunk of text.  \n",
        "  Embeddings are numeric representations that capture the meaning of the text, making it searchable and retrievable later on. We'll rely on an embedding provider to handle this step for us.\n",
        "\n",
        "Each node plays a critical role in making our documents **retrieval-ready** for downstream RAG applications.\n",
        "\n",
        "If you're curious about the different configuration options available for these processing steps, you can explore more details in the [Concepts documentation](https://docs.unstructured.io/ui/document-elements).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2tJmoCP0u1W",
        "outputId": "0a2fab62-3528-494f-cc5c-845e61703070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"created_at\": \"2025-08-06T14:57:11.657721Z\",\n",
            "    \"destinations\": [\n",
            "        \"3122da51-b23b-415c-a544-e329ba964c66\"\n",
            "    ],\n",
            "    \"id\": \"974f7a59-df45-469e-94d2-09e0ec1f2500\",\n",
            "    \"name\": \"Reranker Tutorial Workflow_1754492231.632472\",\n",
            "    \"sources\": [\n",
            "        \"e63b3e59-58e7-4e0b-90b3-85a7a6f5ad69\"\n",
            "    ],\n",
            "    \"status\": \"active\",\n",
            "    \"workflow_nodes\": [\n",
            "        {\n",
            "            \"name\": \"Partitioner\",\n",
            "            \"subtype\": \"vlm\",\n",
            "            \"type\": \"partition\",\n",
            "            \"id\": \"639c45c1-8009-4bfa-80a1-5e0e4b325467\",\n",
            "            \"settings\": {\n",
            "                \"provider\": \"anthropic\",\n",
            "                \"provider_api_key\": null,\n",
            "                \"model\": \"claude-3-7-sonnet-20250219\",\n",
            "                \"output_format\": \"text/html\",\n",
            "                \"prompt\": null,\n",
            "                \"format_html\": true,\n",
            "                \"unique_element_ids\": true,\n",
            "                \"is_dynamic\": false,\n",
            "                \"allow_fast\": true\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Chunker\",\n",
            "            \"subtype\": \"chunk_by_title\",\n",
            "            \"type\": \"chunk\",\n",
            "            \"id\": \"c51efc7d-6a8e-4663-b852-5bb5f7023539\",\n",
            "            \"settings\": {\n",
            "                \"unstructured_api_url\": null,\n",
            "                \"unstructured_api_key\": null,\n",
            "                \"multipage_sections\": false,\n",
            "                \"combine_text_under_n_chars\": null,\n",
            "                \"include_orig_elements\": false,\n",
            "                \"new_after_n_chars\": 1000,\n",
            "                \"max_characters\": 4096,\n",
            "                \"overlap\": 150,\n",
            "                \"overlap_all\": false,\n",
            "                \"contextual_chunking_strategy\": null\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Embedder\",\n",
            "            \"subtype\": \"azure_openai\",\n",
            "            \"type\": \"embed\",\n",
            "            \"id\": \"a34562a2-766d-4519-92be-4833288f7f83\",\n",
            "            \"settings\": {\n",
            "                \"model_name\": \"text-embedding-3-large\"\n",
            "            }\n",
            "        }\n",
            "    ],\n",
            "    \"reprocess_all\": false,\n",
            "    \"schedule\": {\n",
            "        \"crontab_entries\": []\n",
            "    },\n",
            "    \"updated_at\": \"2025-08-06T14:57:11.672337Z\",\n",
            "    \"workflow_type\": \"custom\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from unstructured_client.models.shared import (\n",
        "    WorkflowNode,\n",
        "    WorkflowType,\n",
        "    Schedule\n",
        ")\n",
        "\n",
        "parition_node = WorkflowNode(\n",
        "    name=\"Partitioner\",\n",
        "    subtype=\"vlm\",\n",
        "    type=\"partition\",\n",
        "    settings={\n",
        "        \"provider\": \"anthropic\",\n",
        "        \"model\": \"claude-3-7-sonnet-20250219\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "chunk_node = WorkflowNode(\n",
        "    name=\"Chunker\",\n",
        "    subtype=\"chunk_by_title\",\n",
        "    type=\"chunk\",\n",
        "    settings={\n",
        "        \"new_after_n_chars\": 1000,\n",
        "        \"max_characters\": 4096,\n",
        "        \"overlap\": 150\n",
        "    }\n",
        ")\n",
        "\n",
        "embedder_node = WorkflowNode(\n",
        "    name='Embedder',\n",
        "    subtype='azure_openai',\n",
        "    type=\"embed\",\n",
        "    settings={\n",
        "        'model_name': 'text-embedding-3-large'\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "response = client.workflows.create_workflow(\n",
        "    request={\n",
        "        \"create_workflow\": {\n",
        "            \"name\": f\"Reranker Tutorial Workflow_{time.time()}\",\n",
        "            \"source_id\": source_response.source_connector_information.id,\n",
        "            \"destination_id\": destination_response.destination_connector_information.id,\n",
        "            \"workflow_type\": WorkflowType.CUSTOM,\n",
        "            \"workflow_nodes\": [\n",
        "                parition_node,\n",
        "                chunk_node,\n",
        "                embedder_node\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "pretty_print_model(response.workflow_information)\n",
        "workflow_id = response.workflow_information.id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTrsjVhIqvTN"
      },
      "source": [
        "## Running the workflow\n",
        "\n",
        "Now that we've defined how we want to process our documentation, let's start the workflow and wait for it to complete:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuX5R8p37Src",
        "outputId": "16f79d45-57ee-46f5-c001-7d415f7b7920"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"created_at\": \"2025-08-06T14:57:13.160450Z\",\n",
            "    \"id\": \"5dfaecab-e7f5-4ff2-84b6-6b460756bdf6\",\n",
            "    \"status\": \"SCHEDULED\",\n",
            "    \"workflow_id\": \"974f7a59-df45-469e-94d2-09e0ec1f2500\",\n",
            "    \"workflow_name\": \"Reranker Tutorial Workflow_1754492231.632472\",\n",
            "    \"job_type\": \"ephemeral\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "res = client.workflows.run_workflow(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id,\n",
        "    }\n",
        ")\n",
        "\n",
        "pretty_print_model(res.job_information)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxc3G6DS7WDR",
        "outputId": "d8034e8f-1b10-4bd1-ec15-8c8cab6d9895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "job_id: 5dfaecab-e7f5-4ff2-84b6-6b460756bdf6\n"
          ]
        }
      ],
      "source": [
        "response = client.jobs.list_jobs(\n",
        "    request={\n",
        "        \"workflow_id\": workflow_id\n",
        "    }\n",
        ")\n",
        "\n",
        "last_job = response.response_list_jobs[0]\n",
        "job_id = last_job.id\n",
        "print(f\"job_id: {job_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zTfzanq4Vf"
      },
      "source": [
        "Now that we've created and started a job, we can poll Unstructured's `get_job` endpoint and check for its status every 30s till completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-DJdmDQ7nki",
        "outputId": "8ccbeb91-b3aa-4364-f948-64eadb6fae11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job is scheduled, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is in progress, polling again in 30 seconds...\n",
            "Job is completed\n",
            "{\n",
            "    \"created_at\": \"2025-08-06T14:57:13.160450\",\n",
            "    \"id\": \"5dfaecab-e7f5-4ff2-84b6-6b460756bdf6\",\n",
            "    \"status\": \"COMPLETED\",\n",
            "    \"workflow_id\": \"974f7a59-df45-469e-94d2-09e0ec1f2500\",\n",
            "    \"workflow_name\": \"Reranker Tutorial Workflow_1754492231.632472\",\n",
            "    \"job_type\": \"ephemeral\",\n",
            "    \"runtime\": \"PT0S\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def poll_job_status(job_id, wait_time=30):\n",
        "    while True:\n",
        "        response = client.jobs.get_job(\n",
        "            request={\n",
        "                \"job_id\": job_id\n",
        "            }\n",
        "        )\n",
        "\n",
        "        job = response.job_information\n",
        "\n",
        "        if job.status == \"SCHEDULED\":\n",
        "            print(f\"Job is scheduled, polling again in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        elif job.status == \"IN_PROGRESS\":\n",
        "            print(f\"Job is in progress, polling again in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        else:\n",
        "            print(\"Job is completed\")\n",
        "            break\n",
        "\n",
        "    return job\n",
        "\n",
        "job = poll_job_status(job_id)\n",
        "pretty_print_model(job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMQbRKJ1NBwX"
      },
      "source": [
        "At this point, we've completed all the foundational steps:\n",
        "\n",
        "- Extracted structured elements from raw documents using a **Partitioner**.\n",
        "- Organized the extracted content into manageable chunks with a **Chunker**.\n",
        "- Generated vector embeddings for those chunks through an **Embedder**.\n",
        "\n",
        "Our processed data is now stored and ready for retrieval.\n",
        "\n",
        "Next, we'll connect the pieces together and build a RAG pipeline that can answer questions grounded in this freshly structured knowledge base.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7imZgkEBOXn"
      },
      "source": [
        "# RAG ðŸ§ \n",
        "\n",
        "With our patent documents now chunked, embedded, and stored in Pinecone â€” weâ€™re ready to move into the **retrieval-augmented generation (RAG)** phase.\n",
        "\n",
        "In this section, we'll wire together:\n",
        "- A **retriever**, backed by Pinecone, to pull relevant chunks.\n",
        "- A **reranker**, using Cohereâ€™s `rerank-english-v3.0`, to boost the most contextually relevant results.\n",
        "- A **generator**, using OpenAIâ€™s `gpt-4o`, to produce accurate, grounded answers based on that refined context.\n",
        "\n",
        "Weâ€™ll also wrap these into a clean RAG pipeline using LangChainâ€™s modular components.\n",
        "\n",
        "\n",
        "For this portion, we will be using:\n",
        "\n",
        "- **`pinecone-client`**: Native SDK to interact with Pinecone vector indices (for inserting, querying, and managing embeddings).\n",
        "- **`cohere`**: Official client to access Cohereâ€™s APIs â€” including rerankers and language models.\n",
        "- **`langchain-*`**: A modular framework for chaining together LLMs, retrievers, tools, rerankers, and more â€” perfect for building custom RAG pipelines.\n",
        "\n",
        "Once everything's installed, we'll connect to our vector store, load our reranker, and build a chain that retrieves â†’ reranks â†’ generates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOs3tfvDafSk",
        "outputId": "cd40c08a-9874-46ce-9a9e-9586b1e0bd4b"
      },
      "outputs": [],
      "source": [
        "!pip install pinecone-client langchain-pinecone langchain-openai langchain-community cohere --upgrade --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnHFw0t2H49B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import urllib3\n",
        "from google.colab import userdata\n",
        "import cohere\n",
        "from pinecone import Pinecone\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.schema import Document\n",
        "from langchain.callbacks import get_openai_callback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTyWC4j7IPEn"
      },
      "source": [
        "Now that weâ€™ve installed our libraries, itâ€™s time to wire up the APIs. Weâ€™ll be using three providers in this RAG pipeline:\n",
        "\n",
        "\n",
        "- **Cohere**: for reranking retrieved chunks based on their actual relevance to a query.\n",
        "- **OpenAI**: for generating answers with `gpt-4o`.\n",
        "- **Pinecone**: to query the vector index we populated earlier.\n",
        "\n",
        "Weâ€™ll securely fetch each API key from Colab secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-JkRoYks5Ka"
      },
      "outputs": [],
      "source": [
        "# Set your API keys using Colab userdata\n",
        "os.environ['COHERE_API_KEY'] = userdata.get(\"COHERE_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"PINECONE_API_KEY\"] = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ[\"PINECONE_INDEX\"] = userdata.get(\"PINECONE_INDEX\")\n",
        "\n",
        "# Initialize Cohere client\n",
        "cohere_client = cohere.Client(os.environ['COHERE_API_KEY'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv0Oj6HMIZ1W"
      },
      "source": [
        "ðŸ›  Fixing Pinecone in Colab\n",
        "\n",
        "If you're running this in a Colab environment, Pineconeâ€™s client can sometimes misbehave due to Colabâ€™s proxy settings.\n",
        "\n",
        "This little fix disables warnings and clears proxy-related environment variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IsTOabPitVl"
      },
      "outputs": [],
      "source": [
        "# run this to ensure pinecone client works in your colab environment\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "# Clear proxy environment variables that might cause connection issues\n",
        "proxy_vars = ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']\n",
        "for var in proxy_vars:\n",
        "    if var in os.environ:\n",
        "        del os.environ[var]\n",
        "\n",
        "original_getproxies = requests.utils.getproxies\n",
        "requests.utils.getproxies = lambda: {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tonjlb1VKHU0"
      },
      "source": [
        "Before wiring things up, hereâ€™s a breakdown of the core functions used in this section.\n",
        "\n",
        "- `connect_pinecone(index_name: str)`  \n",
        "  Sets up a connection to the Pinecone index and wraps it as a LangChain-compatible vectorstore using OpenAIâ€™s `text-embedding-3-large`. Returns the vectorstore so we can use it for retrieval.\n",
        "\n",
        "- `retrieve_docs(vectorstore, query: str, k: int = 20)`  \n",
        "  Performs basic similarity search against the vectorstore. Grabs the top-k chunks closest to the query based on embeddings.\n",
        "\n",
        "- `rerank_docs(query: str, docs: list[Document], top_n: int = 5)`  \n",
        "  Takes the initial retrieved results and reorders them using Cohereâ€™s reranker model. This lets us prioritize documents that are actually useful for answering the question â€” not just semantically close.\n",
        "\n",
        "- `generate_answer(query: str, docs: list[Document])`  \n",
        "  Feeds the reranked context to GPT-4o to generate a final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhJw9h3DtG4M",
        "outputId": "d490c77b-794b-4a4d-ff71-01c1b92cbaee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to Pinecone index: uns-demo1\n"
          ]
        }
      ],
      "source": [
        "def connect_pinecone(index_name: str):\n",
        "    \"\"\"\n",
        "    Connect to Pinecone vectorstore\n",
        "\n",
        "    Args:\n",
        "        index_name: Name of the Pinecone index\n",
        "\n",
        "    Returns:\n",
        "        Configured vectorstore\n",
        "    \"\"\"\n",
        "    try:\n",
        "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "        pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "        index = pc.Index(index_name)\n",
        "\n",
        "        vectorstore = PineconeVectorStore(\n",
        "            index=index,\n",
        "            embedding=embeddings,\n",
        "            text_key=\"text\",\n",
        "            namespace='Default'\n",
        "        )\n",
        "\n",
        "        print(f\"Connected to Pinecone index: {index_name}\")\n",
        "        return vectorstore\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to connect to Pinecone: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def retrieve_docs(vectorstore, query: str, k: int = 20):\n",
        "    \"\"\"\n",
        "    Retrieve documents from vectorstore\n",
        "\n",
        "    Args:\n",
        "        vectorstore: Pinecone vectorstore\n",
        "        query: Search query\n",
        "        k: Number of documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        List of relevant documents\n",
        "    \"\"\"\n",
        "    try:\n",
        "        docs = vectorstore.similarity_search(query, k=k)\n",
        "        print(f\"Retrieved {len(docs)} documents\")\n",
        "        return docs\n",
        "    except Exception as e:\n",
        "        print(f\"Document retrieval failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def rerank_docs(query: str, docs: list[Document], top_n: int = 5):\n",
        "    \"\"\"\n",
        "    Rerank documents using Cohere's reranking model\n",
        "\n",
        "    Args:\n",
        "        query: Original search query\n",
        "        docs: List of retrieved documents\n",
        "        top_n: Number of top documents to return\n",
        "\n",
        "    Returns:\n",
        "        List of reranked documents\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = cohere_client.rerank(\n",
        "            query=query,\n",
        "            documents=[doc.page_content for doc in docs],\n",
        "            top_n=top_n,\n",
        "            model=\"rerank-english-v3.0\"\n",
        "        )\n",
        "\n",
        "        reranked_docs = [docs[r.index] for r in response.results]\n",
        "        print(f\"Reranked to top {len(reranked_docs)} documents\")\n",
        "        return reranked_docs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Reranking failed: {e}\")\n",
        "        return docs[:top_n]  # Fallback\n",
        "\n",
        "def generate_answer(query: str, docs: list[Document]):\n",
        "    \"\"\"\n",
        "    Generate answer using retrieved documents\n",
        "\n",
        "    Args:\n",
        "        query: User question\n",
        "        docs: List of relevant documents\n",
        "\n",
        "    Returns:\n",
        "        Generated answer\n",
        "    \"\"\"\n",
        "    try:\n",
        "        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        prompt = f\"\"\"Answer the following question using the context below. Answer only based on the context provided, if there is not enough information, mention that there's not enough information:\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Answer:\"\"\"\n",
        "        with get_openai_callback() as cb:\n",
        "          response = llm.invoke(prompt)\n",
        "\n",
        "          result = {\n",
        "            \"answer\": response.content,\n",
        "            \"prompt_tokens\": cb.prompt_tokens,\n",
        "            \"completion_tokens\": cb.completion_tokens,\n",
        "            \"total_tokens\": cb.total_tokens,\n",
        "            \"total_cost\": cb.total_cost\n",
        "          }\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Answer generation failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Connect to vectorstore\n",
        "vectorstore = connect_pinecone(os.environ[\"PINECONE_INDEX\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUBBvWKGMrB2"
      },
      "source": [
        "### Vanilla RAG\n",
        "\n",
        "Weâ€™ll start with a simple retrieval-augmented generation setup: grab the top-k documents from Pinecone using embedding similarity, and pass them directly to GPT-4o.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qw1HcYKSvig6"
      },
      "outputs": [],
      "source": [
        "class BasicRAGSystem:\n",
        "    def __init__(self, vectorstore, k=10):\n",
        "        self.vectorstore = vectorstore\n",
        "        self.k = k\n",
        "\n",
        "    def query(self, question):\n",
        "        \"\"\"Execute basic RAG pipeline\"\"\"\n",
        "\n",
        "        # Retrieve documents\n",
        "        docs = retrieve_docs(self.vectorstore, question, k=self.k)\n",
        "\n",
        "        # Generate answer\n",
        "        answer = generate_answer(question, docs)\n",
        "\n",
        "\n",
        "        result = {\n",
        "            \"documents\": docs,\n",
        "            \"num_docs\": len(docs)\n",
        "        }\n",
        "        result.update(answer)\n",
        "\n",
        "        return result\n",
        "\n",
        "basic_rag = BasicRAGSystem(vectorstore,10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALLZZ7gQvsNZ",
        "outputId": "3e708506-8318-4b52-9244-3138ef8c17d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic RAG Results:\n",
            "--------------------------------------------------\n",
            "Retrieved 10 documents\n",
            "Answer: The primary function of the context analysis engine described in US11886826B1 is to analyze input data and/or user instructions to output a set of context parameters associated with the input data. These context parameters may include information such as location (\"where\"), person (\"who\"), time period or time of day (\"when\"), event (\"what\"), or causal reasoning (\"why\") associated with the input data. The context analysis engine may also retain the output of the set of context parameters through multiple iterations of execution, allowing for retention of context information for changes without needing to reload large amounts of information.\n",
            "Retrieved 10 documents\n",
            "Total Tokens: 7741\n"
          ]
        }
      ],
      "source": [
        "test_query = \"What is the primary function of the context analysis engine described in US11886826B1?\"\n",
        "\n",
        "print(\"Basic RAG Results:\")\n",
        "print(\"-\" * 50)\n",
        "basic_result = basic_rag.query(test_query)\n",
        "\n",
        "print(f\"Answer: {basic_result['answer']}\")\n",
        "print(f\"Retrieved {basic_result['num_docs']} documents\")\n",
        "print(f\"Total Tokens: {basic_result['total_tokens']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9s9WUB8MAEV"
      },
      "source": [
        "The vanilla setup gets the right answer here. It finds the relevant chunk in the top 10 and generates a clean response.\n",
        "\n",
        "Now let's try a more complex question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEp5dJ76MKKf",
        "outputId": "2a72400d-fa14-46cc-8cac-6b1dda490aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic RAG Results:\n",
            "--------------------------------------------------\n",
            "Retrieved 10 documents\n",
            "Answer: There's not enough information to determine which of the two patents does not reference reward-based optimization and what training approach it uses instead.\n",
            "Retrieved 10 documents\n",
            "Total Tokens: 7234\n"
          ]
        }
      ],
      "source": [
        "test_query = \"Which of the two patents doesâ€¯not reference rewardâ€‘based optimization, and what training approach does it use instead?\"\n",
        "\n",
        "print(\"Basic RAG Results:\")\n",
        "print(\"-\" * 50)\n",
        "basic_result = basic_rag.query(test_query)\n",
        "\n",
        "print(f\"Answer: {basic_result['answer']}\")\n",
        "print(f\"Retrieved {basic_result['num_docs']} documents\")\n",
        "print(f\"Total Tokens: {basic_result['total_tokens']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Ay_UciMQfI"
      },
      "source": [
        "Even though we retrieved 10 chunks, none had what we needed. Let's try out a different approach to fetch the chunks **most relavant** to the query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QMpaRyQMtpR"
      },
      "source": [
        "### RAG with Reranking\n",
        "\n",
        "\n",
        "Plain vector search can only get us so far. Itâ€™s fast and useful, but itâ€™s not perfect, sometimes the right chunk doesnâ€™t make it into the top-10.\n",
        "\n",
        "To fix this, we add a reranking step.\n",
        "\n",
        "Hereâ€™s how it works:\n",
        "\n",
        "- First, we fetch a **larger set of candidate chunks** â€” say 30 â€” from the vectorstore.\n",
        "- Then we use a **reranker model** (in this case, Cohereâ€™s `rerank-english-v3.0`) to score each chunk by how well it matches the question.\n",
        "- We keep only the **top-N** (e.g. top 10) reranked chunks and send those to the LLM.\n",
        "\n",
        "This extra scoring step helps surface the most relevant content, especially for nuanced or multi-part questions that vector search might miss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ElPAmYgvtfj"
      },
      "outputs": [],
      "source": [
        "class EnhancedRAGSystem:\n",
        "    def __init__(self, vectorstore, k=40, top_n=20):\n",
        "        self.vectorstore = vectorstore\n",
        "        self.k = k\n",
        "        self.top_n = top_n\n",
        "\n",
        "    def query(self, question):\n",
        "        \"\"\"Execute enhanced RAG pipeline with reranking\"\"\"\n",
        "\n",
        "        initial_docs = retrieve_docs(self.vectorstore, question, k=self.k)\n",
        "\n",
        "        reranked_docs = rerank_docs(question, initial_docs, top_n=self.top_n)\n",
        "\n",
        "        answer = generate_answer(question, reranked_docs)\n",
        "\n",
        "        result = {\n",
        "            \"documents\": reranked_docs,\n",
        "            \"initial_docs\": initial_docs,\n",
        "            \"num_docs\": len(reranked_docs)\n",
        "        }\n",
        "        result.update(answer)\n",
        "        return result\n",
        "\n",
        "# Initialize enhanced RAG system to fetch 30 candidate docs -> 10 reranked docs\n",
        "enhanced_rag = EnhancedRAGSystem(vectorstore,30,10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4cESO-8Nz9J"
      },
      "source": [
        "And now, a query that failed with Vanilla RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUulXbcFv2ww",
        "outputId": "eb555370-1487-4392-ff98-95722576ddac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enhanced RAG with Reranking:\n",
            "--------------------------------------------------\n",
            "Retrieved 30 documents\n",
            "Reranked to top 10 documents\n",
            "Answer: The patent US 11,886,826 B1 does not reference reward-based optimization. Instead, it uses an iterative training approach based on one or more datasets, which may include user instruction data or user-labeled data.\n",
            "Retrieved 10 documents (from 30 initial)\n",
            "Total Tokens: 7662\n"
          ]
        }
      ],
      "source": [
        "test_query = \"Which of the two patents doesâ€¯not reference rewardâ€‘based optimization, and what training approach does it use instead?\"\n",
        "\n",
        "print(\"\\nEnhanced RAG with Reranking:\")\n",
        "print(\"-\" * 50)\n",
        "enhanced_result = enhanced_rag.query(test_query)\n",
        "\n",
        "print(f\"Answer: {enhanced_result['answer']}\")\n",
        "print(f\"Retrieved {enhanced_result['num_docs']} documents (from {len(enhanced_result['initial_docs'])} initial)\")\n",
        "print(f\"Total Tokens: {enhanced_result['total_tokens']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vCKuBqZOkSa"
      },
      "source": [
        "So what changed?\n",
        "\n",
        "Turns out the key chunk was buried deeper in the retrieval set, somewhere in the top 30, but not in the top 10 that vanilla RAG uses.\n",
        "\n",
        "With reranking, weâ€™re able to pull it up and pass it to the LLM, which now has enough signal to answer correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6pH1Q31M-ih"
      },
      "source": [
        "### Why not send the entire context to the LLM?\n",
        "\n",
        "Let's test it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqIT4IBGCiYZ",
        "outputId": "9a9c1abd-3a43-4060-ce28-225cca5a3197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic RAG Results:\n",
            "--------------------------------------------------\n",
            "Retrieved 30 documents\n",
            "Answer: The patent that does not reference reward-based optimization is US 2024/0256582 A1. Instead, it uses a training approach that involves generating a set of search results for a search query and providing the set of search results as part of an input prompt to guide a generative AI model in generating a summary response of the set of search results.\n",
            "Retrieved 30 documents\n",
            "Total Tokens: 22521\n"
          ]
        }
      ],
      "source": [
        "basic_rag = BasicRAGSystem(vectorstore,30)\n",
        "test_query = \"Which of the two patents doesâ€¯not reference rewardâ€‘based optimization, and what training approach does it use instead?\"\n",
        "\n",
        "print(\"Basic RAG Results:\")\n",
        "print(\"-\" * 50)\n",
        "basic_result = basic_rag.query(test_query)\n",
        "\n",
        "print(f\"Answer: {basic_result['answer']}\")\n",
        "print(f\"Retrieved {basic_result['num_docs']} documents\")\n",
        "print(f\"Total Tokens: {basic_result['total_tokens']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEN3_Yyx-RvK",
        "outputId": "a3370c05-1acc-4911-98ca-f32dcbef0338"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enhanced RAG with Reranking:\n",
            "--------------------------------------------------\n",
            "Retrieved 30 documents\n",
            "Reranked to top 10 documents\n",
            "Answer: The patent US 11,886,826 B1 does not reference reward-based optimization. Instead, it uses an iterative training approach based on one or more datasets, which may include user instruction data or user-labeled data.\n",
            "Retrieved 10 documents (from 30 initial)\n",
            "Total Tokens: 7662\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\nEnhanced RAG with Reranking:\")\n",
        "print(\"-\" * 50)\n",
        "enhanced_result = enhanced_rag.query(test_query)\n",
        "\n",
        "print(f\"Answer: {enhanced_result['answer']}\")\n",
        "print(f\"Retrieved {enhanced_result['num_docs']} documents (from {len(enhanced_result['initial_docs'])} initial)\")\n",
        "print(f\"Total Tokens: {enhanced_result['total_tokens']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB7IBMgbPSAr"
      },
      "source": [
        "Here, Vanilla RAG also gave a confused answer from using all 30 chunks as context and the cost difference is also huge.\n",
        "\n",
        "- **Vanilla RAG (k=30)** sends all 30 chunks straight to the LLM.\n",
        "- **Reranked RAG** pulls 30 candidates, scores them, and keeps only the top 10.\n",
        "\n",
        "Thatâ€™s **3x fewer tokens** for the same output.\n",
        "\n",
        "This isnâ€™t just about cost. With longer inputs, LLM latency also goes up.  \n",
        "Reranking helps us trim the fat and stay within context limits without sacrificing accuracy.\n",
        "\n",
        "So if you're going to over-fetch from the vector store, it's almost always better to rerank before you send."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlDHmBDD0BbH"
      },
      "source": [
        "If youâ€™re building anything question-answering or doc-heavy, try plugging in a reranker.  \n",
        "Itâ€™s a simple addition that can boost accuracy, trim cost, and make your LLMs look smarter.\n",
        "\n",
        "You can adapt the exact same setup to papers, reports, contracts â€” anything longform where chunk retrieval alone might not cut it.\n",
        "\n",
        "Start from this notebook, swap in your own data, and see what changes."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
